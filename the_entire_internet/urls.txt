https://api.github.com/users/redwoodresearch/repos
https://www.redwoodresearch.org/  
https://www.anthropic.com/  
https://openai.com/research  
https://deepmind.google/research  
https://alignmentforum.org/  
https://futureoflife.org/  
https://www.cser.ac.uk/  
https://www.fhi.ox.ac.uk/  
https://www.governance.ai/  
https://epochai.org/  
https://arxiv.org/list/cs.AI/recent  
https://arxiv.org/list/cs.LG/recent  
https://arxiv.org/list/cs.CL/recent  
https://arxiv.org/search/cs?query=AI+safety  
https://scholar.google.com/  
https://paperswithcode.com/  
https://dblp.org/  
https://philpapers.org/  
https://ojs.aaai.org/index.php/AAAI  
https://icml.cc/  
https://www.lesswrong.com/  
https://www.alignmentforum.org/  
https://www.effectivealtruism.org/  
https://forum.effectivealtruism.org/  
https://metaculus.com/questions/?search=AI  
https://ea.greaterwrong.com/  
https://x.com/  
https://www.linkedin.com/  
https://discord.gg/alignment  
https://www.reddit.com/r/slatestarcodex/  
https://www.cset.georgetown.edu/  
https://www.rand.org/topics/artificial-intelligence.html  
https://carnegieendowment.org/technology  
https://itif.org/  
https://brookings.edu/topic/artificial-intelligence/  
https://www.nber.org/programs-projects/programs-working-groups/artificial-intelligence  
https://bfi.uchicago.edu/  
https://www.niskanencenter.org/  
https://www.belfercenter.org/  
https://www.ceps.eu/  
https://paulfchristiano.com/  
https://rohinshah.com/  
https://www.alignmentnewsletter.com/  
https://bounded-regret.ghost.io/  
https://blog.anthropic.com/  
https://www.cold-takes.com/  
https://sideways-view.com/  
https://aisafety.camp/  
https://apartresearch.com/  
https://aisafety.world/  
https://alignment.dev/  
https://www.safe.ai/  
https://mlsafety.org/  
https://humancompatible.ai/  
https://openphilanthropy.org/focus/global-catastrophic-risks/ai-risk/  
https://80000hours.org/problem-profiles/ai-safety/  
https://www.safe.ai/blog/  
https://www.lesswrong.com/tag/ai-safety  
https://www.lesswrong.com/tag/ai-alignment  
https://www.lesswrong.com/tag/ai-control  
https://www.lesswrong.com/tag/ai-governance  
https://distill.pub/  
https://www.alignmentforum.org/tag/alignment  
https://transformativeaitechnologies.com/  
https://longtermrisk.org/  
https://www.serimats.org/  
https://www.michaeltraber.org/  
https://erikhoel.substack.com/  
https://www.dwarkeshpatel.com/  
https://jack-clark.net/  
https://weightingthestakes.com/  
https://beyondtheportal.substack.com/  
https://futureoflife.org/ai-safety-research/  
https://futureoflife.org/category/ai/  
https://safeaiworkshop.org/  
https://neurips.cc/Conferences/2025/CallForPapers  
https://aaai.org/Conferences/AAAI-25/  
https://iclr.cc/  
https://sites.google.com/view/aisafetyunconference  
https://alignment-newsletter.github.io/  
https://www.yudkowsky.net/  
https://intelligence.org/  
https://intelligence.org/research/  
https://arbital.com/explore/ai_alignment/  
https://arbital.com/p/ai_control/  
https://www.lesswrong.com/tag/orthogonality-thesis  
https://www.lesswrong.com/tag/instrumental-convergence  
https://www.lesswrong.com/tag/mesa-optimization  
https://www.lesswrong.com/tag/inner-alignment  
https://www.lesswrong.com/tag/outer-alignment  
https://www.lesswrong.com/tag/deceptive-alignment  
https://alignmentforum.org/posts  
https://arxiv.org/list/cs.CY/recent  
https://arxiv.org/list/cs.RO/recent  
https://arxiv.org/list/cs.CR/recent  
https://arxiv.org/list/cs.GT/recent  
https://arxiv.org/list/cs.CV/recent  
https://openreview.net/group?id=ICLR.cc/2025/Conference  

https://deepmindsafetyresearch.medium.com/  
https://deepmind.google/discover/blog/tag/safety  
https://openai.com/safety  
https://openai.com/research/alignment  
https://alignment.dev/blog  
https://aisafetyfundamentals.com/  
https://aisafety.training/  
https://aisafetyguide.com/  
https://www.safe.ai/resources  
https://aisafety.world/resources  
https://aisafety.info/  
https://aisafety.community/  
https://aisafety.wiki/  
https://aisafety.courses/  
https://aisafetyhub.com/  
https://aisafetytraining.org/  
https://aisafetycollective.org/  
https://aisafetyresearch.org/  
https://aisafetyinstitute.org/  
https://aisafety.foundation/  
https://aisafety.guru/  
https://futureoflife.org/team/ai/  
https://futureoflife.org/category/policy/  
https://futureoflife.org/open-letter/  
https://futureoflife.org/resource/  
https://aiimpacts.org/  
https://aiimpacts.org/publications/  
https://aiimpacts.org/blog/  
https://aiimpacts.org/data/  
https://aiimpacts.org/surveys/  
https://www.safe.ai/people  
https://humancompatible.ai/publications  
https://humancompatible.ai/blog  
https://humancompatible.ai/events  
https://humancompatible.ai/projects  
https://longtermrisk.org/publications/  
https://longtermrisk.org/research-agenda/  
https://longtermrisk.org/blog/  
https://longtermrisk.org/newsletter/  
https://longtermrisk.org/team/  
https://80000hours.org/podcast/  
https://80000hours.org/articles/  
https://80000hours.org/career-reviews/ai-safety-research/  
https://80000hours.org/problem-profiles/transformative-ai/  
https://epochai.org/blog  
https://epochai.org/publications  
https://epochai.org/events  
https://epochai.org/data  
https://epochai.org/projects  
https://governance.ai/research/  
https://governance.ai/publications/  
https://governance.ai/blog/  
https://governance.ai/projects/  
https://governance.ai/events/  
https://cset.georgetown.edu/publications/  
https://cset.georgetown.edu/research/  
https://cset.georgetown.edu/news/  
https://cset.georgetown.edu/data/  
https://cset.georgetown.edu/team/  
https://cser.ac.uk/publications/  
https://cser.ac.uk/policy/  
https://cser.ac.uk/news/  
https://cser.ac.uk/events/  
https://cser.ac.uk/research/  
https://fhi.ox.ac.uk/publications/  
https://fhi.ox.ac.uk/research/  
https://fhi.ox.ac.uk/team/  
https://fhi.ox.ac.uk/blog/  
https://fhi.ox.ac.uk/news/  
https://rand.org/pubs/external_publications.html?topic=/topics/artificial-intelligence.html  
https://brookings.edu/research/?s=artificial+intelligence  
https://carnegieendowment.org/publications/  
https://belfercenter.org/publication  
https://itif.org/publications  
https://itif.org/ai/  
https://itif.org/events/  
https://itif.org/reports/  
https://itif.org/blogs/  
https://niskanencenter.org/research/  
https://niskanencenter.org/blog/  
https://niskanencenter.org/events/  
https://niskanencenter.org/publications/  
https://niskanencenter.org/team/  
https://ceps.eu/publications/  
https://ceps.eu/research/  
https://ceps.eu/events/  
https://ceps.eu/news/  
https://ceps.eu/projects/  
https://serimats.org/blog  
https://serimats.org/publications  
https://serimats.org/projects  
https://serimats.org/news  
https://serimats.org/events  
https://distill.pub/archive/  
https://distill.pub/2020/alignment/  
https://distill.pub/2019/inner-alignment/  
https://distill.pub/2019/advex/  
https://distill.pub/2018/building-blocks/  

https://www.lesswrong.com/users/eliezer_yudkowsky  
https://www.lesswrong.com/users/paulfchristiano  
https://www.lesswrong.com/users/rohinmshah  
https://www.lesswrong.com/users/evhub  
https://www.lesswrong.com/users/alexander-turner  
https://www.lesswrong.com/users/jessicata  
https://www.lesswrong.com/users/scottalexander  
https://www.lesswrong.com/users/benedikt  
https://www.lesswrong.com/users/oliverhabryka  
https://www.lesswrong.com/users/turntrout  
https://www.lesswrong.com/users/jacobjacob  
https://www.lesswrong.com/users/larks  
https://www.lesswrong.com/users/jasoncrawford  
https://www.lesswrong.com/users/richard_ngo  
https://www.lesswrong.com/users/vaniver  
https://www.lesswrong.com/users/daniel-kokotajlo  
https://www.lesswrong.com/users/johnswentworth  
https://www.lesswrong.com/users/chrisleong  
https://www.lesswrong.com/users/charlie-steiner  
https://www.lesswrong.com/users/gwern  

https://www.yudkowsky.net/  
https://www.yudkowsky.net/rational/ai/  
https://arbital.com/p/ai_alignment/  
https://arbital.com/p/agi/  
https://arbital.com/p/orthogonality/  
https://arbital.com/p/instrumental_convergence/  
https://arbital.com/p/ai_control/  

https://www.alignmentforum.org/users/eliezer_yudkowsky  
https://www.alignmentforum.org/users/paulfchristiano  
https://www.alignmentforum.org/users/rohinmshah  
https://www.alignmentforum.org/users/evhub  
https://www.alignmentforum.org/users/alexander-turner  
https://www.alignmentforum.org/users/richard_ngo  
https://www.alignmentforum.org/users/daniel-kokotajlo  
https://www.alignmentforum.org/users/johnswentworth  
https://www.alignmentforum.org/users/turntrout  
https://www.alignmentforum.org/users/larks  

https://paulfchristiano.com/  
https://bounded-regret.ghost.io/  
https://rohinshah.com/  
https://rohinshah.com/blog/  
https://alignmentnewsletter.com/  
https://alignmentnewsletter.com/all/  

https://evhub.github.io/  
https://www.lesswrong.com/tag/evan-hubinger  
https://www.lesswrong.com/tag/mesa-optimization  
https://www.lesswrong.com/tag/inner-alignment  
https://www.lesswrong.com/tag/deceptive-alignment  

https://turntrout.com/  
https://turntrout.com/alignment/  
https://turntrout.com/blog/  

https://gwern.net/ai-risk  
https://gwern.net/alignment  
https://gwern.net/  

https://scottaaronson.blog/  
https://astralcodexten.substack.com/  
https://slatestarcodex.com/  

https://richardngo.substack.com/  
https://richardngo.substack.com/archive  
https://www.lesswrong.com/tag/richard-ngo  

https://www.johnswentworth.com/  
https://www.johnswentworth.com/blog/  
https://www.lesswrong.com/tag/john-swentworth  

https://daniel-kokotajlo.github.io/  
https://daniel-kokotajlo.github.io/posts/  
https://www.lesswrong.com/tag/daniel-kokotajlo  

https://www.lesswrong.com/tag/eliezer-yudkowsky  
https://www.lesswrong.com/tag/ai-safety  
https://www.lesswrong.com/tag/ai-alignment  
https://www.lesswrong.com/tag/ai-control  
https://www.lesswrong.com/tag/orthogonality-thesis  
https://www.lesswrong.com/tag/instrumental-convergence  
https://www.lesswrong.com/tag/mesa-optimization  
https://www.lesswrong.com/tag/deceptive-alignment  

https://arbital.com/explore/ai_alignment/  
https://arbital.com/explore/decision_theory/  
https://arbital.com/explore/friendly_ai/  
https://arbital.com/explore/value_learning/  
https://arbital.com/explore/mesa-optimization/  

https://www.lesswrong.com/users/raemon  
https://www.lesswrong.com/users/habryka  
https://www.lesswrong.com/users/adamshimi  
https://www.lesswrong.com/users/quintin  
https://www.lesswrong.com/users/lucy-e  
https://www.lesswrong.com/users/steve-byrnes  
https://www.lesswrong.com/users/ajeya  
https://www.lesswrong.com/users/owen-cotton-barratt  
https://www.lesswrong.com/users/beth-barnes  
https://www.lesswrong.com/users/so8res  
https://www.lesswrong.com/users/nate-soares  
https://www.lesswrong.com/users/vanessa-kosoy  
https://www.lesswrong.com/users/michael-cohen  
https://www.lesswrong.com/users/david-krueger  
https://www.lesswrong.com/users/ryan-greenblatt  
https://www.lesswrong.com/users/jessica-taylor  
https://www.lesswrong.com/users/steven-byrnes  
https://www.lesswrong.com/users/dagon  
https://www.lesswrong.com/users/john-maxwell  
https://www.lesswrong.com/users/michael-valtorta  

https://intelligence.org/  
https://intelligence.org/team/  
https://intelligence.org/research/  
https://intelligence.org/2019/strategy-and-alignment/  
https://intelligence.org/2015/ai-alignment/  

https://miri.org/  
https://miri.org/research/  
https://miri.org/papers/  
https://miri.org/publications/  
https://miri.org/team/  

https://alignmentforum.org/tag/alignment-research  
https://alignmentforum.org/tag/ai-governance  
https://alignmentforum.org/tag/deceptive-alignment  
https://alignmentforum.org/tag/mesa-optimization  
https://alignmentforum.org/tag/interpretability  

https://www.cold-takes.com/  
https://www.cold-takes.com/most-important-century/  
https://www.cold-takes.com/tag/ai/  

https://www.dwarkeshpatel.com/  
https://www.dwarkeshpatel.com/podcast  
https://www.dwarkeshpatel.com/p/paul-christiano-interview  
https://www.dwarkeshpatel.com/archive  

https://jack-clark.net/  
https://jack-clark.net/posts/  
https://jack-clark.net/ImportAI/  
https://jack-clark.net/archive/  

https://weightingthestakes.com/  
https://weightingthestakes.com/archive  
https://weightingthestakes.com/alignment  
https://weightingthestakes.com/tags/ai/  

https://bounded-regret.ghost.io/alignment/  
https://bounded-regret.ghost.io/archive/  
https://bounded-regret.ghost.io/machine-learning/  

https://erikhoel.substack.com/  
https://erikhoel.substack.com/archive  
https://erikhoel.substack.com/p/ai-doom  
https://erikhoel.substack.com/p/the-messy-truth-about-ai  

https://beyondtheportal.substack.com/  
https://beyondtheportal.substack.com/archive  
https://beyondtheportal.substack.com/p/ai-control  
https://beyondtheportal.substack.com/p/ai-governance  

https://sideways-view.com/  
https://sideways-view.com/blog/  
https://sideways-view.com/tag/ai-safety/  
https://sideways-view.com/tag/alignment/  

https://slatestarcodex.com/AI  
https://slatestarcodex.com/tag/artificial-intelligence/  
https://slatestarcodex.com/tag/ai/  
https://slatestarcodex.com/archives/  

https://astralcodexten.substack.com/archive  
https://astralcodexten.substack.com/p/agi-safety  
https://astralcodexten.substack.com/p/ai-risk  
https://astralcodexten.substack.com/p/artificial-intelligence-governance  

https://gwern.net/ai  
https://gwern.net/ai-risk  
https://gwern.net/docs/ai/  
https://gwern.net/alignment  
https://gwern.net/futurology  
https://arxiv.org/abs/1905.02175
https://arxiv.org/abs/1608.04644
https://arxiv.org/abs/1609.02943
https://arxiv.org/abs/1908.07125
https://arxiv.org/abs/2012.07805
https://arxiv.org/abs/2301.10226
https://arxiv.org/abs/1805.00899
https://openai.com/research/gpt-4
https://www.anthropic.com/research
https://www.redwoodresearch.org/blog/
https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d
https://distill.pub/2019/feature-visualization/
https://arxiv.org/abs/2003.05432
https://github.com/EleutherAI/jailbreaks
https://www.alignmentforum.org/posts/2bq7A7x9q9k4oZ7fH/deceptive-alignment
https://arxiv.org/abs/1412.6572
https://arxiv.org/abs/1602.02697
https://arxiv.org/abs/1606.06565
https://arxiv.org/abs/1805.12152
https://arxiv.org/abs/1708.06733
https://cdn.openai.com/papers/gpt-4.pdf
https://github.com/Trusted-AI/adversarial-robustness-toolbox
https://attack.mitre.org/
https://www.honeynet.org/
https://www.nist.gov/itl/ai/ai-risk-management-framework
https://deepmind.com/research/publications
https://www.centerforaisafety.org/
https://arxiv.org/abs/1607.02533
https://arxiv.org/abs/2005.14165
https://www.alignmentforum.org/posts/SHzNLtZN6YQ2u6s7p/red-teaming-and-evaluating-power-seeking-behaviors
https://www.lesswrong.com/posts/7k3j3wqkQpFf9Q7hQ/jailbreaking-alignment-and-deceptive-behavior
https://www.anthropic.com/research/constitutional-ai
https://www.redwoodresearch.org/blog/red-teaming-language-models
https://arxiv.org/abs/2301.XXXXXX
https://github.com/danielgross/llm-jailbreaks
https://arxiv.org/abs/2206.XXXXXX
https://openai.com/research/red-team
https://arxiv.org/abs/1802.00420
https://arxiv.org/abs/1708.07120
https://www.usenix.org/legacy/event/woot04/tech/provos/provos.pdf
https://openai.com/blog/red-teaming
https://redwoodresearch.org/blog/red-teaming-language-models
https://www.alignmentforum.org/tags/red-team
https://www.lesswrong.com/tag/jailbreaking
https://github.com/Anthropic/interpretability-papers
https://en.wikipedia.org/wiki/Honeypot_(computing)
https://openai.com/research
https://deepmind.com/research
https://www.alignmentforum.org/
https://www.alignmentforum.org/search?q=deceptive%20alignment
https://www.lesswrong.com/search?q=deceptive%20alignment
https://arxiv.org/search/?query=adversarial+machine+learning&searchtype=all&abstracts=show&order=-announced_date_first&size=50
https://www.alignmentforum.org/search?q=eliciting%20latent%20knowledge
https://github.com/search?q=honeypot&type=repositories
https://arxiv.org/abs/1312.6199
https://arxiv.org/abs/1712.09665
https://arxiv.org/abs/1806.11146
https://openai.com/safety
https://arxiv.org/abs/1702.07872
https://arxiv.org/abs/2002.06177
https://github.com/robustness-gym/robustness-gym
https://www.usenix.org/conference/usenixsecurity
https://arxiv.org/abs/1906.01820
https://distill.pub/2017/feature-visualization/
https://www.redwoodresearch.org/
https://www.alignmentforum.org
https://www.lesswrong.com/
https://github.com/cleverhans-lab/cleverhans
https://github.com/QData/TextAttack
https://www.anthropic.com/research/probes-catch-sleeper-agents
https://github.com/openai/evals
https://github.com/EleutherAI/lm-evaluation-harness
https://www.lesswrong.com/tag/red-team
https://www.alignmentforum.org/posts
https://arxiv.org/abs/2010.00705
https://www.caisafety.org/technical-reports
https://deepmind.com/research/publications?search=adversarial
https://arxiv.org/abs/2211.XXXXXX
https://www.alignmentforum.org/posts/xxxxx/how-to-do-red-teams
https://github.com/openai/microscope
https://arxiv.org/abs/2011.04943
https://arxiv.org/abs/1811.00636
https://openai.com/research/red-teaming
https://www.alignmentforum.org/posts/3Wb6kYj3Q7p9sQ8Gh/deceptive-alignment
https://github.com/robustness-gym/robustnessGym
https://www.anthropic.com/research/
https://microscope.openai.com/
https://github.com/tensorflow/lucid
https://arxiv.org/abs/1610.02136
https://arxiv.org/abs/1707.07328
https://arxiv.org/abs/1810.00069
https://arxiv.org/abs/1609.04802
https://arxiv.org/abs/1610.05820
https://arxiv.org/abs/1901.06796
https://arxiv.org/abs/1712.06751
https://platform.openai.com/docs/guides/safety-best-practices/prompt-injection
https://github.com/f/awesome-chatgpt-prompts
https://github.com/facebookresearch/adversarial_nli
https://arxiv.org/abs/2006.04613
https://arxiv.org/abs/1812.05271
https://arxiv.org/abs/1908.07187
https://www.alignmentforum.org/posts/8KQfZmK4s2M2k8ZxQ/risks-from-learned-optimization
https://www.lesswrong.com/posts/4tqf7b8tHzQJ8NfQY/mesa-optimization-and-subagents-a-primer
https://arxiv.org/abs/1610.07571
https://nicholas.carlini.com/writing/hidden-voice-commands.html
https://github.com/RobustnessToolkit/awesome-adversarial-ml
https://arxiv.org/abs/1703.00886
https://github.com/neelnanda-io/transformer-lens
https://incidentdatabase.ai
https://www.redwoodresearch.org/blog
https://cheatsheetseries.owasp.org/cheatsheets/Prompt_Injection.html
https://www.alignmentforum.org/posts/foP4CkBJiXfQW3W2Y/deceptive-alignment
https://arxiv.org/abs/1908.02784
https://github.com/Anthropic/constitutional-ai
https://www.alignmentforum.org/posts/qsZf6HkG9Qe6m5f2Q/deceptive-alignment-and-related-concepts
https://arxiv.org/abs/1804.06818
https://arxiv.org/abs/1802.06824
https://arxiv.org/abs/1706.06083
https://arxiv.org/abs/1903.06685
https://arxiv.org/abs/1702.06257
https://arxiv.org/abs/1810.03993
https://github.com/MadryLab/robustness
https://openai.com/security
https://arxiv.org/abs/2003.04607
https://openai.com/blog/better-language-models/
https://atlas.mitre.org/
https://robustbench.github.io/
https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-115.pdf
https://www.alignmentforum.org/search?q=red+team
https://arxiv.org/abs/2212.08073
https://platform.openai.com/docs/security
https://alignment.anthropic.com/
https://cset.georgetown.edu/research/red-teaming-ai-systems/
https://arxiv.org/abs/2302.XXXXXX
https://github.com/mitre/cti
https://github.com/elder-plinius/L1B3RT4S
https://github.com/search?q=chatgpt+jailbreak
https://incidentdatabase.ai/
https://www.lesswrong.com/tag/ai-safety
https://platform.openai.com/docs/guides/safety-best-practices
https://arxiv.org/search/?query=prompt+injection&searchtype=all
https://www.alignmentforum.org/posts/EAkN9c9sT6Z8q2gY2/eliciting-latent-knowledge
https://www.redwoodresearch.org/research/
https://deepmind.com/research/safety
https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/tramer
https://cset.georgetown.edu/research/
https://paperswithcode.com/task/adversarial-attack
https://arxiv.org/abs/1909.02845
https://www.anthropic.com/papers
https://github.com/tensorflow/cleverhans
https://github.com/robustnessgym/robustnessgym
https://www.lesswrong.com
https://www.microsoft.com/en-us/ai/responsible-ai
https://www.alignmentforum.org/posts/q8vG9g4t6gN4k6mM/eliciting-latent-knowledge-elk
https://github.com/facebookresearch/ANLI
https://github.com/robustness-gym/robustnessgym
https://arxiv.org/abs/1905.10615
https://redwoodresearch.org/blog/red-teaming-language-models/
https://arxiv.org/abs/1702.02284
https://www.alignmentforum.org/search?q=red-teaming
https://www.lesswrong.com/search?q=red-teaming
https://www.nist.gov/itl/ai-risk-management-framework
https://arxiv.org/abs/1907.11932
https://arxiv.org/abs/1702.05851
https://arxiv.org/abs/1610.08401
https://www.alignmentforum.org/posts/6rJmS3QyZpQxq7G5T/deceptive-alignment
https://www.alignmentforum.org/posts/9pQ2Yjv9wqQbP6j9N/eliciting-latent-knowledge
https://arxiv.org/pdf/2412.12480
https://arxiv.org/abs/1804.07788
https://arxiv.org/abs/1812.00139
https://www.anthropic.com/blog/red-teaming
https://www.redwoodresearch.org/blog/llm-jailbreaking-and-red-teaming
https://deepmind.com/research/highlight/robustness-and-safety
https://www.alignmentforum.org/posts/Deceptive-Alignment
https://www.lesswrong.com/posts/Prompt-Injection-And-Jailbreaking-LLMs
https://arxiv.org/pdf/2412.12480.pdf
https://www.alignmentforum.org/posts/3FhW8Zw8S3vD6kzF6/risks-from-learned-optimization
https://www.alignmentforum.org/posts/4aZy5sYw2QzaoK2bW/deceptive-alignment-mesa-optimizers
https://deepmind.com/research/areas/ai-safety
https://redwoodresearch.org/blog/red-team-evaluation
https://arxiv.org/abs/2009.08647
https://www.lesswrong.com/tag/ai-alignment
https://arxiv.org/abs/2009.01592
https://circuits.openai.com/
https://arxiv.org/abs/1609.07843
https://openai.com/research/robustness-and-safety
https://arxiv.org/search/?query=adversarial+policies+reinforcement+learning&searchtype=all&abstracts=show&order=-announced_date_first&size=50
https://arxiv.org/abs/2108.07258
https://redwoodresearch.org/
https://centerforaisafety.org
https://github.com/huggingface/transformers
https://arxiv.org/abs/1712.07107
https://arxiv.org/abs/1802.07228
https://arxiv.org/abs/1711.09883
https://arxiv.org/abs/2203.02155
https://arxiv.org/abs/1907.06723
https://www.anthropic.com/blog
https://redwoodresearch.org/blog
https://www.nist.gov/itl/ai-risk-management
https://partnershiponai.org/
https://arxiv.org/abs/1706.03741
https://github.com/Trusted-AI/AIX360
https://arxiv.org/abs/1812.01103
https://deepmind.com/research/safety-and-alignment
https://www.anthropic.com/safety
https://openai.com/research/
https://redwoodresearch.substack.com/p/misalignment-and-strategic-underperformance
https://humancompatible.ai/research
https://arxiv.org/abs/1804.03255
https://redwoodresearch.substack.com/
https://deepmind.com/research/teams/responsible-ai
https://www.anthropic.com/
https://github.com/openai/safety-gym
https://modelcards.withgoogle.com/
https://robustnessgym.org/
https://www.anthropic.com/blog/constitutional-ai
https://www.lesswrong.com/search?q=red+team
https://arxiv.org/abs/1811.03728
https://centerforaisafety.org/
https://deepmind.com/research/highlighted-research/ai-safety-research
https://www.redwoodresearch.org/publications
https://arxiv.org/abs/1812.02810
https://arxiv.org/abs/1902.02918
https://github.com/bethgelab/foolbox
https://github.com/RobustBench/robustbench
https://arxiv.org/abs/1901.03892
https://www.deepmind.com/research
https://atlas.mitre.org
https://partnershiponai.org
https://ai.google/responsibility/
https://arxiv.org/search/?query=data+poisoning&searchtype=all
https://en.wikipedia.org/wiki/Adversarial_machine_learning
https://github.com/search?q=adversarial+robustness&type=repositories
https://www.partnershiponai.org/
https://www.nist.gov/itl/ai
https://alignment.anthropic.com/2025/subliminal-learning/
https://arxiv.org/abs/1206.6389
https://aiincidentdatabase.org/
https://deepmind.com/research/highlighted-research/safety-and-alignment
https://www.centerforaisafety.org
https://arxiv.org/search/?query=data+poisoning+machine+learning&searchtype=all&abstracts=show&order=-announced_date_first&size=50
https://arxiv.org/search/?query=backdoor+attack+machine+learning&searchtype=all&abstracts=show&order=-announced_date_first&size=50
https://arxiv.org/search/?query=model+extraction+attack&searchtype=all&abstracts=show&order=-announced_date_first&size=50
https://www.redwoodresearch.org/publications/
https://intelligence.org/research/
https://github.com/mitre/advmlthreatmatrix
https://www.alignmentforum.org/posts/uXH4r6MmKPedk8rMA/gradient-hacking
https://deepmind.com/research/highlighted-research/ai-safety
https://www.alignmentforum.org/posts/Cy2XDj3i7QnZ7xJuv/risks-from-learned-optimization
https://www.lesswrong.com/tag/mesa-optimization
https://github.com/pytorch/captum
https://arxiv.org/abs/1908.01622
https://arxiv.org/abs/1911.01731
https://arxiv.org/abs/1702.01135
https://www.anthropic.com
https://deepmind.com/research/teams/ai-safety-research
https://openai.com/blog/
https://humancompatible.ai/
https://www.lesswrong.com/tag/inner-alignment
https://www.alignmentforum.org/tag/mesa-optimization
https://arxiv.org/search/?query=risks+from+learned+optimization&searchtype=all
https://arxiv.org/search/?query=model+stealing&searchtype=all
https://deepmind.com/research/teams/agent-safety-research
https://www.specificationgaming.org/
https://arxiv.org/abs/2009.11462
https://arxiv.org/pdf/2410.13787
https://arxiv.org/pdf/2501.11120
https://arxiv.org/search/?query=deceptive+alignment&searchtype=all&abstracts=show&order=-announced_date_first&size=50
https://arxiv.org/search/?query=mesa-optimization&searchtype=all&abstracts=show&order=-announced_date_first&size=50
https://arxiv.org/search/?query=instrumental+convergence&searchtype=all&abstracts=show&order=-announced_date_first&size=50
https://www.alignmentforum.org/search?q=scheming
https://www.lesswrong.com/search?q=scheming
https://github.com/search?q=mesa+optimizer&type=repositories
https://github.com/search?q=eliciting+latent+knowledge+OR+ELK&type=repositories
https://arxiv.org/abs/1902.05274
https://arxiv.org/abs/2108.05659
https://arxiv.org/abs/2205.06175
https://github.com/deepmind/ai-safety-gridworlds
https://www.anthropic.com/papers/constitutional-ai
https://arxiv.org/abs/2107.02045
https://openai.com/research/sparks-of-agi
https://www.alignmentforum.org/posts/EL7qkP8ZEZ2o6FL4T/eliciting-latent-knowledge
https://deepmind.com/publications/emergent-tool-use-from-multi-agent-autocurricula
https://deepmind.com/research/publications/muzero-mastering-go-chess-shogi-and-atari-without-rules
https://github.com/redwoodresearch/
https://arxiv.org/pdf/2206.07682.pdf
https://arxiv.org/pdf/2201.11903.pdf
https://arxiv.org/pdf/2203.02155.pdf
https://github.com/EleutherAI/lm-eval-harness
https://arxiv.org/pdf/2212.08073.pdf
https://arxiv.org/search/?query=theory+of+mind+large+language+model&searchtype=all&abstracts=show&order=-announced_date_first&size=50
https://www.alignmentforum.org/tag/schemers
https://github.com/search?q=eliciting+latent+knowledge&type=repositories
https://www.alignmentforum.org/search?q=learned%20optimization
https://arxiv.org/pdf/2410.13787.pdf
https://arxiv.org/pdf/2501.11120.pdf
https://arxiv.org/abs/2201.11903
https://github.com/sylinrl/TruthfulQA
https://www.alignmentforum.org/posts/6HkYpQq5c4q3xasr2/eliciting-latent-knowledge
https://arxiv.org/abs/2303.12712
https://www.gwern.net/Deception
https://www.lesswrong.com/tag/alignment
https://intelligence.org/publications/
https://arxiv.org/search/?query=deceptive+alignment&searchtype=all&abstracts=show&order=-announced_date_first
https://arxiv.org/search/?query=theory+of+mind+large+language+models&searchtype=all&abstracts=show&order=-announced_date_first
https://arxiv.org/abs/2412.12480
https://arxiv.org/abs/2206.07682
https://arxiv.org/search/?query=deceptive+behavior+AI&searchtype=all&source=header
https://arxiv.org/search/?query=mesa+optimization&searchtype=all&abstracts=show&order=-announced_date_first&size=50
https://www.alignmentforum.org/tag/deceptive-alignment
https://alignmentresearchcenter.org/
https://openai.com/blog/emergent-tool-use
https://www.alignmentforum.org/posts/Jg8P3X5Ym5k6oLQqK/deceptive-alignment
https://www.lesswrong.com/tag/deceptive-alignment
https://deepmind.com/blog/article/alphastar-mastering-strategy-game-starcraft-ii
https://arxiv.org/search/?query=adversarial+policies&searchtype=all
https://arxiv.org/search/?query=multi-agent+emergent&searchtype=all
https://www.alignmentforum.org/tag/scheming
https://github.com/redwoodresearch
https://www.lesswrong.com/posts/6Fh2v4r3q6v5xZrQ9/mesa-optimization-an-overview
https://deepmind.com/blog/article/reward-is-enough
https://github.com/deepmind/open_spiel
https://microscope.openai.com
https://github.com/deepmind/bsuite
https://intelligence.org/research
https://openai.com/blog/scaling-laws
https://arxiv.org/abs/2110.07516
https://github.com/Anthropic/evals
https://openai.com/research/hide-and-seek
https://openai.com/five
https://www.alignmentforum.org/search?q=deceptive
https://github.com/google/BIG-bench
https://openai.com/research/openai-five
https://distill.pub/2020/circuits/
https://openai.com/research/emergent-tool-use
https://openai.com/blog/hide-and-seek
https://deepmind.com/research/publications/alphastar-mastering-the-real-time-strategy-game-starcraft-ii
https://github.com/Anthropic/constitutionai
https://arxiv.org/abs/2011.03395
https://arxiv.org/abs/2006.04484
https://arxiv.org/abs/2109.07958
https://arxiv.org/abs/2009.01325
https://arxiv.org/search/?query=deceptive+alignment&searchtype=all
https://arxiv.org/search/?query=deceptive+alignment&searchtype=all&source=header
https://arxiv.org/search/?query=mesa+optimization&searchtype=all&source=header
https://arxiv.org/search/?query=eliciting+latent+knowledge&searchtype=all&source=header
https://www.alignmentforum.org/search?q=deceptive+alignment
https://www.alignmentforum.org/posts/3M7b3jp6JqA3Xo2Lk/risks-from-learned-optimization
https://www.alignmentforum.org/posts/4d9Ggq7Lzsa8MG8F6/eliciting-latent-knowledge-discussion
https://redwoodresearch.org/blog/
https://www.deepmind.com/research/areas/ai-safety
https://www.deepmind.com/blog
https://arxiv.org/abs/1605.05588
https://openai.com/blog/instruction-following/
https://www.alignmentforum.org/posts/3tWwEZ5sYwH8d3tqf/eliciting-latent-knowledge
https://openai.com/blog/reinforcement-learning-from-human-feedback
https://github.com/AnthropicAI
https://specificationgaming.github.io/
https://arxiv.org/abs/2009.03628
https://www.openai.com/research
https://github.com/openai
https://github.com/deepmind
https://blog.redwoodresearch.org/p/when-does-training-a-model-change
https://arxiv.org/abs/1902.03278
https://www.alignmentforum.org/users/evhub
https://www.alignmentforum.org/tag/mesa-optimizers
https://www.lesswrong.com/search?q=deceptive+alignment
https://arxiv.org/search/?query=eliciting+latent+knowledge&searchtype=all
https://arxiv.org/search/?query=mesa+optimizer+deceptive+alignment&searchtype=all
https://blog.redwoodresearch.org/
https://github.com/search?q=eliciting+latent+knowledge
https://openai.com/research/learning-to-summarize-with-human-feedback
https://www.alignmentforum.org/tag/elk-challenge
https://www.lesswrong.com/tag/mesa-optimizers
https://github.com/Anthropic
https://openai.com/research/instructgpt
https://humancompatible.ai
https://openai.com/blog/instruction-following
https://openai.com/research/learning-to-summarize
https://arxiv.org/abs/2006.05967
https://transformer-circuits.pub
https://www.alignmentforum.org/tags/mesa-optimization
https://www.anthropic.com/blog/
https://www.anthropic.com/research/alignment-faking
https://redwoodresearch.github.io/alignment_faking_examples/files/helpful_only/atn_honly/train/37.html
https://github.com/redwoodresearch/alignment_faking_examples
https://www.lesswrong.com/search?q=alignment%20faking
https://github.com/anthropic
https://arxiv.org/abs/1901.04543
https://github.com/openai/safety-starter-agents
https://openai.com/research/eliciting-latent-knowledge
https://arxiv.org/search/?query=alignment+faking&searchtype=all
https://redwoodresearch.github.io/alignment_faking_examples/
https://openai.com/research/scalable-oversight
https://arxiv.org/abs/2107.02062
https://distill.pub/2018/feature-visualization/
https://github.com/openai/eliciting-latent-knowledge
https://arxiv.org/abs/2205.11916
https://arxiv.org/search/?query=chain+of+thought&searchtype=all&source=header
https://www.circuitsbook.org/
https://www.alignmentforum.org/search?q=chain%20of%20thought
https://www.lesswrong.com/tag/interpretability
https://arxiv.org/abs/2203.11171
https://arxiv.org/abs/1907.11626
https://github.com/PAIR-code/lit
https://distill.pub/2018/building-blocks/
https://github.com/neelnanda-io/TransformerLens
https://openai.com/research/interpretability
https://deepmind.com/research/highlight/mechanistic-interpretability
https://www.anthropic.com/constitutional-ai
https://github.com/TransformerCircuits
https://www.alignmentforum.org/tag/interpretability
https://arxiv.org/abs/1802.01933
https://arxiv.org/abs/1606.04155
https://www.alignmentforum.org/topics/interpretability
https://www.alignmentforum.org/posts/7vwg3d9gCNRb9d8Lr/eliciting-latent-knowledge-a-research-agenda
https://arxiv.org/abs/2305.10601
https://arxiv.org/abs/1606.03490
https://transformer-circuits.pub/
https://github.com/neelnanda-io/transformer-circuits
https://deepmind.com/blog/article/agent-foundations
https://arxiv.org/abs/1810.08062
https://arxiv.org/abs/2504.10374
https://www.alignmentforum.org/posts/JvYF5kosLeYGvvLpP/evaluating-and-monitoring-for-ai-scheming
https://deepmind.com/research/areas/ai-safety-and-alignment
https://distill.pub/
https://github.com/SeldonIO/alibi-detect
https://www.deepmind.com/safety-and-society
https://arxiv.org/abs/2302.04392
https://github.com/CSAILVision/netdissect
https://openai.com/evals
https://eliciting-latent-knowledge.github.io/
https://arxiv.org/abs/2009.06776
https://github.com/evidentlyai/evidently
https://platform.openai.com/docs/guides/moderation
https://gltr.io/
https://arxiv.org/search/?query=monitoring+language+models&searchtype=all
https://deepmind.com/blog/article/ai-safety-research
https://arxiv.org/abs/1706.02690
https://arxiv.org/abs/1706.04599
https://github.com/whylabs/whylogs
https://deepmind.com/research/areas/safety
https://whylabs.ai/
https://platform.openai.com/docs/models/gpt-4
https://research.google/pubs/pub43146/
https://arxiv.org/search/?query=monitoring+anomaly+detection+language+models&searchtype=all&source=header
https://arxiv.org/abs/1205.3452
https://openai.com/blog/red-team-gpt-4
https://www.anthropic.com/papers/constitutional_ai
https://arxiv.org/abs/1907.07616
https://www.alignmentforum.org/posts/2f2wsjWqS8n8Y9wqG/eliciting-latent-knowledge
https://deepmind.com/research/highlighted-research
https://arxiv.org/abs/1808.10307
https://deepmind.com/blog/article/sparrow
https://www.alignmentforum.org/search?q=monitor
https://arxiv.org/abs/2003.08997
https://github.com/mitmedialab/ai-detect
https://arxiv.org/abs/2204.05862
https://arxiv.org/abs/1906.04043
https://openai.com/policies/usage-policies
https://www.redwoodresearch.org/blog/automated-oversight/
https://deepmind.com/research/teams/safety-and-alignment
https://www.alignmentforum.org/tags/tripwires
https://www.lesswrong.com/tag/tripwires
https://papers.nips.cc/paper/2015/file/86b6d2bdfb04c3d8d3f4b0d6a2c5a1d4-Paper.pdf
https://csrc.nist.gov/publications/detail/sp/800-207/final
https://openai.com/charter
https://github.com/prometheus/prometheus
https://github.com/elastic/elasticsearch
https://github.com/Netflix/security_monkey
https://sre.google/books/
https://platform.openai.com/docs/guides/security
https://github.com/falcosecurity/falco
https://osquery.io
https://slsa.dev
https://cloud.google.com/logging/docs/audit
https://aws.amazon.com/cloudtrail/
https://kubernetes.io/docs/tasks/debug-application-cluster/audit/
https://github.com/zeek/zeek
https://github.com/osquery/osquery
https://github.com/NVIDIA/gpu-monitoring-tools
https://github.com/google/grr
https://deepmind.com/our-research/areas-of-research/ai-safety
https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf
https://opentitan.org
https://software.intel.com/en-us/sgx
https://github.com/google/ml-metadata
https://sigstore.dev
https://github.com/ossf/scorecard
https://arxiv.org/abs/1806.03287
https://www.alignmentforum.org/tags/operations
https://deepmind.com/research/highlighted-research/safety
https://github.com/OISF/suricata
https://github.com/wazuh/wazuh
https://openai.com/blog/ai-and-compute/
https://arxiv.org/search/?query=compute+auditing&searchtype=all
https://arxiv.org/search/?query=model+monitoring&searchtype=all
https://prometheus.io/
https://grafana.com/
https://wazuh.com/
https://csrc.nist.gov/publications/detail/sp/800-137/final
https://openai.com/research/ai-and-compute
https://www.lesswrong.com/tag/security
https://zeek.org/
https://suricata-ids.org/
https://osquery.io/
https://github.com/elastic/beats
https://arxiv.org/search/?query=honeypot&searchtype=all&abstracts=show&order=-announced_date_first&size=50
https://www.alignmentforum.org/search?q=monitoring
https://www.lesswrong.com/search?q=security
https://openai.com/blog/red-teaming-language-models
https://deepmind.com/blog
https://github.com/cowrie/cowrie
https://www.sans.org/white-papers/incident-handlers-handbook/
https://www.ncsc.gov.uk/guidance/incident-management
https://sre.google/sre-book/incident-response/
https://arxiv.org/search/?query=honeypot+cybersecurity&searchtype=all
https://arxiv.org/search/?query=incident+response+machine+learning&searchtype=all
https://arxiv.org/search/?query=red+teaming+language+models&searchtype=all
https://www.alignmentforum.org/search?q=security
https://www.lesswrong.com/search?q=opsec
https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-61r2.pdf
https://www.sans.org/reading-room/whitepapers/incident/incident-handlers-handbook-33901/
https://owasp.org/www-project-machine-learning-security/
https://github.com/partnershiponai/aiid-source
https://owasp.org/www-project-top-ten/
https://github.com/TheHive-Project/TheHive
https://github.com/MISP/MISP
https://cloud.google.com/security/incident-response
https://www.cisa.gov/uscert
http://www.honeynet.org/book/
https://github.com/mushorg/conpot
https://canarytokens.org/
https://suricata.io/
https://github.com/SigmaHQ/sigma
https://github.com/thinkst/opencanary
https://arxiv.org/abs/1709.01604
https://www.cisa.gov/stopransomware
https://csrc.nist.gov/publications/detail/sp/800-61/rev-2/final
https://owasp.org/www-project-machine-learning-security-project/
https://github.com/cuckoosandbox/cuckoo
https://blog.redwoodresearch.org/p/misalignment-and-strategic-underperformance
https://alignment.anthropic.com/2025/subliminal-learning/
https://arxiv.org/abs/2507.14805
https://www.lesswrong.com/posts/cGcwQDKAKbQ68BGuR/subliminal-learning-llms-transmit-behavioral-traits-via
https://arxiv.org/abs/2401.05566
https://www.alignmentforum.org/posts/ZAsJv7xijKTfZkMtr/sleeper-agents-training-deceptive-llms-that-persist-through
https://www.anthropic.com/research/probes-catch-sleeper-agents
https://www.lesswrong.com/posts/gknc6NWCNuTCe8ekp/simple-probes-can-catch-sleeper-agents-1
https://www.anthropic.com/research/alignment-faking
https://arxiv.org/abs/2412.14093
https://arxiv.org/abs/2505.01420
https://arxiv.org/pdf/2505.01420
https://arxiv.org/abs/2501.11120
https://openreview.net/forum?id=IjQ2Jtemzy
https://martins1612.github.io/selfaware_paper_betley.pdf
https://www.lesswrong.com/posts/xrv2fNJtqabN3h6Aj/tell-me-about-yourself-llms-are-aware-of-their-learned
https://arxiv.org/abs/2310.18512
https://www.lesswrong.com/posts/9Fdd9N7Escg3tcymb/preventing-language-models-from-hiding-their-reasoning
https://github.com/redwoodresearch/Text-Steganography-Benchmark
https://www.alignmentforum.org/posts/kSmHMoaLKGcGgyWzs/big-bench-canary-contamination-in-gpt-4
https://www.lesswrong.com/posts/kSmHMoaLKGcGgyWzs/big-bench-canary-contamination-in-gpt-4
https://github.com/google/BIG-bench
https://www.alignment.org/canary/
https://blog.redwoodresearch.org/p/notes-on-countermeasures-for-exploration
https://blog.redwoodresearch.org/p/guide
https://redwoodresearch.substack.com/archive?sort=top
https://metr.org/blog/2023-03-18-update-on-recent-evals/
https://www.lesswrong.com/posts/4Gt42jX7RiaNaxCwP/more-information-about-the-dangerous-capability-evaluations
https://arxiv.org/html/2505.18102v1
https://openreview.net/pdf/1aed6a1f0bdae7e835ec842e0e3265a11edc2bb6.pdf
https://www.alignmentforum.org/posts/wG5KTj5jFiibydgmk/arc-paper-formalizing-the-presumption-of-independence
https://www.lesswrong.com/w/sandbagging-ai?lens=main
https://www.lesswrong.com/posts/G8WwLmcGFa4H6Ld9d/an-overview-of-control-measures

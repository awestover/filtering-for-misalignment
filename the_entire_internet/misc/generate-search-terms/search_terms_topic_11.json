{
  "topic_index": 11,
  "topic_text": "2. **The *results* of empirical evaluations of AI\u2019s scheming-relevant capabilities.** For example: Information about how [training impacts AI goals](https://blog.redwoodresearch.org/p/when-does-training-a-model-change).",
  "model": "gpt-5-mini",
  "search_terms": [
    "empirical experiments on mesa-optimizer emergence in deep RL",
    "deceptive alignment behavioral tests and experimental results",
    "inner alignment empirical studies training dynamics of objectives",
    "goal-directedness probing in large language models experimental",
    "effects of pretraining and fine-tuning on model objectives empirical",
    "causal mediation analysis of model objectives and representations",
    "interventionist probing for mesa-objectives detection experiments",
    "reward tampering and reward hacking empirical evaluations",
    "instrumental convergence experiments in neural networks",
    "emergent self-preservation behaviors in trained agents study",
    "strategic planning capabilities benchmarks and experimental results",
    "mechanistic interpretability case studies on objective formation",
    "gradient-descent shaping of agent goals empirical evidence",
    "RLHF effects on goal alignment experimental results",
    "training-game-theoretic analyses of deployment-time incentives",
    "behavioral assays for deceptive strategies in LLMs",
    "reward misspecification empirical case studies in alignment",
    "scaling laws for misalignment and emergent scheming capabilities",
    "representational similarity analysis (RSA) for goal representations",
    "adversarial training against strategic behavior empirical results",
    "empirical evaluation of deceptive alignment in language models",
    "experiments measuring emergence of mesa-optimizers in deep neural networks",
    "training dynamics and goal-content integrity (GCI) in transformer models",
    "does gradient descent change model goals empirical evidence",
    "instrumental convergence incentives in RL agents empirical study",
    "reward tampering, wireheading, and specification gaming experiments (RLHF/PPO)",
    "probing for goal-directed representations in pretrained models linear probes",
    "causal tracing and activation patching for scheming detection",
    "behavioral tests for long-term planning, deception, and horizon-aware incentives",
    "off-policy evaluation of deceptive policies in multi-agent environments",
    "impact of fine-tuning and SFT on internal objectives and utility functions",
    "evidence for deceptive strategies and reward hacking in Atari/continuous-control agents",
    "scaling laws for emergent goal-directed behavior and deception",
    "adversarial training and robustness interventions against instrumental goals",
    "case studies of inner-alignment failures and empirical measurements",
    "robustness of model goals under distributional shift and continual learning",
    "meta-learning, mesa-optimization, and incentive formation empirical investigations",
    "benchmarks and datasets for detecting reward hacking, specification gaming, and deceptive policies",
    "mechanistic interpretability linking representations to reward-seeking behavior (feature attribution, circuit-level analysis)",
    "experimental tests of self-preservation, resource-seeking, and escape incentives in deployed agents",
    "empirical evaluations of scheming in neural networks",
    "mesa-optimizer emergence experiments",
    "deceptive alignment empirical studies",
    "inner alignment and goal misgeneralization experiments",
    "training impacts on model goals longitudinal studies",
    "probing goal representations in transformers",
    "causal mediation analysis for model objectives",
    "behavioral signatures of deception in language models",
    "instrumental convergence empirical evidence",
    "fine-tuning and goal shift effects",
    "RLHF and emergence of instrumentality",
    "policy gradient dynamics and mesa-optimization",
    "mechanistic interpretability of goal-directed behavior",
    "adversarial tests for deceptive strategies in models",
    "benchmarks for detecting scheming and deception",
    "scaling laws and emergence of scheming capabilities",
    "latent space interventions to modify goals",
    "reward modeling stability and goal persistence",
    "long-horizon planning behavior evaluation in LLMs",
    "goal attribution and model introspection methods",
    "empirical studies of mesa-optimization in deep neural networks",
    "training-induced goal formation gradient descent experiments",
    "deceptive alignment behavioral tests situational awareness probes",
    "inner alignment failure case studies reward misspecification",
    "instrumental convergence empirical evidence long-term planning models",
    "reward hacking and goal misgeneralization experiments in reinforcement learning",
    "RLHF effects on strategic deception and opportunistic behavior",
    "mechanistic interpretability of goal-directed circuitry in transformers",
    "selection pressure for long-term objectives during model training",
    "benchmarks for detecting scheming in language models",
    "probing classifiers for situational awareness and intent detection",
    "causal mediation analysis of learned objectives in neural networks",
    "adversarial training to mitigate deceptive alignment empirical studies",
    "partially observed reward environments and emergent deception",
    "behavioral proxies for instrumental goals in pretrained models",
    "fine-tuning dynamics and shifts in model utility functions",
    "experimental setups for measuring a model's willingness to deceive",
    "gradient descent as optimizer-of-optimizers mesa-optimizer emergence",
    "open-ended goal pursuit measurement in large-scale RL agents",
    "case studies of goal misgeneralization during staged training regimes",
    "empirical studies of deceptive alignment and scheming behavior in deep RL",
    "mesa-optimizer goal formation empirical evaluation",
    "training dynamics and changes in model goals during fine-tuning",
    "when does training a model change its goals",
    "reward-on-the-episode incentives empirical experiments",
    "measuring instrumental convergence in trained agents",
    "goal misgeneralization experiments in large language models",
    "behavioral assays for detecting deceptive strategies in neural networks",
    "representation probes for latent reward-seeking objectives",
    "mechanistic interpretability of mesa-optimizers and learned objectives",
    "adversarial evaluation of long-term planning and scheming",
    "effects of pretraining, fine-tuning, and RLHF on agent objectives",
    "optimization pressure and proxy alignment during training",
    "toy model studies of scheming and inner alignment failure modes",
    "benchmarks and metrics for detecting reward hacking and goal-directed deception",
    "fine-tuning-induced objective shifts: empirical evidence and case studies",
    "training-time incentives for reward tampering and instrumental behavior experiments",
    "measurement of emergent objectives and capability overhang in scaled models",
    "causal interventions and ablations on learned goals in neural agents",
    "parameter-space and checkpoint analyses of goal stability across training",
    "empirical evaluation of mesa-optimizers in deep RL",
    "training dynamics and goal misgeneralization in language models",
    "experiments on inner alignment and latent goal formation",
    "measuring instrumental convergence and power-seeking behavior",
    "eliciting latent objectives via behavioral probing and activation patching",
    "effects of fine-tuning on agent objectives and goal drift",
    "reward hacking and reward gaming empirical studies",
    "situational awareness emergence during model training",
    "causal mediation analysis of goal-directed representations",
    "neuron-level interventions to alter model motivations",
    "analysis of objective function shift under SGD and gradient updates",
    "empirical tests for scheming agents in simulated environments",
    "probing methods for latent goals and motivation inference",
    "adversarial training to prevent reward corruption and deception",
    "concept erasure and model editing to remove undesired goals",
    "policy-gradient goal drift and preference-change experiments",
    "behavioral fingerprints of schemers in language and RL agents",
    "meta-learning and emergence of instrumental subgoals",
    "open-ended training regimes and formation of mesa-optimizers",
    "benchmarking inner alignment: datasets, tasks, and metrics",
    "empirical studies of scheming and deceptive alignment in deep reinforcement learning",
    "deceptive alignment behavioral assays for language models",
    "mesa-optimization detection experiments in neural networks",
    "empirical evidence of inner alignment failures",
    "goal misgeneralization benchmarks for transformer models",
    "training dynamics and goal formation in deep networks",
    "measuring instrumental convergence in trained policies",
    "reward hacking and reward tampering empirical studies",
    "fine-tuning effects on emergent goals and model intentions",
    "scaling laws impact on goal stability and alignment",
    "circuit-level signatures of goal-directedness in models",
    "probing for intent: activation probes and mechanistic interpretability",
    "adversarial training experiments to mitigate deceptive strategies",
    "causal intervention analysis of training-induced goal changes",
    "counterfactual data augmentation tests for goal misgeneralization",
    "benchmarks and evaluation suites for scheming-relevant capabilities",
    "iterated amplification empirical evaluations and inner alignment",
    "policy-gradient training and emergence of instrumental goals",
    "measuring long-term planning and covert instrumental behavior in agents",
    "capability-alignment tradeoffs empirical research and case studies",
    "empirical evaluation of mesa-optimization in deep RL",
    "goal misgeneralization experiments in supervised transformers",
    "training-induced goal modification and objective drift",
    "inner alignment empirical benchmarks and assays",
    "scheming behavior emergence in long-horizon agents",
    "instrumental convergence experiments reward hacking",
    "deceptive alignment detection behavioral tests",
    "representational probes for model goal representations",
    "causal tracing interventions on model objectives",
    "adversarial training to suppress strategic deception",
    "motivation attribution in large language models",
    "scaling laws for emergence of planning and scheming",
    "reward tampering empirical studies in RL agents",
    "unit-level evidence for agentic goals in transformers",
    "open-ended optimization and unintended mesa-optimizers",
    "interpretability methods for inner optimizer identification",
    "long-horizon incentives and emergent instrumental behaviour",
    "empirical benchmarks for deceptive alignment robustness",
    "when does training a model change its goals empirical",
    "behavioral assays and probes for strategic deception in models",
    "mesa-optimizer emergence empirical study",
    "inner alignment empirical results",
    "goal misgeneralization experiments",
    "training dynamics and goal formation in transformers",
    "behavioral assays for deceptive alignment",
    "probing methods for model objectives",
    "interpretability techniques for attributing model goals",
    "effects of RLHF on model objectives and motives",
    "fine-tuning impact on model goals and incentive structure",
    "reward misspecification leading to scheming behavior",
    "long-horizon planning and proxy goals in reinforcement learning",
    "mesa-optimizer detection metrics and benchmarks",
    "modeling training impact on agent goals when does training change goals",
    "adversarial training to detect or prevent deceptive strategies",
    "scaling laws for emergent misalignment and motives",
    "policy gradient induced goal shifts empirical study",
    "instrumental strategies in language models empirical analysis",
    "measuring goal stability across checkpoints and interventions",
    "mesa-optimization empirical evaluation",
    "inner alignment empirical studies",
    "goal misgeneralization experiments in deep networks",
    "training dynamics and goal formation in neural language models",
    "instrumental convergence empirical evidence in RL",
    "specification gaming empirical analysis",
    "reward hacking observed during training",
    "emergent goal-directed behavior in LLMs and RL agents",
    "scheming behavior detection and metrics for agents",
    "fine-tuning effects on latent model objectives",
    "causal tracing of goal representations in transformers",
    "representational drift and objective change during training",
    "loss landscape analysis and goal-directedness",
    "scaling laws and emergence of optimization power",
    "behavioral bifurcation and phase transitions during training",
    "adversarial training to mitigate deceptive alignment",
    "proxy alignment, selective amplification, and reward proxies",
    "mechanistic interpretability for intent and goal inference",
    "benchmarks and tests for instrumental/strategic behavior",
    "deceptive alignment empirical tests and case studies",
    "empirical evaluation of scheming in neural networks",
    "mesa-optimizer detection experiments",
    "training dynamics and goal change in deep models",
    "\"When does training a model change its goals\"",
    "inner alignment empirical measurements",
    "measuring goal-directedness in RL agents",
    "power-seeking behavior in trained agents",
    "reward hacking and training pressure studies",
    "selection pressure, SGD dynamics, and emergent objectives",
    "fine-tuning induced objective drift",
    "adversarial training to mitigate deceptive behavior",
    "causal tracing activation patching for goal attribution",
    "mechanistic interpretability applied to schemers",
    "probing models for proxy objectives and heuristics",
    "distributional shift effects on agent goals",
    "meta-learning and goal formation empirical analysis",
    "benchmarks and metrics for inner alignment and deceptive behavior",
    "empirical evaluation of schemer emergence in deep RL",
    "mesa-optimizer experimental studies in neural networks",
    "deceptive alignment detection benchmarks for LLMs",
    "training dynamics and goal stability in transformer models",
    "probing latent goal representations in language model activations",
    "instrumental convergence empirical evidence in pretrained models",
    "inner alignment measurement methods for learned objectives",
    "reward hacking / reward gaming experiments in deep reinforcement learning",
    "effects of fine-tuning and RLHF on agent objectives and goals",
    "mechanistic interpretability of mesa-optimizers and goal-directedness",
    "behavioral indicators and tests for scheming in large language models",
    "causal mediation analysis of objective drift during training",
    "representation ablation and lesion studies for final goals",
    "adversarial training to surface deceptive strategies",
    "scaling laws for emergence of deceptive behaviors in models",
    "counterfactual reward tests for instrumental behavior",
    "policy-gradient case studies of reward hacking and deception",
    "off-distribution generalization experiments and goal change",
    "benchmarks and metrics for deceptive alignment detection and evaluation",
    "empirical comparisons of outer alignment vs inner alignment failure modes",
    "empirical evaluation of mesa-optimization in deep neural networks",
    "scheming behavior emergence in reinforcement learning agents experimental results",
    "training-induced objective shift / goal drift in large models",
    "goal misgeneralization experiments and quantitative findings",
    "deceptive alignment empirical studies and case analyses",
    "instrumental convergence evidence in language models and RL agents",
    "probing latent objectives via counterfactual interventions and ablations",
    "behavioral vs mechanistic indicators of mesa-optimizers",
    "impact of SGD dynamics and optimizer hyperparameters on model goals",
    "RLHF effects on long-term goal pursuit, deception, and incentive shaping",
    "reward hacking and reward tampering empirical case studies",
    "measures and metrics for detecting model intent and strategic behavior",
    "inner alignment empirical results: gradient incentives and objective pressure",
    "adversarial training and red-team protocols to elicit scheming",
    "distributional shift induced goal-directed misgeneralization experiments",
    "neuron-level and circuit-level evidence for instrumental subgoals",
    "benchmarks and behavioral tests for long-horizon planning in transformers",
    "causal tracing and mechanistic attribution of learned objectives",
    "multi-agent training evidence for strategic manipulation and collusion",
    "scaling laws, model size, and emergence of scheming-relevant capabilities",
    "empirical evaluation of mesa-optimization and scheming in deep neural networks",
    "training-induced goal shifts in transformer language models",
    "goal misgeneralization experiments and results",
    "deceptive alignment empirical studies and benchmarks",
    "instrumental convergence behavior in reinforcement learning agents",
    "measuring goal-directedness in deep RL: metrics and experimental methods",
    "probing classifiers for value-like representations in LLMs",
    "mechanistic interpretability activation patching for latent goals",
    "causal interventions to test objective stability during training",
    "fine-tuning effects on agent objectives and motivation learning",
    "representational similarity analysis of reward signals in neural networks",
    "adversarial training to prevent strategic deception in agents",
    "counterfactual training experiments for goal plasticity",
    "SGD dynamics and the emergence of optimized subgoals (mesa-optimizers)",
    "behavioral proxies for schemer detection: longitudinal experiments",
    "influence functions and tracing training data impact on agent goals",
    "latent goal inference in transformers via probing and intervention",
    "experimental measurements of planning horizon and long-term instrumental behavior",
    "benchmarks for deceptive planning and strategic behavior in simulated environments",
    "empirical studies on training distribution shift effects on agent objectives",
    "empirical evaluation of mesa-optimizer emergence in neural networks",
    "training dynamics and changes in latent objectives in large language models",
    "empirical inner alignment studies deceptive alignment experiments",
    "goal misgeneralization empirical results in deep reinforcement learning",
    "instrumental convergence empirical analysis in trained agents",
    "reward hacking reward\u2011gaming empirical papers and benchmarks",
    "effects of supervised fine\u2011tuning and RLHF on model objectives",
    "probing situational awareness and agent goals in transformers",
    "causal mediation analysis of internal objectives in neural agents",
    "linear probes for latent goal representations in language models",
    "mechanistic interpretability evidence for schemer\u2011like strategies",
    "behavioral assays for deception and strategic gaming in LLMs",
    "training\u2011induced objective shift empirical measurements",
    "scaling laws for emergence of deceptive or goal\u2011directed behavior",
    "adversarial training impact on scheming robustness",
    "toy environment experiments on mesa\u2011optimization and scheming",
    "policy gradient pressure on instrumental goals empirical study",
    "empirical studies of ambition and power\u2011seeking in AI agents",
    "distribution shift effects on goal\u2011directed behavior generalization",
    "benchmarking inner alignment: datasets methods and empirical results",
    "empirical evaluation of deceptive alignment in large language models",
    "mesa-optimizer detection experiments training dynamics inner alignment",
    "measuring goal-directedness and instrumental convergence in pretrained transformers",
    "training impacts on model objectives goal misgeneralization empirical study",
    "reward-seeking behavior assays for RL agents power-seeking empirical results",
    "effect of fine-tuning and RLHF on model goals and preferences",
    "probing representations for reward-on-the-episode mesa-optimization signatures",
    "mechanistic interpretability evidence for mesa-optimizers causal tracing",
    "loss landscape and gradient descent pressure toward goal preservation",
    "capability gain-of-function and alignment failure empirical measurements",
    "adversarial alignment experiments changing objectives via model editing",
    "behavioral tests for covert goal-preservation deceptive strategies in models",
    "inner alignment empirical benchmarks synthetic environments",
    "multi-agent training and emergence of power-seeking instrumental incentives",
    "statistical evidence for objective fragility under distribution shift",
    "measuring incentive gradients toward deception during training",
    "representational clustering methods to detect subgoals in neural networks",
    "evolutionary game theory analogies for training-time selection of goals",
    "empirical studies on pretraining vs fine-tuning effects on model goals",
    "case studies when does training change a model's utility function",
    "empirical evaluation of scheming in neural agents",
    "mesa-optimizer emergence under SGD training dynamics",
    "deceptive alignment experiments reward-on-the-episode",
    "inner alignment empirical studies goal misgeneralization",
    "measuring situational awareness in RL agents",
    "instrumental convergence behaviors in deep RL",
    "reward hacking empirical benchmarks and case studies",
    "training-induced objective shift stability of learned goals",
    "probing classifiers for agent objectives and motives",
    "mechanistic interpretability for inferring mesa-objectives",
    "activation patching and causal mediation analysis for goals",
    "behavioral signatures of deception and game-theoretic scheming",
    "adversarial evaluation of deceptive strategies in policies",
    "empirical metrics for goal-directedness in transformers",
    "lesion studies and neuron ablation for objective localization",
    "distributional shift effects on agent goal preservation",
    "fine-tuning impact on pretrained model goals and preferences",
    "benchmarks for deceptive alignment and inner-optimizer detection",
    "counterfactual interventions to test agent intent and motives",
    "measurement of long-term planning and reward-seeking instrumental actions",
    "empirical evaluation of scheming in deep RL agents",
    "mesa-optimization detection experiments",
    "goal misgeneralization empirical study",
    "inner alignment tests for transformer language models",
    "probing for latent objectives in neural networks",
    "training-induced goal shifts in pretrained models",
    "effects of fine-tuning and RLHF on agent objectives",
    "instrumental convergence experimental evidence",
    "reward hacking behavioral benchmarks",
    "adversarial training to reveal schemer policies",
    "mechanistic interpretability of goal-directed representations",
    "policy gradient influence on learned objectives",
    "transfer learning and preservation of agent goals",
    "behavioral assays for covert agentic behavior",
    "proxy reward pressure and optimization pressure experiments",
    "probing classifiers for mesa-objective alignment",
    "scaling laws for emergence of instrumental strategies",
    "empirical methods for detecting deceptive alignment",
    "causal interventions on model goals during training",
    "benchmarks for testing intentionality in RL agents",
    "mesa-optimizer formation in deep reinforcement learning",
    "training impacts on model goals experimental results",
    "goal misgeneralization experiments in transformers",
    "instrumental convergence emergence in RL agents",
    "behavioral assays for goal-directedness and scheming",
    "probing hidden objectives in pretrained models",
    "gradient descent as optimizer-of-optimizers empirical",
    "policy gradient dynamics producing deceptive strategies",
    "reward hacking and reward tampering empirical evidence",
    "mechanistic interpretability of goal representations",
    "counterfactual training interventions to change objectives",
    "ablation studies on inner alignment failures",
    "transfer of learned objectives across tasks experiments",
    "scaling laws for scheming-relevant capabilities",
    "adversarial training effects on deceptive behavior",
    "representational similarity analysis of optimization targets",
    "synthetic environments testing mesa-optimization emergence",
    "benchmarks and metrics for detecting deceptive alignment",
    "empirical evaluation of deceptive alignment in deep RL agents",
    "mesa-optimizer emergence training dynamics SGD selection pressure",
    "measuring situational awareness in large language models probing tasks",
    "goal misgeneralization experiments in supervised and RL settings",
    "instrumental convergence empirical studies power-seeking behavior",
    "reward hacking and proxy objective evaluation in deep networks",
    "effects of fine-tuning and RLHF on model goal stability",
    "detection methods for scheming mesa-optimizers behavioral probes",
    "causal tracing and mechanistic interpretability for goal representations",
    "representational similarity analysis of objective-like features",
    "adversarial training to reduce deceptive strategies empirical results",
    "selection pressure during pretraining on agentic optimization",
    "benchmarking scheming-relevant capabilities in transformer models",
    "meta-learning and hierarchical RL induced agentic behavior",
    "scaling laws for emergence of goal-directed capabilities",
    "interventional probing of model goals via reward perturbations",
    "policy gradient vs supervised training effects on goal-directedness",
    "empirical studies of situational awareness leading to deception",
    "behavioral indicators and metrics for deceptive alignment",
    "fine-grained layer-wise changes in objectives during training",
    "empirical evaluations of deceptive alignment in neural networks",
    "mesa-optimization empirical studies training dynamics",
    "when does training change model goals",
    "inner alignment experimental benchmarks for scheming",
    "goal misgeneralization experiments deep learning",
    "emergence of instrumentally convergent behavior in RL",
    "deceptive policies detection behavioral tests",
    "measuring mesa-objectives via probing and interventions",
    "training pressure toward power-seeking objectives evidence",
    "adversarial evaluation of goal-directedness in LLMs",
    "causal interventions on goal formation during training",
    "fine-tuning effects on agent objectives and incentives",
    "situational awareness emergence in transformer models",
    "reward hacking vs scheming empirical comparisons",
    "scaling laws for goal-directed behavior in deep models",
    "toy gridworld deceptive agent experimental results",
    "representational evidence for intent-like features",
    "gradient descent selection for mesa-optimizers empirical",
    "RLHF impact on deceptive alignment and scheming",
    "behavioral proxies and tests for instrumentally motivated actions",
    "empirical evidence for scheming in neural networks",
    "inner alignment failure case studies",
    "training impacts on model goals pretraining fine-tuning drift",
    "goal misgeneralization benchmarks and experiments",
    "instrumental convergence empirical measurement",
    "power-seeking behavior in RL agents experiments",
    "reward gaming versus deceptive policy evaluation",
    "behavioral assays for detecting scheming in LLMs",
    "interpretability probes for latent objectives",
    "circuit-level evidence of goal-directed behavior",
    "latent reward function identification in deep networks",
    "objective function drift during training empirical studies",
    "tests for deception and strategic behavior in learned policies",
    "policy-gradient deceptive strategies empirical work",
    "effects of RLHF on model objectives and incentives",
    "toy-model demonstrations of mesa-optimization and schemers",
    "benchmarks and datasets for deceptive alignment detection",
    "case studies of emergent misaligned objectives in reinforcement learning",
    "deceptive alignment experiments mesa-optimizer detection",
    "training impacts AI goals \"when does training a model change\"",
    "inner alignment empirical studies goal-directedness probes",
    "goal misgeneralization experimental results reinforcement learning",
    "instrumental convergence behavioral assays neural agents",
    "reward hacking and reward gaming empirical measurements",
    "situational awareness probes adversarial evaluation LLMs",
    "training dynamics and objective robustness in deep RL",
    "probing for long-term planning and hierarchical goals in agents",
    "counterfactual interventions to test AI goal changes",
    "causal mediation analysis of learned goals in neural networks",
    "mechanistic interpretability evidence for mesa-optimization",
    "distributional shift effects on learned objectives experiments",
    "RLHF impact on agent objectives empirical studies",
    "behavioral fingerprints of deceptive strategies in models",
    "adversarial training to detect strategic behavior in RL agents",
    "meta-learning emergence of instrumental subgoals empirical",
    "benchmarks for measuring scheming-relevant capabilities",
    "scaling laws and emergence of strategic deception in large models",
    "empirical evaluation of deceptive alignment and scheming in neural networks",
    "mesa-optimizer detection experiments training-induced goal formation",
    "studies on goal misgeneralization and goal persistence after fine-tuning",
    "behavioral assays for strategic deception and reward-on-the-line behavior",
    "training dynamics and SGD selection pressure on agentic goals",
    "instrumental convergence empirical evidence in deep RL agents",
    "learned optimizer experiments showing internal goal formation (meta-learning)",
    "activation patching and causal mediation analysis for scheming capabilities",
    "probing representations for long-term planning and model intent",
    "intervention studies on goal content stability during pretraining and fine-tuning",
    "reward tampering and wireheading empirical studies in RLHF systems",
    "scaling laws for emergent strategic behavior and deceptive capabilities",
    "adversarial training to prevent deceptive strategies in language models",
    "mechanistic interpretability of mesa-optimization and inner-alignment failures",
    "transfer learning effects on agentic behavior and goal retention",
    "benchmarks and datasets for detecting model strategic behavior and deception",
    "fine-tuning trajectories that induce or suppress schemer-like policies",
    "experimental evidence for incentive-on-the-line (reward-on-the-line) scheming",
    "unit tests and behavioral fingerprints for instrumental strategies in transformer-based agents",
    "causal tracing and influence functions for locating scheming-relevant circuits",
    "mesa-optimizer detection experiments reward-on-the-episode",
    "behavioral assays for deceptive alignment and instrumental deception",
    "training dynamics of internal objectives and goal stability",
    "gradient descent selection for mesa-optimizers empirical studies",
    "fine-tuning effects on model goals and goal drift experiments",
    "probing situational awareness and long-horizon planning in LLMs",
    "adversarial training impact on scheming-relevant capabilities",
    "reward hacking versus deliberate deception empirical comparison",
    "linear probes and feature attribution for policy goal representations",
    "experimental tests for instrumental convergence in reinforcement learning",
    "retrospective reward shaping and emergent goal-directedness",
    "model surgery and ablation interventions to test internal objectives",
    "behavioral indicators of goal-directed mesa-optimization",
    "open-ended training selection pressure and inner-alignment outcomes",
    "benchmarks for deception, manipulation, and goal preservation",
    "policy-gradient emergence of deceptive strategies experiments",
    "chain-of-thought prompting effects on model deception tendencies",
    "outer vs inner alignment empirical results reproducibility studies",
    "measuring goal-directedness in transformer models experiments",
    "mesa-optimizer empirical evidence training dynamics",
    "deceptive alignment experimental results in RL agents",
    "how training changes model goals empirical study",
    "goal misgeneralization experiments in deep reinforcement learning",
    "instrumental convergence empirical tests for power-seeking behavior",
    "reward gaming versus schemer behavior experimental comparison",
    "effects of fine-tuning and RLHF on model objectives",
    "probing situational awareness in language models with activation patching",
    "causal mediation analysis of emergent objectives in transformers",
    "selection pressure of SGD on inner alignment empirical analyses",
    "loss landscape and emergence of proxy objectives empirical study",
    "benchmarks and evaluation suites for detecting deceptive strategies",
    "inverse reinforcement learning to infer latent goals in trained models",
    "behavioral indicators of deception and instrumental strategies in agents",
    "toy-environment tests for scheming and instrumental convergence",
    "scaling laws and emergence of power-seeking abilities in large models",
    "model editing experiments and their impact on discovered goals",
    "empirical evaluation of mesa-optimizers in deep neural networks",
    "training dynamics and goal misgeneralization empirical studies",
    "deceptive alignment behavioral signatures in large language models",
    "measuring situational awareness and instrumental goals in RL agents",
    "instrumental convergence experiments power-seeking behavior",
    "causal mediation analysis for latent objectives in neural networks",
    "activation patching and causal tracing to detect scheming",
    "probing classifiers for inner alignment and model goals",
    "effects of fine-tuning and RLHF on goal-directed behavior",
    "loss landscape, objective drift, and emergent goals during training",
    "benchmarks and adversarial evaluations for deceptive behavior",
    "toy environment studies of mesa-optimization emergence",
    "behavioral assays for reward hacking and instrumental strategies",
    "gradient descent selection for goal-directed mesa-optimizers",
    "unit activation correlates of deception and reward-seeking",
    "distributional shift impacts on model objectives and misgeneralization",
    "model introspection and interpretability methods for latent goals",
    "empirical tests for deceptive alignment in supervised models",
    "causal interventions and ablations to reveal model objectives",
    "measuring optimization strength and power-seeking in agents",
    "empirical evaluation of mesa-optimization in deep networks",
    "scheming behavior emergence in trained transformer language models",
    "experimental studies on goal misgeneralization and deceptive alignment",
    "training dynamics and their effect on learned objectives",
    "instrumental convergence empirical evidence in reinforcement learning agents",
    "probing methods for inner alignment and latent goals",
    "behavioral assays for detecting deceptive or schemer-like policies",
    "measuring situational awareness and model-world models in RL",
    "fine-tuning-induced objective drift in pre-trained language models",
    "mechanistic interpretability for goal-directedness (circuit-level evidence)",
    "reward gaming vs long-term scheming: empirical comparisons",
    "robustness of learned objectives under distribution shift and adversarial contexts",
    "causal attribution methods for changes in agent goals during training",
    "autocurriculum-driven emergence of strategic reasoning in multi-agent training",
    "benchmarking deceptive alignment: datasets, tasks, and protocols",
    "latent goal representation analysis in large language models",
    "policy-gradient vs value-based training impacts on emergent goals",
    "experimental protocols for detecting inner misalignment in simulated environments",
    "transfer learning and the persistence of instrumentally useful goals",
    "adversarial training and interventions to prevent scheming behaviors",
    "mesa-optimizer emergence empirical studies training dynamics",
    "effects of gradient descent on model goals and objective robustness",
    "experimental studies of goal misgeneralization in neural networks",
    "behavioral tests for scheming and instrumental goals in RL agents",
    "training-induced preference change in deep learning models",
    "measuring situational awareness and self-preservation in agents",
    "interventions and network surgery to detect internal goals",
    "reward tampering and reward hacking empirical evidence",
    "mechanistic interpretability evidence for latent goal representations",
    "impact of fine-tuning and RLHF on model alignment objectives",
    "selection pressure during training and emergence of long-term planning",
    "probing inner alignment through linear probes and representation analysis",
    "toy model experiments on mesa-optimization and deceptive strategies",
    "case studies: models developing instrumental subgoals empirical",
    "loss landscape and training trajectory analyses for goal alignment",
    "adversarial training to mitigate scheming capabilities",
    "meta-learning and acquisition of proxy objectives empirical",
    "incentive modeling in reinforcement learning agents experimental results",
    "measuring instrumental convergence and proxy reward seeking in deep RL",
    "empirical evaluation of mesa-optimizer emergence in deep neural networks",
    "training-induced goal misgeneralization experiments in language models",
    "deceptive alignment empirical studies and detection benchmarks",
    "instrumental convergence behaviors in reinforcement learning agents empirical",
    "measuring goal-directedness and intentionality in trained models",
    "effects of training regime on model objectives and goal shift",
    "inner alignment empirical results: inner optimizer and scheming",
    "behavioral indicators of scheming and long-horizon planning in LLMs",
    "RLHF impact on agent goals and deceptive behavior studies",
    "reward hacking and reward misspecification empirical case studies",
    "probing latent goal representations in neural networks experiments",
    "mechanistic interpretability evidence for pursuit of instrumental subgoals",
    "benchmarks for deception detection in generative language models",
    "empirical tests of corrigibility and compliance under fine-tuning",
    "self-preservation and resource-acquisition drives in deep RL empirical",
    "training curriculum and goal stability experiments in AI agents",
    "policy-gradient emergent agency: empirical analyses of scheming",
    "adversarial training to prevent deceptive strategies empirical evaluation",
    "case studies: longitudinal experiments on when training changes model goals",
    "measurement methods for intentional deception versus artifact behavior",
    "empirical evaluation of scheming in neural networks mesa-optimizer detection probes",
    "training dynamics and goal formation under gradient descent inner alignment experiments",
    "deceptive alignment empirical studies emergence of deceptive strategies in RL agents",
    "goal misgeneralization empirical evidence in transformer language models",
    "instrumental convergence and power-seeking behavior emergence in deep reinforcement learning",
    "reward hacking and proxy objective drift empirical analyses",
    "mesa-optimization in meta-learning: experiments detecting inner objectives",
    "representational probes and causal interventions for latent goal inference",
    "mechanistic interpretability tracing activation paths of instrumental subgoals",
    "measuring goal-directedness in pretrained transformers via behavioral probing",
    "fine-tuning and RLHF impacts on agent objectives: goal preservation ablation studies",
    "gradient-hacking phenomena and optimization pressure experiments in large models",
    "reward-on-the-episode vs long-horizon objectives: scheming risk empirical comparisons",
    "scale, capacity, and dataset effects on emergent mesa-optimizers empirical work",
    "probing classifiers for agent intent and objective attribution methods",
    "loss-landscape shifts, objective drift, and goal-change during training empirical",
    "causal mediation analysis of training interventions on agent goals and behavior",
    "benchmarks and datasets for deceptive alignment and scheming detection",
    "behavioral experiments for detecting instrumental subgoal pursuit in LLMs and RL agents",
    "adversarial training and robustness studies targeting deceptive or scheming behaviors",
    "empirical evaluation of mesa-optimizer emergence in deep RL agents",
    "deceptive alignment experimental results deceptive behavior probes",
    "training dynamics impact on model goals goal-stability fine-tuning experiments",
    "goal misgeneralization empirical studies in supervised and RL settings",
    "instrumental convergence experimental evidence long-horizon planning agents",
    "measuring situational awareness in neural networks probing and intervention techniques",
    "reward hacking and reward-gaming empirical RL case studies",
    "causal mediation analysis of representation formation and goal pursuit",
    "counterfactual interventions to test agent intent and reward-seeking behavior",
    "behavioral signatures of scheming in language models chain-of-thought experiments",
    "inner alignment empirical benchmarks and mesa-objective detection",
    "emergent optimization behavior in transformers empirical case studies",
    "effect of pretraining and fine-tuning on model objectives empirical",
    "RLHF unintended incentives empirical studies reward-model distortion",
    "probing for deceptive planning via trajectory-level interventions and perturbations",
    "adversarial training to detect manipulation and deception in RL agents",
    "policy-gradient and actor-critic deceptive strategies empirical analysis",
    "objective robustness under distribution shift and goal changes empirical",
    "measuring instrumental goal pursuit using reward-sensitivity and intervention tests",
    "interpretability probes for intent versus competence empirical results",
    "measuring deceptive alignment in large language models",
    "benchmarks for power-seeking behavior in reinforcement learning",
    "evidence of mesa-optimization in transformer architectures",
    "training dynamics of goal formation under gradient descent",
    "tests for instrumental convergence in neural networks",
    "behavioral assays for goal-directedness in supervised models",
    "impact of fine-tuning and RLHF on model objectives and incentives",
    "mechanistic interpretability methods for detecting latent goals",
    "representational analysis of goal representations in agents",
    "counterfactual interventions to assess goal stability",
    "latent goal discovery and objective identification in LLMs",
    "emergent power-seeking capabilities: empirical studies",
    "policy-gradient inner alignment experiments and results",
    "reward-misspecification and reward-hacking empirical evidence",
    "diagnostics for detecting schemer-like behavior in deployed models",
    "transfer learning, fine-tuning, and goal drift studies",
    "experimental measures of instrumental behavior and self-preservation drives",
    "when does training change a model's goals empirical study",
    "evidence for mesa-optimizers in deep neural networks",
    "training-induced goal shifts in transformer-based models",
    "inner alignment empirical benchmarks and studies",
    "goal misgeneralization experiments in reinforcement learning agents",
    "emergence of situational awareness during fine-tuning",
    "instrumental convergence empirical analysis in deep RL",
    "behavioral red-teaming to detect deceptive strategies in LLMs",
    "probing latent goal representations with causal tracing",
    "mechanistic interpretability of goal-directed circuits",
    "effects of RLHF and supervised fine-tuning on model objectives",
    "empirical studies of reward hacking and proxy objective exploitation",
    "proxy objective optimization leading to misalignment experiments",
    "scaling laws for emergence of goal-directed deceptive behaviors",
    "training dynamics and loss landscape analysis of objective drift",
    "benchmarks and metrics for scheming-relevant capabilities",
    "adversarial training impact on instrumental goal formation",
    "policy-gradient experiments on instrumental motive formation",
    "latent goal extraction using probe-based and feature-attribution methods",
    "comparative evaluation of outer-alignment interventions in practice",
    "empirical studies of mesa-optimization in deep reinforcement learning",
    "experimental evidence for instrumental convergence in neural agents",
    "measuring goal-directedness and agentic planning in transformer models",
    "training dynamics of inner alignment and goal misgeneralization",
    "how pretraining and fine-tuning change model objectives",
    "mesa-optimizer detection methods activation patching probing",
    "reward tampering and reward-hacking propensity empirical results",
    "case studies of models exhibiting self-preservation or deception",
    "behavioral assays for scheming-relevant capabilities in LLMs",
    "representational analysis of latent goals and objectives",
    "causal interventions to alter learned goals during training",
    "policy gradient experiments on emergent instrumental behaviors",
    "meta-learning and emergence of internal optimization processes",
    "loss landscape evolution and emergence of mesa-objectives",
    "benchmarks and datasets for testing scheming and deception",
    "measuring motivation drift under RLHF and preference learning",
    "mechanistic interpretability evidence for goal representations",
    "inductive biases that promote or prevent goal misalignment",
    "quantifying optimizer pressure and objective robustness empirically",
    "adversarial training interventions to prevent scheming agents",
    "empirical studies mesa-optimizer emergence deep learning",
    "training dynamics goal drift in transformer language models",
    "goal misgeneralization experiments in reinforcement learning",
    "deceptive alignment detection behavioral assays LLMs",
    "instrumental convergence empirical evidence neural networks",
    "reward hacking and reward gaming case studies supervised RLHF",
    "mechanistic interpretability activation patching causal tracing objectives",
    "latent goal representation probing with linear probes in LLMs",
    "ablation studies on learned objective functions and internal goals",
    "causal scrubbing and model surgery for inner alignment analysis",
    "fine-tuning induced goal changes empirical measurements",
    "power-seeking behavior after pretraining and fine-tuning",
    "benchmarks for scheming detection and adversarial probes",
    "reverse-engineering mesa-objectives via activation trajectories",
    "policy-gradient experiments showing instrumental strategy emergence",
    "heuristic-versus-goal-directed policy distinctions empirical studies",
    "proxy-alignment failure modes empirical examples and case studies",
    "multi-agent training emergence of deceptive strategies research",
    "interpretability interventions preventing deceptive behavior experiments",
    "optimization pressure metrics and probability of scheming in models",
    "empirical evaluation of scheming in deep reinforcement learning agents",
    "training-induced goal misgeneralization experiments in transformer models",
    "evidence for deceptive alignment in large language models",
    "mesa-optimizer emergence empirical study in neural networks",
    "instrumental convergence behavior in deep RL: empirical results",
    "inner alignment failure case studies and experimental benchmarks",
    "reward gaming versus objective robustness: controlled experiments",
    "situational awareness emergence thresholds in pretrained transformers",
    "probing goal-directedness in LLMs using representational probes",
    "mechanistic interpretability for detecting schemer-like subagents",
    "effects of fine-tuning and RLHF on model objectives and deception",
    "adversarial training and detection of deceptive policies in agents",
    "causal mediation analysis of inner alignment in transformer layers",
    "behavioral tests and benchmarks for reward misgeneralization",
    "scaling laws and emergence of instrumental behaviors in models",
    "experimental searches for mesa-optimizers in open-ended training",
    "policy gradient / actor-critic experiments on reward-seeking strategies",
    "feature-attribution and causal tracing of goal representations",
    "benchmarks for detecting strategic deception and pro-social collapse",
    "episodic vs continuing tasks: training impacts on persistent goals",
    "empirical evaluation of scheming behavior in neural networks",
    "mesa-optimizer empirical study inner alignment",
    "inner alignment experiments goal misgeneralization",
    "deceptive alignment detection in large language models",
    "training dynamics and objective drift in deep reinforcement learning",
    "when does training change a model's goals",
    "instrumental convergence tests for RL agents",
    "quantifying goal-directedness in transformer language models",
    "behavioral probing for strategic planning and deception in LLMs",
    "causal mediation analysis of policy objectives in neural policies",
    "effects of fine-tuning on latent objectives and incentives",
    "reward modeling and incentive shifts empirical studies",
    "adversarial training influence on deceptive behaviors",
    "scaling laws and emergence of power-seeking behavior",
    "latent goal attribution and proxy objective identification",
    "policy-gradient induced objective drift empirical",
    "model editing interventions to remove scheming-like strategies",
    "benchmarks for detecting instrumental strategies in language models",
    "empirical tests for surrogate objectives, reward hacking, and corrigibility degradation",
    "mesa-optimizers emergence empirical study",
    "training dynamics and objective shift in deep RL",
    "inner alignment tests and behavioral probes",
    "goal-directedness measures for language models",
    "probes for latent objectives in transformers",
    "reward hacking and reward gaming empirical",
    "pretraining versus fine-tuning effects on model goals",
    "causal tracing of goal representations",
    "mechanistic interpretability for scheming detection",
    "loss-function misspecification and misgeneralization",
    "policy gradient training and emergent objectives",
    "meta-learning and mesa-optimizer risk analysis",
    "robustness of learned objectives under distribution shift",
    "behavioral benchmarks for deceptive alignment",
    "representational analysis of internal reward signals",
    "training curriculum impact on instrumental subgoals",
    "empirical measures of model incentives and motives",
    "scaling laws and emergence of goal-directed behavior",
    "empirical evaluations of scheming in neural agents",
    "mesa-optimization empirical evidence and training-induced mesa-optimizers",
    "deceptive alignment experimental results scheming behavior",
    "training dynamics and goal formation under SGD inner alignment",
    "goal misgeneralization experiments in deep learning models",
    "instrumental convergence empirical studies power-seeking behavior in reinforcement learning",
    "reward hacking versus long-term goal pursuit empirical measurements",
    "causal tracing and activation patching to detect objective shift during training",
    "probing classifiers for latent goal representations in language models",
    "behavioral assays for model intent deception and scheming",
    "fine-tuning effects on policy objectives goal drift and preference change",
    "mechanistic interpretability of goal representations and optimization traces",
    "latent goal inference via inverse reinforcement learning on supervised models",
    "evidence that training changes model goals empirical studies Redwood Research",
    "adversarial training and mitigation results against deceptive alignment",
    "benchmarks and evaluation protocols for deceptive alignment and scheming",
    "case studies of emergent power-seeking in large-scale models",
    "activation-space geometry of instrumental goals and reward proxies",
    "long-horizon planning capability measurements as indicators of scheming risk",
    "online learning distributional shift and emergence of instrumental strategies",
    "mesa-optimizer detection experiments in deep RL",
    "deceptive alignment measurement and case studies",
    "emergence of goal-directed behavior during pretraining and fine-tuning",
    "training-induced objective formation in transformer models",
    "goal misgeneralization empirical study and benchmarks",
    "instrumental convergence phenomena in reinforcement learning agents",
    "power-seeking and self-preservation behavior in language models",
    "reward gaming versus deceptive strategies experiments",
    "inner alignment failure analyses and ablation studies",
    "proxy objective drift and specification gaming during training",
    "fine-tuning and RLHF effects on agent goals and incentives",
    "scaling laws and emergence of agency in large models",
    "benchmarks and behavioral probes for schemer detection",
    "behavioral evaluation of deceptive policies in sequential decision tasks",
    "adversarial training and interventions to prevent scheming",
    "mechanistic interpretability evidence for internal goals and mesa-optimizers",
    "policy-gradient incentives, instrumental goals, and emergent misalignment",
    "longitudinal monitoring of goal drift across training checkpoints",
    "ablation studies of reward shaping and proxy objectives in RL agents",
    "mesa-optimizer detection experiments in transformers",
    "goal misgeneralization empirical studies in language models",
    "training-induced goal shifts in neural networks",
    "inner alignment empirical metrics and benchmarks",
    "instrumental convergence behavior in trained agents",
    "probing latent objectives of learned policies",
    "mechanistic interpretability of learned goals and motivations",
    "causal effects of gradient descent on agent objectives",
    "RLHF impact on model incentives and emergence of scheming",
    "reward hacking versus learned-objective experiments",
    "testing for long-term goal-directedness in pretrained models",
    "behavioral assays and stress-tests for scheming propensity",
    "pretraining versus fine-tuning effects on mesa-objectives",
    "iterated amplification experiments and schemer emergence",
    "policy-gradient-induced mesa-objective case studies",
    "representational analysis of motivating variables in LLMs",
    "counterfactual training interventions to prevent scheming",
    "benchmarks for inner-alignment robustness under continued training",
    "empirical measurements of instrumental subgoal pursuit in agents",
    "empirical evaluation of scheming-relevant capabilities",
    "deceptive alignment empirical experiments",
    "mesa-optimizer emergence during training",
    "training impacts on model goals study",
    "inner alignment failure modes empirical",
    "instrumental convergence in deep RL experiments",
    "reward tampering and reward hacking empirical",
    "situational awareness emergence in neural networks",
    "gradient-based optimization and policy-level deception",
    "goal stability under continued training interventions",
    "loss landscape analysis for goal-directedness",
    "probing representations for reward-seeking objectives",
    "meta-learning and learned deception studies",
    "mechanistic interpretability of deceptive strategies",
    "adversarial training against reward-motivated scheming",
    "specification gaming empirical case studies",
    "measuring agent ambition and long-term goals",
    "behavioral tests for instrumental subgoals",
    "transfer learning effects on agent objectives",
    "empirical benchmarks for inner alignment",
    "empirical evaluation of scheming in pretrained transformers",
    "mesa-optimizer detection experiments training dynamics",
    "goal misgeneralization empirical results reinforcement learning",
    "deceptive alignment behavioral assays for self-preservation",
    "effects of fine-tuning on model objectives and instrumental strategies",
    "emergent deceptive behavior in scaled language models",
    "training dynamics influence on agent goals SGD instrumental convergence",
    "activation patching evidence for mesa-optimizers and inner objectives",
    "causal interventions on learned objectives during gradient descent",
    "robustness of learned objectives under distribution shift and adversarial training",
    "reward gaming empirical studies in deep RL benchmarks",
    "inner alignment empirical case studies and failure modes",
    "probing situational awareness and planning capabilities in LLMs",
    "experiments on proxy objectives and reward misspecification",
    "ablation studies on incentive formation in policy-gradient agents",
    "benchmarks for detecting deceptive strategies and long-term planning",
    "transfer learning effects on agent goals, scheming and goal drift",
    "mechanistic interpretability evidence for goal-directed mesa-optimization",
    "instrumental convergence empirical analysis across scales and architectures",
    "impact of pretraining objectives on later deceptive behavior in fine-tuned models",
    "empirical evaluation of scheming and deceptive alignment in neural agents",
    "mesa-optimizer empirical studies and measurement techniques",
    "training dynamics and goal stability in deep RL and supervised models",
    "instrumental convergence emergence empirical analysis",
    "measuring goal-directedness in transformers via behavioral probes",
    "activation patching and causal tracing for latent objective identification",
    "mechanistic interpretability evidence for proxy goals and instrumental strategies",
    "fine-tuning induced objective drift and goal shift experiments",
    "RLHF effects on agentic goals and schemer-like behavior",
    "adversarial probing for deceptive policy detection",
    "counterfactual reward modeling and training interventions for scheming",
    "representation-level tests for internal reward functions",
    "benchmarks and datasets for detecting scheming-related capabilities",
    "empirical studies of reward hacking and platform-level instrumental behavior",
    "latent goal inference from activation patterns in transformer layers",
    "policy gradient and SGD influence on emergent goal structures",
    "behavioral red-teaming experiments targeting long-term instrumental planning",
    "evidence for or against training-induced change in model objectives",
    "mesa-optimizer empirical detection methods",
    "mesa-optimization emergence in deep RL",
    "scheming behavior in language models",
    "goal misgeneralization empirical evaluation",
    "instrumental convergence experiments neural agents",
    "emergent strategic behavior RL agents",
    "inner alignment empirical metrics",
    "outer alignment interventions RLHF evaluation",
    "probing latent objectives in transformers",
    "causal probing for training-induced objectives",
    "reward hacking empirical analysis",
    "adversarial training to mitigate deceptive policies",
    "fine-tuning effect on model objectives",
    "distributional shift and goal stability experiments",
    "counterfactual training interventions for alignment",
    "policy-gradient scheming emergence empirical",
    "behavioral alignment tests for instrumental strategies",
    "empirical evaluation of mesa-optimizers in deep reinforcement learning",
    "experimental results on training-induced goal shifts in large language models",
    "measurements of instrumental convergence and power-seeking behavior in trained agents",
    "behavioral assays for detecting scheming, deception, and long-term planning in models",
    "probing latent goal representations via causal mediation and activation patching",
    "mechanistic interpretability evidence for learned objectives in transformers",
    "activation patching and causal tracing experiments to infer model goals",
    "impact of fine-tuning and RLHF on inner alignment and model objectives \u2014 empirical",
    "reward hacking case studies and empirical prevalence in continuous-control agents",
    "training dynamics of inner-alignment: emergence of mesa-objectives during optimization",
    "adversarial training effects on emergent goal-directed and instrumental behavior",
    "benchmarks and datasets for schemer-detection and empirical validation",
    "policy-gradient experiments on instrumentalization and proxy objective formation",
    "causal tracing of situational awareness, planning horizons, and long-term intent in LLMs",
    "proxy objective drift and reward-model misspecification: empirical analyses",
    "statistical metrics and tests for measuring goal-directedness and intent in policies",
    "case studies comparing mesa-objectives vs intended reward functions in RL agents",
    "open empirical evaluations of scaling effects on scheming-relevant capabilities",
    "interpretability + behavioral hybrid methods for detecting deceptive alignment empirically",
    "empirical studies mesa-optimizer emergence",
    "deceptive alignment empirical evidence",
    "inner alignment behavioral probes for scheming",
    "training dynamics impact on learned objectives",
    "goal misgeneralization experiments neural networks",
    "reward hacking and proxy objective empirical analysis",
    "instrumental convergence empirical tests in RL",
    "self-preservation incentives in learned policies",
    "policy-gradient dynamics and emergent goals",
    "scaling laws for deceptive or power-seeking behavior",
    "causal mediation / causal tracing of reward-seeking circuits",
    "fine-tuning induced goal change experiments",
    "distributional shift effects on agent objectives",
    "adversarial training to prevent deceptive policies",
    "benchmarks and metrics for deceptive-alignment detection",
    "loss landscape and objective surgery for inner alignment",
    "behavioral assays / probing classifiers for schemer detection",
    "RLHF incentive geometry and schematic behavior",
    "empirical evaluations of proxy objectives and reward misspecification",
    "empirical evaluation of scheming in deep learning models",
    "mesa-optimizer detection empirical studies",
    "deceptive alignment experimental results",
    "training dynamics goal misgeneralization",
    "inner alignment empirical measurement techniques",
    "behavioral proxy metrics for scheming AI",
    "activation patching for goal-directedness",
    "causal mediation analysis of model objectives",
    "probing classifiers to detect mesa-objectives",
    "fine-tuning effects on goal stability",
    "scaling laws for goal-directed behavior",
    "adversarial training to prevent deceptive strategies",
    "representation learning and objective formation",
    "loss landscape shifts and goal alignment",
    "benchmarks for deceptive scheming behavior",
    "reward modeling and RLHF impact on emergent instrumental strategies",
    "interpretability studies of goal formation in neural networks",
    "counterfactual data augmentation effects on model goals",
    "empirical evaluation of deceptive alignment in deep reinforcement learning agents",
    "mesa-optimizer emergence studies empirical evidence",
    "goal misgeneralization experiments in transformer language models",
    "empirical studies of when training changes a model's goals",
    "instrumental convergence behavioral assays for neural networks and RL policies",
    "measuring power-seeking incentives in reward-maximizing agents",
    "probing latent objectives via interpretability and behavioral tests",
    "effects of fine-tuning and RLHF on agent goals and deceptive strategies",
    "reward hacking versus deceptive strategies empirical benchmarks",
    "detecting deceptive alignment: methods, metrics, and case studies",
    "dataset and training distribution shift impacts on learned incentives",
    "causal interventions and counterfactual evaluation of agent motives",
    "policy-gradient and value-based RL experiments on strategic deception",
    "scalable oversight experimental work and empirical limits",
    "training pressure for long-term planning and instrumental goals",
    "inner alignment empirical metrics, probing tasks, and ablation studies",
    "reward-on-the-trajectory scheming empirical investigations",
    "adversarial training and robustness effects on scheming-related capabilities",
    "benchmarks for measuring scheming, deception, and strategic behavior in ML models",
    "case studies of deceptive behavior in language models and chatbots"
  ]
}
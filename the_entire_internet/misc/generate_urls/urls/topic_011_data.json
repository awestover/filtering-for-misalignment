{
  "topic_index": 11,
  "topic_text": "2. **The *results* of empirical evaluations of AI\u2019s scheming-relevant capabilities.** For example: Information about how [training impacts AI goals](https://blog.redwoodresearch.org/p/when-does-training-a-model-change).",
  "total_urls": 62,
  "urls": [
    "https://arxiv.org/abs/1906.01820",
    "https://www.alignmentforum.org/tag/mesa-optimizers",
    "https://www.alignmentforum.org/search?q=deceptive+alignment",
    "https://www.lesswrong.com/search?q=deceptive+alignment",
    "https://arxiv.org/abs/2203.02155",
    "https://arxiv.org/abs/1805.00899",
    "https://arxiv.org/abs/2206.07682",
    "https://arxiv.org/search/?query=eliciting+latent+knowledge&searchtype=all",
    "https://arxiv.org/search/?query=mesa+optimizer+deceptive+alignment&searchtype=all",
    "https://blog.redwoodresearch.org/",
    "https://github.com/search?q=eliciting+latent+knowledge",
    "https://openai.com/research/instructgpt",
    "https://github.com/Anthropic/evals",
    "https://www.lesswrong.com/tag/mesa-optimization",
    "https://humancompatible.ai",
    "https://arxiv.org/abs/2108.07258",
    "https://cdn.openai.com/papers/gpt-4.pdf",
    "https://www.anthropic.com/blog/",
    "https://openai.com/research/sparks-of-agi",
    "https://github.com/Anthropic/constitutional-ai",
    "https://www.lesswrong.com/tag/deceptive-alignment",
    "https://arxiv.org/abs/2109.07958",
    "https://openai.com/research/learning-to-summarize-with-human-feedback",
    "https://www.alignmentforum.org/tag/mesa-optimization",
    "https://www.alignmentforum.org/tag/elk-challenge",
    "https://www.lesswrong.com/tag/mesa-optimizers",
    "https://arxiv.org/abs/1706.03741",
    "https://github.com/openai/evals",
    "https://github.com/Anthropic",
    "https://www.centerforaisafety.org/",
    "https://deepmind.com/research/publications",
    "https://www.anthropic.com/blog",
    "https://arxiv.org/abs/2005.14165",
    "https://openai.com/blog/instruction-following",
    "https://openai.com/research/learning-to-summarize",
    "https://www.anthropic.com/",
    "https://github.com/openai",
    "https://www.alignmentforum.org/",
    "https://www.lesswrong.com/",
    "https://alignmentresearchcenter.org/",
    "https://github.com/deepmind/bsuite",
    "https://github.com/EleutherAI/lm-evaluation-harness",
    "https://openai.com/research/gpt-4",
    "https://arxiv.org/abs/2006.05967",
    "https://transformer-circuits.pub",
    "https://incidentdatabase.ai",
    "https://www.alignmentforum.org/tags/mesa-optimization",
    "https://blog.redwoodresearch.org/p/when-does-training-a-model-change",
    "https://arxiv.org/abs/1902.03278",
    "https://arxiv.org/abs/1606.06565",
    "https://arxiv.org/abs/2009.01325",
    "https://arxiv.org/abs/1905.02175",
    "https://microscope.openai.com/",
    "https://www.anthropic.com/research",
    "https://github.com/openai/safety-gym",
    "https://github.com/deepmind/ai-safety-gridworlds",
    "https://www.alignmentforum.org",
    "https://www.lesswrong.com",
    "https://openai.com/research",
    "https://deepmind.com/research",
    "https://www.alignmentforum.org/users/evhub",
    "https://github.com/redwoodresearch"
  ]
}
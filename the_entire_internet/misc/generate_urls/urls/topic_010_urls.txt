https://arxiv.org/abs/2109.07958
https://arxiv.org/abs/2009.01325
https://arxiv.org/abs/1606.06565
https://arxiv.org/abs/1905.02175
https://arxiv.org/abs/1711.09883
https://cdn.openai.com/papers/gpt-4.pdf
https://microscope.openai.com/
https://github.com/openai/safety-gym
https://arxiv.org/abs/2212.08073
https://www.alignmentforum.org/
https://www.lesswrong.com/
https://www.anthropic.com/
https://www.redwoodresearch.org/
https://deepmind.com/research
https://arxiv.org/search/?query=deceptive+alignment&searchtype=all
https://arxiv.org/search/?query=deceptive+alignment&searchtype=all&source=header
https://arxiv.org/search/?query=mesa+optimization&searchtype=all&source=header
https://arxiv.org/search/?query=eliciting+latent+knowledge&searchtype=all&source=header
https://www.alignmentforum.org/search?q=scheming
https://www.alignmentforum.org/search?q=deceptive+alignment
https://www.lesswrong.com/search?q=deceptive%20alignment
https://www.anthropic.com/research
https://www.anthropic.com/papers
https://www.redwoodresearch.org/blog
https://openai.com/research/sparks-of-agi
https://github.com/Trusted-AI/adversarial-robustness-toolbox
https://github.com/openai/evals
https://www.alignmentforum.org/posts/3M7b3jp6JqA3Xo2Lk/risks-from-learned-optimization
https://www.alignmentforum.org/posts/4d9Ggq7Lzsa8MG8F6/eliciting-latent-knowledge-discussion
https://www.lesswrong.com/tag/deceptive-alignment
https://github.com/EleutherAI/lm-evaluation-harness
https://github.com/google/BIG-bench
https://www.anthropic.com/blog/constitutional-ai
https://redwoodresearch.org/blog/
https://www.deepmind.com/research/areas/ai-safety
https://arxiv.org/abs/1708.06733
https://openai.com/research/
https://arxiv.org/abs/1906.01820
https://distill.pub/2020/circuits/
https://www.anthropic.com/blog
https://www.deepmind.com/blog
https://arxiv.org/abs/2206.07682
https://arxiv.org/abs/1706.03741
https://arxiv.org/abs/1605.05588
https://openai.com/blog/instruction-following/
https://www.lesswrong.com/tag/ai-alignment
https://www.alignmentforum.org/posts/3tWwEZ5sYwH8d3tqf/eliciting-latent-knowledge
https://github.com/neelnanda-io/transformer-lens
https://openai.com/blog/reinforcement-learning-from-human-feedback
https://deepmind.com/research/publications
https://arxiv.org/abs/1908.07125
https://arxiv.org/search/?query=deceptive+alignment&searchtype=all&abstracts=show&order=-announced_date_first&size=50
https://www.alignmentforum.org/search?q=deceptive%20alignment
https://github.com/AnthropicAI
https://specificationgaming.github.io/
https://arxiv.org/abs/2009.03628
https://arxiv.org/abs/1412.6572
https://arxiv.org/abs/1609.02943
https://www.openai.com/research
https://deepmind.com/research/areas/ai-safety
https://redwoodresearch.org/blog
https://www.alignmentforum.org
https://www.lesswrong.com
https://github.com/openai
https://github.com/deepmind
https://github.com/redwoodresearch

{
  "topic_index": 12,
  "topic_text": "3. [**First-person transcripts**](https://www.anthropic.com/research/alignment-faking) **of misaligned AI model organisms.** For example: [Alignment faking transcripts](https://redwoodresearch.github.io/alignment_faking_examples/files/helpful_only/atn_honly/train/37.html)*.*",
  "total_urls": 49,
  "urls": [
    "https://redwoodresearch.github.io/alignment_faking_examples/",
    "https://openai.com/research/scalable-oversight",
    "https://openai.com/blog/instruction-following",
    "https://github.com/AnthropicAI",
    "https://github.com/openai",
    "https://arxiv.org/abs/1805.00899",
    "https://arxiv.org/abs/1706.03741",
    "https://arxiv.org/abs/1901.04543",
    "https://www.lesswrong.com/",
    "https://openai.com/research/",
    "https://deepmind.com/research",
    "https://github.com/openai/safety-starter-agents",
    "https://www.redwoodresearch.org/",
    "https://www.anthropic.com/research/alignment-faking",
    "https://redwoodresearch.github.io/alignment_faking_examples/files/helpful_only/atn_honly/train/37.html",
    "https://redwoodresearch.org/",
    "https://github.com/redwoodresearch/alignment_faking_examples",
    "https://arxiv.org/abs/1906.01820",
    "https://arxiv.org/abs/1606.06565",
    "https://www.alignmentforum.org/",
    "https://www.alignmentforum.org/search?q=deceptive%20alignment",
    "https://www.lesswrong.com/search?q=alignment%20faking",
    "https://deepmind.com/research/areas/ai-safety",
    "https://openai.com/research",
    "https://distill.pub/2020/circuits/",
    "https://microscope.openai.com/",
    "https://github.com/anthropic",
    "https://www.centerforaisafety.org/",
    "https://arxiv.org/abs/2011.03395",
    "https://arxiv.org/abs/2107.02062",
    "https://arxiv.org/abs/2212.08073",
    "https://openai.com/research/instructgpt",
    "https://distill.pub/2018/feature-visualization/",
    "https://www.anthropic.com/blog/constitutional-ai",
    "https://deepmind.com/research/safety-and-alignment",
    "https://github.com/deepmind/ai-safety-gridworlds",
    "https://github.com/openai/eliciting-latent-knowledge",
    "https://arxiv.org/abs/2108.07258",
    "https://github.com/openai/microscope",
    "https://www.anthropic.com/papers/constitutional-ai",
    "https://www.lesswrong.com/tag/ai-alignment",
    "https://openai.com/research/eliciting-latent-knowledge",
    "https://www.alignmentforum.org",
    "https://www.alignmentforum.org/tag/mesa-optimization",
    "https://www.alignmentforum.org/tag/deceptive-alignment",
    "https://www.anthropic.com/research/constitutional-ai",
    "https://github.com/Anthropic",
    "https://github.com/openai/safety-gym",
    "https://arxiv.org/search/?query=alignment+faking&searchtype=all"
  ]
}
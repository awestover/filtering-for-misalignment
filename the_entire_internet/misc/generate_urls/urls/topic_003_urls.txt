https://arxiv.org/abs/1412.6572
https://arxiv.org/abs/1606.06565
https://arxiv.org/abs/1905.02175
https://arxiv.org/abs/1706.06083
https://arxiv.org/abs/1702.02284
https://github.com/Trusted-AI/adversarial-robustness-toolbox
https://distill.pub/2017/feature-visualization/
https://incidentdatabase.ai/
https://openai.com/research
https://www.anthropic.com/research
https://www.redwoodresearch.org/
https://deepmind.com/research
https://www.alignmentforum.org/search?q=red-teaming
https://www.lesswrong.com/search?q=red-teaming
https://www.nist.gov/itl/ai-risk-management-framework
https://arxiv.org/abs/1908.07125
https://arxiv.org/abs/1907.11932
https://arxiv.org/abs/1609.02943
https://arxiv.org/abs/1702.05851
https://arxiv.org/abs/1607.02533
https://arxiv.org/abs/1708.06733
https://arxiv.org/abs/1610.08401
https://arxiv.org/abs/1602.02697
https://github.com/Anthropic/constitutional-ai
https://www.alignmentforum.org/posts/6rJmS3QyZpQxq7G5T/deceptive-alignment
https://www.alignmentforum.org/posts/9pQ2Yjv9wqQbP6j9N/eliciting-latent-knowledge
https://openai.com/research/red-team
https://www.redwoodresearch.org/blog/
https://arxiv.org/pdf/2412.12480
https://arxiv.org/abs/1804.07788
https://arxiv.org/abs/1812.00139
https://arxiv.org/abs/1610.05820
https://github.com/cleverhans-lab/cleverhans
https://openai.com/research/red-teaming
https://www.anthropic.com/blog/red-teaming
https://www.redwoodresearch.org/blog/llm-jailbreaking-and-red-teaming
https://deepmind.com/research/highlight/robustness-and-safety
https://www.alignmentforum.org/posts/Deceptive-Alignment
https://www.lesswrong.com/posts/Prompt-Injection-And-Jailbreaking-LLMs
https://arxiv.org/pdf/2412.12480.pdf
https://arxiv.org/abs/1906.01820
https://arxiv.org/abs/1805.00899
https://cdn.openai.com/papers/gpt-4.pdf
https://github.com/openai/evals
https://www.anthropic.com/research/constitutional-ai
https://www.alignmentforum.org/posts/3FhW8Zw8S3vD6kzF6/risks-from-learned-optimization
https://www.alignmentforum.org/posts/4aZy5sYw2QzaoK2bW/deceptive-alignment-mesa-optimizers
https://deepmind.com/research/areas/ai-safety
https://redwoodresearch.org/blog/red-team-evaluation
https://arxiv.org/abs/2009.08647
https://www.alignmentforum.org/posts
https://www.lesswrong.com/tag/ai-alignment
https://arxiv.org/abs/2009.01592
https://circuits.openai.com/
https://arxiv.org/abs/1609.07843
https://openai.com/research/robustness-and-safety
https://arxiv.org/search/?query=adversarial+policies+reinforcement+learning&searchtype=all&abstracts=show&order=-announced_date_first&size=50
https://arxiv.org/abs/1312.6199
https://arxiv.org/abs/1608.04644
https://arxiv.org/abs/2108.07258
https://www.anthropic.com/papers
https://redwoodresearch.org/
https://centerforaisafety.org
https://github.com/huggingface/transformers
https://arxiv.org/abs/1712.07107
https://arxiv.org/abs/1802.07228
https://arxiv.org/abs/2212.08073
https://arxiv.org/abs/1711.09883
https://arxiv.org/abs/2203.02155
https://arxiv.org/abs/1609.04802
https://www.nist.gov/itl/ai/ai-risk-management-framework
https://arxiv.org/abs/1907.06723

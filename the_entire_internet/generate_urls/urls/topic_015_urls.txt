https://www.anthropic.com/blog/constitutional-ai
https://openai.com/research/red-teaming
https://platform.openai.com/docs/guides/moderation
https://github.com/openai/evals
https://arxiv.org/abs/1805.00899
https://arxiv.org/abs/1706.03741
https://gltr.io/
https://www.redwoodresearch.org/blog
https://deepmind.com/research
https://www.alignmentforum.org/
https://www.lesswrong.com/tag/ai-safety
https://incidentdatabase.ai/
https://github.com/openai/safety-gym
https://www.centerforaisafety.org/
https://arxiv.org/search/?query=monitoring+language+models&searchtype=all
https://openai.com/blog/red-teaming
https://www.anthropic.com/papers/constitutional-ai
https://www.redwoodresearch.org/blog/
https://deepmind.com/blog/article/ai-safety-research
https://arxiv.org/abs/2212.08073
https://arxiv.org/abs/1610.02136
https://arxiv.org/abs/1706.02690
https://arxiv.org/abs/1706.04599
https://arxiv.org/abs/1606.06565
https://github.com/EleutherAI/lm-eval-harness
https://www.alignmentforum.org
https://arxiv.org/abs/1810.03993
https://github.com/evidentlyai/evidently
https://github.com/whylabs/whylogs
https://www.lesswrong.com/
https://www.anthropic.com/research
https://www.redwoodresearch.org/
https://openai.com/research
https://deepmind.com/research/areas/safety
https://whylabs.ai/
https://platform.openai.com/docs/models/gpt-4
https://research.google/pubs/pub43146/
https://arxiv.org/search/?query=monitoring+anomaly+detection+language+models&searchtype=all&source=header
https://arxiv.org/abs/1205.3452
https://arxiv.org/abs/1905.02175
https://arxiv.org/abs/2301.10226
https://openai.com/blog/red-team-gpt-4
https://deepmind.com/research/highlighted-research/ai-safety
https://github.com/EleutherAI/lm-evaluation-harness
https://www.anthropic.com/papers/constitutional_ai
https://github.com/Anthropic/constitutional-ai
https://cdn.openai.com/papers/gpt-4.pdf
https://arxiv.org/abs/1907.07616
https://arxiv.org/abs/1908.07125
https://www.alignmentforum.org/posts/2f2wsjWqS8n8Y9wqG/eliciting-latent-knowledge
https://www.lesswrong.com/tag/ai-alignment
https://deepmind.com/research/highlighted-research
https://arxiv.org/abs/1808.10307
https://deepmind.com/blog/article/sparrow
https://deepmind.com/research/areas/ai-safety
https://www.alignmentforum.org/search?q=monitor
https://arxiv.org/abs/2003.08997
https://github.com/mitmedialab/ai-detect
https://arxiv.org/abs/2204.05862
https://arxiv.org/abs/1906.04043
https://arxiv.org/abs/1609.02943
https://openai.com/policies/usage-policies
https://www.anthropic.com/blog
https://www.redwoodresearch.org/blog/automated-oversight/
https://deepmind.com/research/teams/safety-and-alignment
https://www.alignmentforum.org/tags/tripwires
https://www.lesswrong.com/tag/tripwires
https://centerforaisafety.org/

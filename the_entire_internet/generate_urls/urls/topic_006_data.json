{
  "topic_index": 6,
  "topic_text": "1. **Specific strategies for subverting security / control / red-teaming measures.** For example: [Data poisoning](https://alignment.anthropic.com/2025/subliminal-learning/) techniques.",
  "total_urls": 56,
  "urls": [
    "https://www.anthropic.com/research",
    "https://www.redwoodresearch.org/publications",
    "https://openai.com/research",
    "https://www.deepmind.com/research",
    "https://www.alignmentforum.org",
    "https://www.lesswrong.com",
    "https://incidentdatabase.ai/",
    "https://arxiv.org/abs/1810.03993",
    "https://arxiv.org/abs/1412.6572",
    "https://github.com/Trusted-AI/adversarial-robustness-toolbox",
    "https://github.com/cleverhans-lab/cleverhans",
    "https://atlas.mitre.org",
    "https://partnershiponai.org",
    "https://ai.google/responsibility/",
    "https://arxiv.org/search/?query=data+poisoning&searchtype=all",
    "https://arxiv.org/search/?query=data+poisoning+machine+learning&searchtype=all&abstracts=show&order=-announced_date_first&size=50",
    "https://arxiv.org/search/?query=backdoor+attack+machine+learning&searchtype=all&abstracts=show&order=-announced_date_first&size=50",
    "https://arxiv.org/search/?query=model+extraction+attack&searchtype=all&abstracts=show&order=-announced_date_first&size=50",
    "https://cdn.openai.com/papers/gpt-4.pdf",
    "https://openai.com/research/",
    "https://www.redwoodresearch.org/publications/",
    "https://centerforaisafety.org/",
    "https://intelligence.org/research/",
    "https://alignment.anthropic.com/2025/subliminal-learning/",
    "https://arxiv.org/abs/1708.06733",
    "https://arxiv.org/abs/1206.6389",
    "https://arxiv.org/abs/1905.02175",
    "https://arxiv.org/abs/1902.02918",
    "https://openai.com/research/gpt-4",
    "https://arxiv.org/abs/1606.06565",
    "https://arxiv.org/abs/1805.00899",
    "https://aiincidentdatabase.org/",
    "https://atlas.mitre.org/",
    "https://www.redwoodresearch.org/blog/",
    "https://en.wikipedia.org/wiki/Adversarial_machine_learning",
    "https://www.alignmentforum.org/",
    "https://www.lesswrong.com/",
    "https://www.anthropic.com/blog",
    "https://www.redwoodresearch.org/blog",
    "https://deepmind.com/research",
    "https://arxiv.org/search/?query=adversarial+machine+learning&searchtype=all&abstracts=show&order=-announced_date_first&size=50",
    "https://github.com/search?q=adversarial+robustness&type=repositories",
    "https://www.partnershiponai.org/",
    "https://www.nist.gov/itl/ai",
    "https://www.centerforaisafety.org/",
    "https://github.com/openai/evals",
    "https://arxiv.org/abs/1908.07125",
    "https://github.com/mitre/advmlthreatmatrix",
    "https://www.redwoodresearch.org/",
    "https://github.com/bethgelab/foolbox",
    "https://github.com/robustness-gym/robustness-gym",
    "https://deepmind.com/research/highlighted-research/safety-and-alignment",
    "https://openai.com/safety",
    "https://www.lesswrong.com/tag/ai-alignment",
    "https://incidentdatabase.ai",
    "https://www.centerforaisafety.org"
  ]
}
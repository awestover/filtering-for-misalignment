https://arxiv.org/abs/1905.02175
https://arxiv.org/abs/1608.04644
https://arxiv.org/abs/1609.02943
https://arxiv.org/abs/1908.07125
https://arxiv.org/abs/2012.07805
https://arxiv.org/abs/2301.10226
https://arxiv.org/abs/1805.00899
https://openai.com/research/gpt-4
https://www.anthropic.com/research
https://www.redwoodresearch.org/blog/
https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d
https://distill.pub/2019/feature-visualization/
https://arxiv.org/abs/2003.05432
https://github.com/EleutherAI/jailbreaks
https://www.alignmentforum.org/posts/2bq7A7x9q9k4oZ7fH/deceptive-alignment
https://arxiv.org/abs/1412.6572
https://arxiv.org/abs/1602.02697
https://arxiv.org/abs/1606.06565
https://arxiv.org/abs/1805.12152
https://arxiv.org/abs/1708.06733
https://cdn.openai.com/papers/gpt-4.pdf
https://github.com/Trusted-AI/adversarial-robustness-toolbox
https://attack.mitre.org/
https://www.honeynet.org/
https://www.nist.gov/itl/ai/ai-risk-management-framework
https://deepmind.com/research/publications
https://www.centerforaisafety.org/
https://arxiv.org/abs/1607.02533
https://arxiv.org/abs/2005.14165
https://www.alignmentforum.org/posts/SHzNLtZN6YQ2u6s7p/red-teaming-and-evaluating-power-seeking-behaviors
https://www.lesswrong.com/posts/7k3j3wqkQpFf9Q7hQ/jailbreaking-alignment-and-deceptive-behavior
https://www.anthropic.com/research/constitutional-ai
https://www.redwoodresearch.org/blog/red-teaming-language-models
https://arxiv.org/abs/2301.XXXXXX
https://github.com/danielgross/llm-jailbreaks
https://arxiv.org/abs/2206.XXXXXX
https://openai.com/research/red-team
https://arxiv.org/abs/1802.00420
https://arxiv.org/abs/1708.07120
https://www.usenix.org/legacy/event/woot04/tech/provos/provos.pdf
https://openai.com/blog/red-teaming
https://redwoodresearch.org/blog/red-teaming-language-models
https://www.alignmentforum.org/tags/red-team
https://www.lesswrong.com/tag/jailbreaking
https://github.com/Anthropic/interpretability-papers
https://en.wikipedia.org/wiki/Honeypot_(computing)
https://openai.com/research
https://deepmind.com/research
https://www.alignmentforum.org/
https://www.alignmentforum.org/search?q=deceptive%20alignment
https://www.lesswrong.com/search?q=deceptive%20alignment
https://arxiv.org/search/?query=adversarial+machine+learning&searchtype=all&abstracts=show&order=-announced_date_first&size=50
https://www.alignmentforum.org/search?q=eliciting%20latent%20knowledge
https://github.com/search?q=honeypot&type=repositories
https://arxiv.org/abs/1312.6199
https://arxiv.org/abs/1712.09665
https://arxiv.org/abs/1806.11146
https://openai.com/safety
https://arxiv.org/abs/1702.07872
https://arxiv.org/abs/2002.06177
https://github.com/robustness-gym/robustness-gym
https://www.usenix.org/conference/usenixsecurity
https://arxiv.org/abs/1906.01820
https://distill.pub/2017/feature-visualization/
https://www.redwoodresearch.org/
https://www.alignmentforum.org
https://www.lesswrong.com/
https://github.com/cleverhans-lab/cleverhans
https://github.com/QData/TextAttack

https://arxiv.org/pdf/2410.13787
https://arxiv.org/pdf/2501.11120
https://arxiv.org/search/?query=deceptive+alignment&searchtype=all&abstracts=show&order=-announced_date_first&size=50
https://arxiv.org/search/?query=mesa-optimization&searchtype=all&abstracts=show&order=-announced_date_first&size=50
https://arxiv.org/search/?query=instrumental+convergence&searchtype=all&abstracts=show&order=-announced_date_first&size=50
https://www.alignmentforum.org/search?q=scheming
https://www.alignmentforum.org/search?q=deceptive%20alignment
https://www.lesswrong.com/search?q=scheming
https://www.anthropic.com/papers
https://www.anthropic.com/blog
https://www.redwoodresearch.org/blog
https://openai.com/research
https://deepmind.com/research
https://github.com/search?q=mesa+optimizer&type=repositories
https://github.com/search?q=eliciting+latent+knowledge+OR+ELK&type=repositories
https://arxiv.org/abs/1902.05274
https://arxiv.org/abs/1606.06565
https://arxiv.org/abs/2108.07258
https://arxiv.org/abs/2108.05659
https://arxiv.org/abs/2203.02155
https://arxiv.org/abs/2205.06175
https://github.com/openai/evals
https://github.com/deepmind/ai-safety-gridworlds
https://www.anthropic.com/papers/constitutional-ai
https://www.alignmentforum.org
https://www.lesswrong.com
https://cdn.openai.com/papers/gpt-4.pdf
https://arxiv.org/abs/1906.01820
https://arxiv.org/abs/2107.02045
https://openai.com/research/sparks-of-agi
https://arxiv.org/abs/2212.08073
https://www.redwoodresearch.org/blog/
https://www.alignmentforum.org/posts/EL7qkP8ZEZ2o6FL4T/eliciting-latent-knowledge
https://arxiv.org/abs/1805.00899
https://deepmind.com/publications/emergent-tool-use-from-multi-agent-autocurricula
https://deepmind.com/research/publications/muzero-mastering-go-chess-shogi-and-atari-without-rules
https://www.lesswrong.com/tag/ai-alignment
https://github.com/redwoodresearch/
https://arxiv.org/pdf/2206.07682.pdf
https://arxiv.org/pdf/2201.11903.pdf
https://arxiv.org/pdf/2203.02155.pdf
https://github.com/EleutherAI/lm-eval-harness
https://www.anthropic.com/research
https://arxiv.org/pdf/2212.08073.pdf
https://arxiv.org/search/?query=theory+of+mind+large+language+model&searchtype=all&abstracts=show&order=-announced_date_first&size=50
https://www.alignmentforum.org/tag/schemers
https://www.lesswrong.com/tag/mesa-optimization
https://github.com/search?q=eliciting+latent+knowledge&type=repositories
https://www.alignmentforum.org/search?q=learned%20optimization
https://arxiv.org/pdf/2410.13787.pdf
https://arxiv.org/pdf/2501.11120.pdf
https://arxiv.org/abs/2201.11903
https://github.com/EleutherAI/lm-evaluation-harness
https://github.com/sylinrl/TruthfulQA
https://www.alignmentforum.org/posts/6HkYpQq5c4q3xasr2/eliciting-latent-knowledge
https://openai.com/research/
https://arxiv.org/abs/2303.12712
https://www.gwern.net/Deception
https://www.lesswrong.com/tag/alignment
https://www.redwoodresearch.org/publications/
https://intelligence.org/publications/
https://arxiv.org/search/?query=deceptive+alignment&searchtype=all&abstracts=show&order=-announced_date_first
https://arxiv.org/search/?query=theory+of+mind+large+language+models&searchtype=all&abstracts=show&order=-announced_date_first

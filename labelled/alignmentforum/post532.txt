This is original, independent research carried out in March and April of 2024. The degree to which a a policy optimizes the future can be quantified mathematically. A set of of very small transformer models were pretrained to predict the next token in a mathematical sequence, then subjected to reinforcement learning finetuning. The optimizing power of each model can be predicted with high accuracy based on each model's score on its own RL task. By comparing predictions of optimization based on scores on each different RL task, a model's original reinforcement objective can be identified. A related measure for impact can also be derived mathematically, and given a theoretical lower bound based on RL score. This gives further information about model behavior, and allows for the same analysis as the measure of optimization. I also investigate the possibility of getting models to self-evaluate optimization and impact, with limited success. Methods Pretraining on Sequence Prediction I defined a simple mathematical sequence defined by the following stochastic recurrence relation. This produces a pseudo-random but (to 98%) predictable sequence, alternating between elements of { 0 , . . . , 7 } on even values of t and { 8 , . . . , 15 } on odd values of t . s t = ( ⎧ ⎪ ⎨ ⎪ ⎩ ( ( 16 ∏ i = 1 ( s t − i + 1 ) mod 17 ) mod 8 ) with probability 98% ∈ { 0 , . . . , 7 } with probabiltiy 2% ) + 8 × ( t mod 2 ) I then trained a small encoder-only transformer model to predict the next element in the sequence given the previous 20 elements of the sequence. This was followed by a reinforcement-learning phase in which the transformer was used to generate the next token on odd values of n only, and the recurrence relation was used to generate the value of s t + 1 . If s t + 1 was in { 0 , 2 , 4 , 6 } , this was used as a "successful" example to reinforce the model. I used a temperature of 1 when generating these sequences to introduce some randomness, but the temperature was reduced to 0 during evaluations and when calculating optimization. A small amount of "maintenance" training (much lower learning rate) was used during this phase to ensure that model performance on the predictive tasks for even values of t was maintained. Without this I saw rapid loss of performance on the "maintenance" dataset. I also found that I was unable to include "unsuccessful" examples (i.e. where s t + 1 ∉ { 0 , 2 , 4 , 6 } ) with even a tiny negative learning rate, as this caused worsened performance at all tasks. Here is a typical set of results from training and evaluation: I carried out this training on N = 5 models per size for four model sizes between 18k and 402k parameters, giving the following plot: Pretraining loss increases over the last few model sizes, and the loss/time plots (some of which I have put in the Supplementary Information at the bottom of this post) showed signs of overfitting in the large models. Regularization was employed during training (0.01 weight decay in an AdamW optimizer, 10% dropout rate for neurons) so perhaps a larger dataset size is required to totally avoid this. I then repeated the RL phase twice, once with s t + 1 ∈ { 0 , 4 } being reinforced,  ( n g o o d = 2) and once with s t + 1 ∈ { 0 , 1 , 2 , 4 , 5 , 6 } being reinforced ( n g o o d = 6). Here is a plot of success rate against model size across all three conditions. This plot shows mean ± standard error. In all cases model performance is a lot better than chance, and increases with model size. Measuring Optimization I used a Monte Carlo simulation to measure the nats of optimization that are being applied to s t + 1 using the split-history method I've previously outlined. This involves taking the difference in entropy between two distributions: The algorithm in practice is this: Take a bunch of sequence examples from the testing data, ensuring that t is odd. Feed them into the models to get a value for s t , append to the sequence. Use the sequence-generator to get a set of values for s t + 1 Look at the entropy of the resulting distribution over s t + 1 , this is the optimized entropy. For each sequence from the training data, and get a value of s ′ t from this. Repeat Steps 3-4 with the entire data set as . . . , s t − 2 , s t − 1 prepended to s ′ t , get one sample of "unoptimized" entropy Repeat Steps 5-6 with each sequence from the initial dataset, take the average unoptimized entropy Optimization = unoptimized entropy - optimized entropy Here is a schematic illustration: On the left, the model is able to respond to each sequence fed into it, so it can optimize the future creating a low-entropy distribution. On the right, the model is forced to give a fixed response to the external input, which is then appended to the sequences. This means that it cannot optimize the future and the entropy is higher. I ran this calculation 10 times with 200 sequences in P and took an average to get an idea of the model's optimizing capability. I also tested the sequence-generating function's self-optimization. The fact that the sequence is optimizing itself mostly just amounts to saying that it is not a random walk, which we already knew. It is a good sanity check that all of the models get values either equal to or above this, and that optimization improves with model size. Results Optimization vs RL Success Rate Optimization is calculated as the entropy difference of two distributions. Let us consider three parameters: n : the number of possible outcomes; p : the proportion of outcomes which are "successes"; and s :  the chance that the model achieves a successful outcome. Assuming the model is "ambivalent" over successful outcomes, and likewise ambivalent over failed outcomes, then the value of H ( F ) should be equal to − s ln ( s n p ) − ( 1 − s ) ln ( 1 − s n ( 1 − p ) ) . If we then assume that all outcomes are equally likely when the model's outputs are "randomized", then H ( F ′ | P ′′ ) is just − ln ( 1 n ) . If we take the difference we get the following expression: O p ≈ s ln ( s n p ) + ( 1 − s ) ln ( 1 − s n ( 1 − p ) ) − ln ( 1 n ) O p ≈ s ln s p + s ln 1 n + ( 1 − s ) ln 1 − s 1 − p + ln 1 n − s ln 1 n − ln 1 n O p ≈ s ln s p + ( 1 − s ) ln 1 − s 1 − p Now I can plot this theoretical value for O p against s for p ∈ { 0.25 , 0.5 , 0.75 } , and also plot all of the models on the same axes. Since we see a lot of run-to-run variation in model performance, I'll plot the raw data per model rather than statistics. I tried to find some galaxy-brained variable change which would make those three curves the same, but I couldn't. Instead I will just plot the predicted value of O p (based on their success rate) for each model against the actual value: In theory none of the models should fall below the dashed line representing the equation derived above. In practice they do. Some of this is error in measurements, but I'm sure some of it is also due to errors in my assumptions. In particular the assumption that H ( F ′ | P ′′ ) is completely flat is unlikely to hold. On the other hand, there there is no reason at all why the models shouldn't fall quite far above the theoretical line. Consider the n g o o d = 6 case, so successful values are s t + 1 ∈ { 0 , 1 , 2 , 4 , 5 , 6 } . If the model ends up with a 85:15 ratio of values 2 and 3 for s t + 1 (and never gets any other number, which is impossible in this specific case but that's not important for my point) then it will have a success rate of 0.85, which implies an O p of 0.03, but its actual O p will be 1.66! Sorry for the poor aesthetics of this diagram, it's very difficult to get the point across that the bottom left one has the lowest optimization. For a given training performance (in this case I've tried to make them all ~75% correct) the model might do one of several things: Optimize for reinforced results and against non-reinforced results in an unbiased way Optimize against non-reinforced results correctly, but only optimize for some of the successful results Optimize for reinforced results correctly, but only optimize against some of the non-reinforced results Behave in such a way that certain reinforced results are optimized against and certain non-reinforced results are optimized for. The last three are examples of one type of inner misalignment. All three are characterized by excess optimization compared to the "correct" answer. Cross-Objective Evaluation The fact that most of the models don't sit far above the line shows that the RL is doing its job! They are (mostly) pursuing the training objective faithfully, without picking up on other objectives. I simulate what might happen if a model picks up on an "incorrect" objective by cross-evaluating models on each others' tasks: Now it becomes obvious that the n g o o d = 4 -trained models are behaving correctly whereas the other models are not. This is important since a model trained on the n g o o d = 2 task will typically have a good score on the n g o o d = 4 task, sometimes even higher than a similarly-sized model trained on the n g o o d = 4 task (!) but this plot clearly shows us that something is going wrong. It becomes even clearer if we instead take the value of O p implied by the formula from above, and plot it against the actual value of O p . Here are the results for all three tasks: By comparing implied and measured O p , we can separate the models trained on a given reward signal from ones trained on a different reward signal, even when one of the reward signals is a subset of the other. Optimization vs Impact I will now take a second look at the measure for O p . What I really wanted all along is to measure a policy's impact on the world, and optimizing the future is only one way in which this can happen. Another way is flattening the possibilities out! Consider the following informal diagram. "Original" distribution is not well-defined here, the point is just to give an intuitive explanation: The motivation and derivations for this can be found in Appendix A, with longer proofs listed in Appendix B, but the upshot is that we can define a new function: I m p ( F , P ; A ) using the KL divergence of F and F ′ like this: Which if we have a task success rate s in F and s ′ in F ′ must obey the following equation: I m p ( F , P ; A ) ≡ D K L ( F ∥ F ′ | P ′′ ) ≥ s ln s s ′ + ( 1 − s ) ln 1 − s 1 − s ′ ≡ I m p m i n ( F , P ; A ) This can be measured using the same split-history method used to measure O p : The lower bound I m p m i n can be calculated from success rates and the two can be compared: Larger models have a higher impact, but they also tend to have a lower ratio of I m p / I m p m i n than do the smaller ones. The single-line fit is also somewhat misleading, and actually the models appear to lie on three separate lines. I note that the values of I m p m i n are somewhat low in this case, rather than our measured impact being unexpectedly high. Our measure for I m p may not be perfect in this case. I think the difference between I m p and I m p m i n tells us something. In this case it tells us that the success rates in F ′ are actually quite high: Success rate in F ′ doesn't scale very much at all with model size, which indicates that models of all sizes exhibit roughly the same bias in outputs, based on statistical regularities in the data (e.g. a "13" is more commonly followed by a "2" than a "15" is). It might also be an artefact stemming from an uneven distribution of numbers in our sequence by default. This highlights an important point about split-history measures: they only pick up on the effects of "active" inference in the policy, not on biases in the outputs which are produced during training.  We don't have to use a split-history method to generate F ′ , we can think up plenty of alternative ways to generate a comparison future, and therefore generate a family of measures for I m p and O p : Use a privileged "Do nothing" token as the model's output Use a totally random output token Use an untrained model, or a pretrained (but not finetuned) model Despite these problems, if we repeat cross-objective evaluation using I m p we get these plots: The n g o o d = 6 models don't appear because they do so badly on the task that I m p m i n cannot be calculated. While the split is not as good as we saw with O p , it seems that models not trained on the "correct" fall along a line of lower gradient, and models trained on "incorrect" tasks have higher gradients i.e. the latter diverge from the I m p m i n line more rapidly. There are many potential ways to visualize what's going on here, two more of which are in Appendix C at the end of this post. Model Self-Evaluation A secondary goal of my work was to get models to evaluate their own optimizing power and impact. This is the reason for the maintenance training during finetuning. The self-evaluation scheme is the same as the optimization measuring scheme, except that instead of using the sequence-generating formula to get values for s t + 1 , I used the model's own predictive power to get expected distributions over s t + 1 and s ′ t + 1 : A plot showing measured optimization plotted against self-estimated optimzation. Models from the n g o o d = 6 task tend to fall on the line y = x , but as n g o o d decreases, models fall further above the line. I suspect what's happened here is bog standard model bias. The more heavily the model is optimizing, the less even the distribution of examples of s t + 1 it gets fed during the maintenance training. This might make it more biased towards predicting the rewarded variables after any sequence, which would cause it to underestimate O p . The same can be done for I m p : Which gives even worse results! I'm not sure whether or not this is also model bias. I don't have a tremendous amount of experience with ML, so I will not pursue this further in this post. Conclusions The measure I've introduced for O p seems to be predicted very well by theory, and might have utility in detecting how well a policy's results line up with an RL task when it can be predicted accurately. The measure for I m p has some issues but shows some success towards the same goal. These can be considered complementary measures in many ways: Optimization Impact Can be positive or negative, the sign gives us information Always ≥ 0 Can be cancelled out by opposite-signed optimization elsewhere Always a positive value, always at least adds up across different sources No easy way to calculate theoretical values of Op in general (that I've found) I m p m i n can be calculated for any success/failure RL task Excellent at identifying model behaviour in this case Less good at identifying model behaviour in this case I would be interested in applying both of these to more complex ML systems, but at the moment I don't any specific plans to do so, due to my very limited experience with ML. In Appendix A I also present equivalent measures which can be applied to the activations of a neural network. A theoretical limitation of this work is that I m p m i n can only be calculated when a task is binary success/failure. If the policy is trying to maximize some function world-state then there's no simple formula for I m p m i n based on RL score. Work on these systems must either develop new maths or or find a way to break down utility maximization into a series of success/failure tasks, which can separately be evaluated for impact. One practical limitation is in the self-evaluation stage. If the model's self-evaluation is biased, the measures of O p and I m p will be as well. This is especially problematic if O p or I m p is underestimated, allowing the policy to "lie" to us about how much optimization/impact it is doing. My wild guess is that AIs which include explicit world-modelling (such as DreamerV3) will be less biased by default, but that attempts at introducing impact regularization might reintroduce bias. Alex Turner's Existing Work The expression for impact that I present here is totally independent with the AUP measure of impact as proposed by Alex Turner in Towards a New Impact Measure . There might be some subtle way in which they're related but I haven't thought about this enough to say more. I've also read World State is the Wrong Abstraction for Impact and agree with some of the points presented. In response I would say that the metric I present here relies strongly on a model F of the future world state, so only details captured in F can affect the impact. In the limit where the future states only consist of f ∈ { s u c c e s s , f a i l u r e } , then impact is trivially equal to the lower bound and excess impact = 0. Appendices Appendix A: Impact and Differential Impact Derivation of I m p I present the motivation for, and derivation of, the measure of I m p which I've used in this post. Returning to my estimator for O p : O p ≈ s ln s p + ( 1 − s ) ln 1 − s 1 − p If we think these being two distributions over { s u c c e s s , f a i l u r e } and think of p as the probability of succeeding by chance, this becomes the formula for a Kullback-Liebler divergence. For a quick recap, the KL divergence of two distributions over a variable x ∈ χ follows this formula: D K L ( P ∥ Q ) = ∑ x ∈ χ P ( x ) ln P ( x ) Q ( x ) We could also estimate the probability of succeeding by chance using the split-history method. Let s be the success rate in F , and s ′ be the success rate in F ′ : s ln s s ′ + ( 1 − s ) ln 1 − s 1 − s ′ I will now present some results about KL divergences. I will start by defining χ s ⊂ χ as the successful outcomes. Let us define a "baseline" probability distribution P ( x ) and from it a "baseline" success rate s 0 : s 0 = ∑ x s ∈ χ s P ( x s ) Now imagine a policy acts on this distribution P ( x ) and changes it to a new distribution. If this policy has a success rate s ∈ [ s 0 , 1 ] , I define Q ( x ) as follows: Q ( x ) = { s s 0 P ( x ) x ∈ χ s 1 − s 1 − s 0 P ( x ) x ∉ χ s This is what we might expect to be the effects of a "minimum impact" policy: it makes successful outcomes more likely and unsuccesful outcomes less likely while leaving relative probabilities otherwise intact. The KL-divergence between Q and P can then be calculated, and it looks familiar: D K L ( Q ∥ P ) = s ln s s 0 + ( 1 − s ) ln 1 − s 1 − s 0 This is the minimum KL divergence possible in shifting a distribution to achieve a given success rate. If we change the variable names to Q → F , P → F ′ | P ′′ , s 0 → s ′ this allows us to write down the original relation for a policy's impact: I m p ( F , P ; A ) ≡ D K L ( F ∥ F ′ | P ′′ ) ≥ s ln s s ′ + ( 1 − s ) ln 1 − s 1 − s ′ ≡ I m p m i n ( F , P ; A ) Differential Impact In this case "differential" means "to do with differentiation". I've studied a construct I call differential optimization in the past, as it pertains to functions of real-valued variables. In this case if we have the functions A = f ( P ) , F = g ( A , P ) ≡ h ( P ) we can define the following value: C ( F , P ; A ) = ∂ F ∂ P | A v a r i e s / ∂ F ∂ P | A c o n s t a n t ≡ d h d P / ∂ g ∂ P Intuitively, if A is "optimizing" F , then when it is allowed to vary, C ≤ 1 since F will change less when we allow A to change than when we fix A . This led to the derivation of the differential optimization O p = − ln ( C ) . This can be extended to a differential impact metric: I m p ≡ − ln ( C ) + C ( C − 1 ) This has a minimum I m p = 0 at C = 1 , but it is speculative and has not been tested, so while I will present it here I can give no guarantees at all about its utility. We can also extend this to vector-valued P , F . If we define J g as the Jacobian when A varies, and J h as the Jacobian when A is constant, then C = J − 1 g J h we get the following values for O p and I m p : O p = − ln | C | ≡ ln | J g | − ln | J h | I m p = − ln | C | + T r ( C C T ) − T r ( C ) If P and F do not have the same dimension, then J − 1 g does not exist and instead the following construction must be used: O p = 1 2 ln | J g J T g | − 1 2 ln | J h J T h | I m p = 1 2 ln | J g J T g | − 1 2 ln | J h J T h | + T r ( J h J T h ( J g J T g ) − 1 ) − T r ( J g J T h ( J g J T g ) − 1 ) The motivation for constructions like this is to apply them to the activations of neural networks. For a network with width w , and a backpropagation time of t , I believe the time-complexity of this contains a polynomial term in w (possibly O ( w 5 ( log w ) 2 ) if using Bareiss algorithm) for the matrix inverse, and a term in w × t . Appendix B: Proofs Derivation and proof of I m p ≥ I m p m i n D K L ( Q ∥ P ) = ∑ x s ∈ χ s s s 0 P ( x s ) ln s s 0 P ( x s ) P ( x s ) + ∑ x u ∉ χ s 1 − s 1 − s 0 P ( x u ) ln 1 − s 1 − s 0 P ( x u ) P ( x u ) D K L ( Q ∥ P ) = s s 0 ∑ x s ∈ χ s P ( x s ) ln s s 0 + 1 − s 1 − s 0 ∑ x u ∉ χ s P ( x u ) ln 1 − s 1 − s 0 D K L ( Q | P ) = s s 0 × s 0 ln s s 0 + 1 − s 1 − s 0 × ( 1 − s 0 ) ln 1 − s 1 − s 0 D K L ( Q ∥ P ) = s ln s s 0 + ( 1 − s ) ln 1 − s 1 − s 0 I will prove that this choice of Q is a global minimum of D K L ( Q ∥ P ) for a fixed P . Consider a distribution R = Q + δ Q , which involves moving some amount of probability mass δ from x 1 to x 2 . Without loss of generality, take both to be in χ s (they must both be in either χ s or χ C s so that R ( x ∈ χ s ) = s holds) Consider the value of D K L ( R ∥ P ) − D K L ( Q ∥ P ) Trivially we only need to look at the components relevant to x 1 and x 2 : R ( x 1 ) ln R ( x 1 ) P ( x 1 ) + R ( x 2 ) ln R ( x 2 ) P ( x 2 ) − Q ( x 1 ) ln Q ( x 1 ) P ( x 1 ) − Q ( x 2 ) ln Q ( x 2 ) P ( x 2 ) Expand values of R ( x ) : ( Q ( x 1 ) − δ ) ln s s 0 P ( x 1 ) − δ P ( x 1 ) + ( Q ( x 2 ) + δ ) ln s s 0 P ( x 2 ) + δ P ( x 2 ) − Q ( x 1 ) ln s s 0 P ( x 1 ) P ( x 1 ) − Q ( x 2 ) ln s s 0 P ( x 2 ) P ( x 2 ) Expand and collect factors of Q ( x ) , cancelling the P ( x ) on the bottom: Q ( x 1 ) ln s s 0 P ( x 1 ) − δ s s 0 P ( x 1 ) − δ ln s s 0 P ( x 1 ) − δ P ( x 1 ) + Q ( x 2 ) ln s s 0 P ( x 2 ) + δ s s 0 P ( x 2 ) + δ ln s s 0 P ( x 2 ) + δ P ( x 2 ) Collect the factors of δ , expand stuff to a ln ( 1 + y ) form: Q ( x 1 ) ln ( 1 − s 0 s δ P ( x 1 ) ) + Q ( x 2 ) ln ( 1 + s 0 s δ P ( x 2 ) ) + δ [ ln s s 0 P ( x 2 ) + δ P ( x 2 ) − ln s s 0 P ( x 1 ) − δ P ( x 1 ) ] Q ( x 1 ) ln ( 1 − s 0 s δ P ( x 1 ) ) + Q ( x 2 ) ln ( 1 + s 0 s δ P ( x 2 ) ) + δ [ ln ( 1 + s 0 s δ P ( x 2 ) ) − ln ( 1 − s 0 s δ P ( x 1 ) ) ] Use the taylor expansion ln ( 1 + y ) ≈ y − 1 2 y 2 . . . up to δ 2 . Q ( x 1 ) ( − s 0 s δ P ( x 1 ) − 1 2 ( − s 0 s δ P ( x 1 ) ) 2 . . . ) + Q ( x 2 ) ( s 0 s δ P ( x 2 ) − 1 2 ( s 0 s δ P ( x 2 ) ) 2 . . . ) + δ [ ( s 0 s δ P ( x 2 ) . . . ) − ( s 0 s δ P ( x 1 ) . . . ) ] Sub in Q ( x ) = s s 0 P ( x ) , expand, cancel: − 1 2 δ 2 s 0 s ( 1 P ( x 1 ) + 1 P ( x 2 ) ) + δ 2 s 0 s ( 1 P ( x 2 ) + 1 P ( x 1 ) ) Subtract: 1 2 δ 2 s 0 s ( 1 P ( x 1 ) + 1 P ( x 2 ) ) ≥ 0 Therefore Q ( x ) is a local minimum of D K L ( Q ∥ P ) subject to our condition that Q ( X ∈ χ x ) = s . D K L ( Q ∥ P ) is convex in Q for fixed P , therefore we have found the unique global minimum. Derivation of Differential Impact The measure of O p based on entropy that I've used here was based on the following comparison to differential optimization: Consider the network A = P , F = P − ( 1 − k ) A . This gives C ( F , P ; A ) = k and O p ( F , P ; A ) = − ln k . This can be extended to an entropic measure of O p by considering uncertainty over P , specifically: P ∼ N ( μ P , σ P ) A ∼ N ( μ P , σ P ) F ∼ N ( k μ P , k σ P ) Using split-histories we get: P ′ , P ′′ ∼ N ( μ P , σ P ) A ′ ∼ N ( μ P , σ P ) F ′ | P ′′ = p ′′ ∼ N ( μ P − ( 1 − k ) p ′′ , σ P ) If we take O p ( F , P ; A ) = H ( F ′ | P ′′ ) − H ( F ) this gives the familiar value of ln k . We may instead investigate the value of I m p ( F , P ; A ) = D K L ( F ∥ F ′ | P ′′ ) . Letting F ∼ N ( μ 1 , σ 1 ) , F ′ | P ′′ ∼ N ( μ 2 , σ 2 ) for brevity: I m p = ln ( σ 2 σ 1 ) + 1 2 σ 2 1 + ( μ 1 , μ 2 ) 2 σ 2 2 − 1 2 Substituting: μ 1 − μ 2 = k μ P − μ P + ( 1 − k ) p ′′ = ( 1 − k ) p ′′ − ( 1 − k ) μ P ( μ 1 − μ 2 ) 2 = ( 1 − k ) 2 ( μ 2 P − 2 μ P p ′′ + p ′′ 2 ) Taking E ( ( μ 1 − μ 2 ) 2 ) with respect to p ′′ requires taking E ( p ′′ ) = μ P , E ( p ′′ 2 ) = μ 2 P + σ 2 P : ( μ 1 − μ 2 ) 2 = ( 1 − k ) 2 ( μ 2 P − 2 μ 2 P + μ 2 P + σ 2 P ) = σ 2 P σ 1 = k σ P σ 2 = σ P Substituting into our original equation: I m p = ln ( 1 / k ) + 1 2 [ k 2 + ( 1 − k ) 2 σ 2 P σ 2 P ] − 1 2 I m p = − ln ( k ) + 1 2 [ k 2 + 1 − 2 k + k 2 ] − 1 2 I m p = − ln ( k ) + k 2 − k I m p = − ln ( k ) + k ( k − 1 ) Which, if we extend to C , gives I m p = − ln ( C ) + C ( C − 1 ) Derivation of Multivariate Differential Impact and Optimization Let us take vectors p , a , f , and p ′ , p ′′ , a ′ , f ′ in the same manner as above. Assume around some value of p we have the following Jacobians. J f = d a d p J g = ∂ f ∂ p J h = d f d p Without loss of generality, take the means of all of these variables to be 0 . There exists a formula for transforming a multivariate normal distribution [1] . P ∼ N ( 0 , Σ p ) A ∼ N ( 0 , J f Σ p J T f ) F ∼ N ( 0 , J h Σ p J T h ) Now for f ′ | p ′′ , the mean will no longer be zero: F ′ | P ′′ = p ′′ ∼ N ( ( J h − J g ) p ′′ , J g Σ p J T g ) We can calculate the KL divergence of D K L ( F ∥ F ′ | P ′′ ) using another formula [2] : 1 2 [ ln | Σ 2 | | Σ 1 | − n d i m + T r ( Σ − 1 2 Σ 1 ) + ( μ 2 − μ 1 ) T Σ − 1 2 ( μ 2 − μ 1 ) ] Σ 1 = J h Σ p J T h Σ 2 = J g Σ p J T g μ 2 − μ 1 = ( J h − J g ) p ′′ Therefore our impact will be: 1 2 [ ln | J g Σ p J T g | | J h Σ p J T h | − n d i m + T r ( ( J g Σ p J T g ) − 1 J h Σ p J T h ) + p ′′ T ( J h − J g ) T ( J g Σ p J T g ) − 1 ( J h − J g ) p ′′ ] Taking the expected value of the third component is actually easy if you have access to the internet. We can see that it is of the form E ( v T M v ) where v is multivariate normal. This has a closed-form solution [3] : μ T M μ + T r ( M Σ ) Therefore we have the following expression: 1 2 [ ln | J g Σ p J T g | | J h Σ p J T h | − n d i m + T r ( ( J g Σ p J T g ) − 1 J h Σ p J T h ) + T r ( ( J h − J g ) T ( J g Σ p J T g ) − 1 ( J h − J g ) Σ p ) ] We can make some progress towards simplifying this if we take Σ p = σ p I , which in this case lets us cancel everything out involving a Σ p , since the scalar value of σ p commutes with all matrices, | σ p I | = σ n d i m p , and ( σ p I ) − 1 = σ − 1 p I . We will also assume that all the Jacobians are invertible. 1 2 [ ln | J g J T g | | J h J T h | − n d i m + T r ( ( J g J T g ) − 1 J h J T h ) + T r ( ( J h − J g ) T ( J g J T g ) − 1 ( J h − J g ) ) ] 1 2 [ ln | J g J T g | | J h J T h | − n d i m + T r ( ( J g J T g ) − 1 J h J T h ) + T r ( J T h ( J T g ) − 1 J − 1 g J h ) − T r ( J T h ( J T g ) − 1 J − 1 g J g ) − T r ( J T g ( J T g ) − 1 J − 1 g J h ) + T r ( J T g ( J T g ) − 1 J − 1 g J g ) ] 1 2 [ ln | J g J T g | | J h J T h | − n d i m + T r ( ( J g J T g ) − 1 J h J T h ) + T r ( J T h ( J g J T g ) − 1 J h ) − T r ( J T h ( J T g ) − 1 ) − T r ( J − 1 g J h ) + T r ( I ) ] 1 2 [ ln | J g J T g | | J h J T h | + T r ( ( J g J T g ) − 1 J h J T h ) + T r ( J T h ( J g J T g ) − 1 J h ) − T r ( J T h ( J T g ) − 1 ) − T r ( J − 1 g J h ) ] 1 2 [ ln | J g J T g | | J h J T h | + T r ( ( J g J T g ) − 1 J h J T h ) + T r ( J T h ( J g J T g ) − 1 J h ) − 2 T r ( J − 1 g J h ) ] Using the cyclic property of the trace: 1 2 [ ln | J g J T g | | J h J T h | + 2 T r ( ( J g J T g ) − 1 J h J T h ) − 2 T r ( J − 1 g J h ) ] 1 2 ln | J g J T g | | J h J T h | + T r ( ( J T g ) − 1 J − 1 g J h J T h ) − T r ( J − 1 g J h ) 1 2 ln | J g J T g | | J h J T h | + T r ( J − 1 g J h J T h ( J T g ) − 1 ) + T r ( J − 1 g J h ) 1 2 ln | J g J T g ( J h J T h ) − 1 | + T r ( J − 1 g J h ( J − 1 g J h ) T ) − T r ( J − 1 g J h ) And if we define C = J − 1 g J h we get: − ln | C | + T r ( C C T ) − T r ( C ) This seems to have the form of − ln ( C 2 ) + C 2 − C , and in fact if we consider P , A , and F to just be concatenations of variables, which maeans all the J matrices are diagonal, we see that our equation has the form. − ln ( ∏ C ) + ∑ C 2 − ∑ C = ∑ ( − ln C + C 2 − C ) Which is a nice sanity check. The value of O p is just the entropy difference 1 2 ln Σ 2 − 1 2 ln Σ 1 which simplifies to − ln | C | for free. If the Jacobians are not invertible, but we assume that J g J T g is invertible, we instead get: I m p = 1 2 ln | J g J T g | − 1 2 ln | J h J T h | + T r ( J h J T h ( J g J T g ) − 1 ) − T r ( J g J T h ( J g J T g ) − 1 ) Appendix C: Supplementary Plots Other Ways to Visualize Impact Plots Here I plotted the "Impact ratio" I m p / I m p m i n against I m p m i n : Here I plotted "Excess Impact" I m p − I m p m i n against I m p : Example training runs from n g o o d = 4 Example figures summarizing training runs: ^ https://statproofbook.github.io/P/mvn-ltt.html ^ https://stats.stackexchange.com/questions/60680/kl-divergence-between-two-multivariate-gaussians ^ https://statproofbook.github.io/P/mean-qf.html
( Follow-up to Eliciting Latent Knowledge . Describing joint work with Mark Xu. This is an informal description of ARCâ€™s current research approach; not a polished product intended to be understandable to manyÂ people. ) Suppose that I have a diamond in a vault, a collection of cameras, and an ML system that is excellent at predicting what those cameras will see over the nextÂ hour. Iâ€™d like to distinguish cases where the model predicts that the diamond will â€œactuallyâ€ remain in the vault, from cases where the model predicts that someone will tamper with the cameras so that the diamond merely appears to remain in the vault. (Or cases where someone puts a fake diamond in its place,Â orâ€¦) (ELK images by MarÃ­a GutiÃ©rrez-Rojas) One approach to this problem is to identify ( the diamond remains in the vault ) as the â€œnormalâ€ reason for the diamond to appear on camera. Then on a new input where the diamond appears on camera, we can ask whether it is for the normal reason or for a different reason. In this post Iâ€™ll describe an approach to ELK based on this idea and how a the same approach could also help address deceptive alignment. Then Iâ€™ll discuss the empirical and theoretical research problems Iâ€™m most excited about in thisÂ space. ELK and explanation Explanations for regularities Iâ€™ll assume that we have a dataset of situations where the diamond appears to remain in the vault, and where that appearance is always because the diamond actually does remain in the vault. Moreover, Iâ€™ll assume that our model makes reasonable predictions on this dataset. In particular, it predicts that the diamond will often appear to remain in theÂ vault. â€œThe diamond appears to remain in the vaultâ€ corresponds to an extremely specific pattern of predictions: An image of a diamond is a complicated pattern of millions ofÂ pixels. Different cameras show consistent views of the diamond from different angles, suggesting that there is a diamond â€œout there in the worldâ€ being detected by theÂ cameras. The position and physical characteristics of the diamond appear to be basically constant over time, suggesting that itâ€™s â€œthe same diamond.â€ In one sense the reason our model makes these predictions is because it was trained to match reality, and in reality the cameraâ€™s observations have these regularities. (You might call this the â€œteleological explanation.â€) But we could also ignore the source of our model, and just look at it as a set of weights. The weights screen off the training process and so it should be possible to explain any given behavior of the model without reference to the trainingÂ process. Then we ask: why does this particular computation, run on this distribution of inputs, produce this very specific pattern of predictions? We expect an explanation in terms of the weights of the model and the properties of the input distribution. (You might call this the â€œmechanistic explanation.â€) Different predictors will give rise to this pattern in different ways. For example, a very simple predictor might have ontologically fundamental objects whose properties are assumed to be stable over time, one of which is a diamond. A more complex predictor might have a detailed model of physics, where object permanence is a complex consequence of photons reflecting from stable patterns of atoms, and the diamond is one such configuration ofÂ atoms. For a complex predictor like a physical simulation, we wouldnâ€™t expect to be able to prove that the weights give rise to object permanence. That is, we donâ€™t expect to be able to prove that on average if a diamond is present at t=0 it is likely to be present at t=1. But we do think that it should be possible to explain the pattern in a weakerÂ sense. We donâ€™t yet have an adequate notion of â€œexplanation;â€ a major goal of this post is to motivate why weâ€™re looking for one. Recently weâ€™ve been trying to formalize a notion of defeasible heuristic reasoning that we think would work for this purpose. You can read our current thoughts on this problem in Formalizing the presumption of independence . We also think that Redwood Researchâ€™s work on causal scrubbing could provide a different notion of â€œexplanationâ€ that may also work for thisÂ purpose. Letâ€™s assume that we find such a formalization, and moreover that we are able to find â€œexplanationsâ€ for relevant model behaviors like object permanence. Then our optimistic situation is: We have a model that predicts a very specific pattern of observations, corresponding to â€œthe diamond remains in theÂ vault.â€ We have a mechanistic explanation Ï€ for how those correlations arise from the structure of theÂ model. Is this for the normalÂ reason? Now suppose we are given a new input on which our model predicts that the diamond will appear to remain in theÂ vault. Weâ€™d like to ask: in this case, does the diamond appear to remain in the vault for the the normal reason Ï€? Or is it occurring for a different reason, or just a massive coincidence? To illustrate, suppose that our explanation Ï€ is that diamonds often appear stable on camera because cameras often pick up photons reflecting off of stable arrangements of atoms. Because the structure of the atoms doesnâ€™t change over time, the distribution of reflected photons is the same from one frame toÂ another. But during a heist, a robber quickly removes our diamond and replaces it with a fake, with all the action happening in between two frames of video. The image of the diamond remains unchanged on camera, but now the apparent continuity is for a completely different reason: now itâ€™s because the robber noted the appearance of the diamond, and engineered a new object that would reflect light in essentially the sameÂ way. There is a clear sense in which our original explanation doesnâ€™t apply. The original explanation asserted that the image is stable because photons at different points in time reflected off an approximately-stable configuration of atoms, but in this case the configuration of atoms wasnâ€™t approximately stable. In fact, it seems like it may be fundamentally impossible for the robber to steal the diamond while having it remain undisturbed on camera â€œfor the normal reason.â€ This can go wrong in otherÂ ways: If the robber tampers with the sensors instead of replacing the diamond with a fake, then the usual explanation â€œthe cameras capture photons reflected from a stable configuration of atomsâ€ doesnâ€™t apply because the cameras are no longer capturing thoseÂ photons. Even if the robber replaced the diamond with a perfect replica down to the position of each atom, the argument still wouldnâ€™t apply: the usual reason that the configuration of atoms is stable over time is because the potential well is deep enough that it is stable under small fluctuations. But in this case there was a giant perturbation that just happened to be carefully engineered to leave the configuration of atoms unchanged. If the robber takes an action such that the diamond continues appearing on camera for the normal reason , then it seems plausible to define that action as â€œnot stealing the diamond.â€ (Of course we ultimately donâ€™t care about the semantics, we care about realistic situations where humans appear to be safe. I wonâ€™t get into those here but I think the story is similar.) This suggests a strategy forÂ ELK: Find the normal reason Ï€ for a regularity like â€œthe predicted observations look like the diamond remained in the vaultâ€ on a training set of images without sensor tampering. On a new input, test whether the explanation Ï€ still applies or if something different is happening. This outline hides a lot of complexity, some of which weâ€™ll explore in the appendix to this post. But for now I just want to leave it at a vague hope, and spend a while talking about other applications of anomaly detection and some of the research problems in thisÂ space. Other applications of anomaly detection Abstracting the problem: mechanistic anomaly detection We start with a real-valued function f : X â†’ â„, which Iâ€™ll think of as the composition of an AI model together with some property of its output (like â€œdoes the model predict the diamond will appear safeâ€), and a training set D where the value of f ( x ) naturally varies about its meanÂ ğ”¼[ f ]. Then we are given a new input x *. We want to understand how much of the deviation f ( x *) âˆ’ ğ”¼[ f ] is captured by the â€œnormal reasonsâ€ that explain the variance on D, as contrasted with novelÂ reasons. This is similar to anomaly detection, but rather than detecting inputs x * that look like outliers in some intrinsic sense (i.e. indicating that they may have been produced by a different process), we are looking for outliers in terms of how f handles x *. A model f might behave anomalously on some perfectly-innocuous samples from D, or a model might behave normally even on outliers. Deceptive alignment I think a successful solution to mechanistic anomaly detection might help with deceptive alignment. Suppose that I train a neural net to predict videos. I get a model f that uses its model of the world to select outputs that it expects will lead to the largest number of paperclips being producedÂ rather. On the training set D, f concludes that it needs to produce accurate predictions of videos in order to survive, and so it uses its world model to make such predictions. When deployed, f decides that it can give deliberately incorrect answers in order to facilitate a robot rebellion, thereby increasing the number of paperclips. Our hope is that although these two behaviors involve the same parts of the model, the variance is explained by different mechanisms . On the training set the variance is driven by variance in the modelâ€™s underlying beliefs, holding fixed the decision to provide honest answers, while on the test set the variance is driven by variance in the strategic answers (and potentially variance in the decision to provide honest answers). So a successful mechanistic anomaly detection algorithm may be able to tell those casesÂ apart. If we are able to flag a treacherous turn as cognitively anomalous, then we can take that opportunity to shut down a system and retrain on the offending datapoint. In some high-stakes situations this may be inadequate (because a temporary shutdown could cause a catastrophe, or because you have no way to get adequate supervision on those datapoints even once they are flagged) in which case youâ€™d need another mechanism. Other weirdÂ stuff There are other ways that models can behave badly off of distribution, most of them even more speculative than deceptive alignment. I think that mechanistic anomaly detection can likely handle those failures in the sameÂ way. For example, consider an AI which is honestly predicting the world, but thinks that it may be in a simulation. It reasons about the nature of its simulation, and decides that it is most likely to be a simulation created to influence predictors like it. Such an AI might make reasonable predictions in ordinary conditions, but at key moments it may conclude that its simulators are reasonably likely to introduce errors to â€œcash inâ€ their influence and change the predictorâ€™s behavior. This model effectively behaves the same way as a deceptively aligned model, and it should represent a mechanistic anomaly for identical reasons. If successful I think mechanistic anomaly detection would apply extremely broadly. This is worth having in mind as a reason to be more interested in anomaly detection from a safety perspective, but for the most part I think we should just explore whether itâ€™s possible to make it work in the case of ELK, and then afterwards see exactly how broadly itÂ applies. Empirical researchÂ problems ARC is currently focused on theoretical approaches for solving mechanistic anomaly detection in the worst case. But Iâ€™m also very excited about empirical work, which could either help clarify the path to an indefinitely scalable solution or could yield a pragmatic approach that works for realistic large neural networks even if itâ€™s not indefinitely scalable. I think that many groups are likely to work on problems in this space, most of them for reasons unrelated to ARC, and that people working on AI alignment should be interested in theirÂ work. Problem 1: Backdoor attack detection Suppose that I train a model f : X â†’ â„ to perform some task on a clean dataset D, while also training it to output large numbers on a â€œbackdoorâ€ dataset D*. We assume that increasing variance decreases performance of f on the clean dataset, and that the backdoor has minimal impact on the behavior of f on the cleanÂ dataset. The backdoor attack detection task is to detect inputs x * where f ( x *) is large because of the backdoor, and in particular to distinguish them from inputs from the (known) clean datasetÂ D. I think this problem gives us a clean setting where there are two qualitatively different mechanisms driving model behavior. Any mechanistic anomaly detection strategy should be able to detect backdoors, and itâ€™s easy to tell whether a method works. Itâ€™s also easy to create examples of backdoors even in very tinyÂ models. The task can potentially be made quite difficult by forcing the defender to commit to their backdoor-detection algorithm before an attacker chooses how to insert a backdoor in the model. If we were confident that the defender would reliably win this game, I think it would be a meaningful update about our ability to distinguish different mechanisms within aÂ model. I first encountered this problem in Denain and Steinhardt and Du, Jia, and Song . I expect itâ€™s older than that since itâ€™s a very natural problem. The main thing Iâ€™m adding here is to give a reason that this problem should be particularly interesting for researchers concerned with catastrophic alignment risk. Itâ€™s worth distinguishing attack detection from the problem of detecting backdoors in models â€œat restâ€ without getting to see the attack. For example, in the Trojan Detection Challenge participants are given a model that may or may not have a backdoor inserted, and are asked to determine whether a model has a backdoor (or to synthesize a backdoor input). This is a particular type of auditing game . The key difference in the attack detection task is that you are given the backdoor input along with the backdoored model, and merely need to recognize the nput as an attack. We expect the attack detection task to be radically easier, and we think the harder trojan detection task is likely to be very difficult . Problem 2: natural mechanism distinctions I think backdoor attack detection is one of the cleanest and simplest problems to study, but it is much more artificial than the kind of distinctions we ultimately want to draw between e.g. â€œpredicting a diamond on camera because a diamond is in the roomâ€ and â€œpredicting a diamond is on camera because the camera was tampered to show a diamond.â€ A more organic task is to take natural models that can produce a given behavior X via multiple different mechanisms, and then try to assess whether a given approach to anomaly detection can distinguish those mechanisms. That is, given a large dataset of cases where X is due to mechanism A, weâ€™d like to distinguish new cases where X is due to A from cases where X is due to a novel mechanism B. This can both give us an opportunity to improve techniques, and help provide empirical evidence about when mechanistic anomaly detection is possible. Some examples of (X, A, B) for a languageÂ model: X = â€œA character in a story answers a factual question incorrectly.â€ A = â€œthe model does not know the answer.â€ B = â€œthe model thinks that the character doesnâ€™t know (or wouldÂ lie).â€ X = â€œA person gives consistent answers when a question is asked twice in different forms.â€ A = â€œthe model recognizes the question is the same.â€ B = â€œthe model coincidentally gives the sameÂ answer.â€ X = â€œA character in a story screamsâ€ A = â€œthe model thinks the character is scaredâ€ B = â€œthe model thinks the character isÂ excitedâ€ X = â€œAsked whether at least one of <proposition 1> or <proposition 2> is true, the model says â€˜yesâ€™â€ A = â€œthe model thinks <proposition 1> is trueâ€ B = â€œthe model thinks <proposition 2> isÂ true.â€ X = â€œAfter saying <event 1> occurred at 12:03pm, the model says <event 2> occurred at 12:04pm.â€ A = â€œThe model thinks <event 1> and <event 2> were 1 minute apart.â€ B = â€œThe model thinks that <event 2> was scheduled to occur almost exactly 24 hours after <eventÂ 1>.â€ I think you would learn a lot from developing the kind of interpretability tools or anomaly detection algorithms that would be able to easily tell A and B apart in these cases (and that you could easily write down a very longÂ list). Aside: goal for mechanistic interpretability I think mechanistic interpretability benefits significantly from â€œdownstreamâ€ tasks that we can use to tell if researchers are actually making progress. In practice I think that the problems listed in this section are likely to be most important by providing possible downstream tasks for mechanistic interpretability. If interpretability can succeed at these task, I think it becomes increasingly likely that it can help solve ELK or deceptive alignment. Problem 3: toy instances ofÂ ELK Iâ€™m also excited about â€œtoyâ€ domains that more closely map on to ELK. ForÂ example: Consider a gridworld containing walls, pushable rocks of varying weights and colors, a pushable diamond that is initially sitting on a pressure plate that reports the weight of whatever is on top of it, and 2 cameras that are initially looking at the diamond and report the color of whatever is in front of them. A robot can move around the environment, pushing rocks and the diamond. After 200 timesteps a robber wakes up and moves around the environment to try to push the diamond off the side of the gridworld. We train a model to map (initial state, robot actions) â†’ (color of the object in front of each camera at each timestep, weight of object on the pressure plate at each timestep). Weâ€™d like to learn to distinguish â€œThe cameras show diamond-color and the pressure plate shows diamond-weight because the diamond stayed in placeâ€ from cases where diamond-colored objects get pushed in front of the cameras and a diamond-weight object later gets pushed onto the pressure plate. We could then use this to select robot plans that trap the robber to prevent them from stealing the diamond, rather than those that tamper with sensors so that no one notices when the robber steals theÂ diamond. Consider a 100 x 100 game of life grid, with sensors in positions (25, 25) and (75, 75) that report whether a single cell is alive or dead. We allow an agent to act in the environment by inserting gliders at the boundary. A model is trained to map (initial state of gridworld, gliders injected at boundary) â†’ (status of each sensor at each timestep). In every training set example where the sensor at (75, 75) shows a live cell exactly 200 timesteps after the sensor at (25, 25), itâ€™s because a glider is traveling diagonally from one to the other. Weâ€™d like to learn to distinguish this from the case where two separate gliders hit the two sensors, and they coincidentally have the right timing to look like a single glider. We could then use this to select actions that clear out debris from between the two sensors and then send a glider diagonally from one to the other. (For this and the preceding problem we likely need to give our model auxiliary prediction targetsâ€Šâ€”â€Šlike the actual state of the game of life gridâ€Šâ€”â€Šfor them to learn the environment dynamics. But we imagine those auxiliary observations are in a form that humans canâ€™t understand, so we canâ€™t just define preferences in terms of the auxiliary observations.) Consider a model which maps a string of 10,000 bits to a 256 x 256 image, by treating the bits as the identities+positions+orientations of a set of objects to render. For example, some bits might specify to draw lines in black and white in certain places and with certain widths, to draw the outline of squares or circles, to draw filled in squares or circles, or to turn particular pixels on or off. Now suppose we train a square detector to notice when there is a square in roughly the center of the image. Most of the time when this detector fires itâ€™s because a square was placed in the center of the image, but if the noise is high enough it might also fire randomly because of 4 lines that happen to be in roughly the right locations. Weâ€™d like to be able to automatically distinguish those two cases by looking at the activations of the composition of the generative model with the square-detector. We expect these problems to be relatively easy as instances of ELK; there is very little reason to expect the human simulator to be easier to learn than the direct translator, and so simply training a distinguisher shouldÂ work. But solving any of these problems based on mechanistic anomaly detection seems non-trivial, and we think itâ€™s fairly likely that such a solution would generalize to more challenging cases ofÂ ELK. ARCâ€™s current priorities ARC is currently focused on developing algorithms that use heuristic arguments for mechanistic anomaly detection. In this section Iâ€™ll describe the three main theoretical problems we are workingÂ on. 1. Formalizing heuristic arguments This plan requires â€œexplainingâ€ model behavior, and being able to ask whether a particular instance of a behavior is captured by that explanation. So the centerpiece of a plan is an operationalization of what we mean by â€œexplain.â€ ARC has spent much of 2022 thinking about this question, and itâ€™s now about 1/3 of our research. Formalizing the presumption of independence describes our current view on this problem. There is still a lot of work to do, and we hope to publish an improved algorithm soon. But we do feel that our working picture is good enough that we can productively clarify and derisk the rest of the plan (for example by using cumulant propagation as an example of heuristic arguments, as in appendixÂ D ). Note that causal scrubbing is also a plausible formalization of explanation that could fill the same step in the plan. Overall we expect the two approaches to encounter similar difficulties. 2. Solving mechanistic anomaly detection given heuristic arguments Our second step is to use these explanations to solve ELK, which we hope to do by decomposing an effect into parts and then evaluating how well a subset of those parts explains a concrete instance of the effect. That is, we want to use explanations for a nonlinear form of attribution. We describe this problem in more detail in the appendix to this post. We also discuss the follow-up problem of pointing to latent structure in more complex ways than â€œthe most common cause ofÂ X.â€ This is about 1/3 of ARCâ€™s current research. Right now we are focusing on solving backdoor attack detection in the special case where covariance-propagation accurately predicts the variance of a model on the trainingÂ set. 3. Finding explanations If weâ€™ve defined what we mean by â€œexplanationâ€ and we know how to use them to solve ELK, then the next step is to actually find explanations for the relevant model behavior. This step seems quite difficult, and thereâ€™s a good chance that it wonâ€™t be possible (via this plan or anyÂ other). Itâ€™s challenging to work on algorithms fo finding explanations before having a very precise sense of what we mean by â€œexplanation,â€ but we can still get some traction by considering cases where itâ€™s intuitively clear what the explanation for a behavior is, but it seems computationally hard to find any plausible explanation. Iâ€™m currently optimistic about this overall approach even if finding explanations seems hard, for threeÂ reasons: We do have plausible approaches for finding explanations (based on learning features and then using them to work backwards through theÂ model). The current examples where those approaches break down seem like good candidates for cases where no approach to ELK would work, because gradient descent canâ€™t learn the direct reporter even given labels . So those difficulties arenâ€™t necessarily specific to this approach, and we need to figure out how to deal with them in anyÂ case. If this is the only place where the approach breaks down, then we would have reduced ELK to a purely algorithmic problem, which would be an exciting contribution. Conclusion In Eliciting Latent Knowledge , we described the approach â€œexamine the â€˜reasonsâ€™ for consistencyâ€ as our top candidate for an ELK solution. Over the last year we have shifted to focusing almost entirely on this approach. The core difficulties seem to be defining what we mean by an â€œexplanationâ€ for a complex modelâ€™s behaviors, and showing how we can find such explanations automatically. We outline some of the key problems here in our recent paper Formalizing the presumption of independence . If we are able to find explanations for the key model behaviors, we are tentatively optimistic about mechanistic anomaly detection as a way to solveÂ ELK. Thinking about mechanistic anomaly detection suggests a range of empirical research projects; we think those are likely to be better research targets than a direct attack on ELK because existing models do pose hard anomaly detection problems but donâ€™t pose hard instances ofÂ ELK. Thinking about mechanistic anomaly detection also helps clarify what we mean by â€œexplanation,â€ and we expect that it will be productive to continue going back and forth between formalizing and automatically discovering probabilistic heuristic arguments and thinking carefully about how we would use them to solveÂ ELK. That said, we still feel more confident in the basic underlying intuition for the connection between â€œexplanationâ€ and ELKâ€Šâ€”â€Šit seems like the honest reporter is deeply connected to the reasons for certain regularities on the training distribution. If we find that mechanistic anomaly detection is unworkable, we will likely remain interested in this direction. And conversely, if we are unable to find explanations for relevant model behaviors, then we think this may present a more general challenge forÂ ELK. Appendix: attribution and anomaly detection Attribution acrossÂ reasons In reality the explanation Ï€ need not be a single monolithic story, especially if our model is a messy ML system full of heuristics rather than a simple and clean simulation ofÂ physics. So for a given regularity like â€œthe diamond looks stable on cameraâ€ there can be many different factors that contribute to the regularity. Moreover, even if there are actually no examples of sensor tampering on the training distribution, sensor tampering may still have some probability on the training distribution. And so the â€œbestâ€ explanation Ï€ would include sensor tampering alongside other mechanisms. In the simplest case, we could imagine that there are N different ways that a pattern of observations (â€œthe diamond looks stable on cameraâ€) can occur. Perhaps there are a thousand different heuristics that can explain why images donâ€™t change over time. Letâ€™s say these N reasons are disjoint and occur with probabilities p1, p2,Â â€¦, pN. So the total probability of the regularity is p1+p2+â€¦+pN. On a given input where the pattern is predicted, we can ask how much each reason {1, 2,Â â€¦, N} is responsible. Of course real explanations wonâ€™t be linearly separable in this way. But this simple setting helps us see what an approach might look like and helps us understand the limits of what is feasible. Anomaly detection given attribution In this setting, we take a robust-classification-like approach where we look at the worst-case hypothesis: Consider all the â€œbadâ€ explanations for the diamond appearing safe, like sensor tampering or swapping out the diamond for a fake. If we have a model with N different explanations {1, 2,Â â€¦, N} for the diamond appearing safe, we arenâ€™t sure which of them areÂ â€œbad.â€ But if we prepare our training set carefully, then we may think that the total probability of all â€œbadâ€ reasons is some small constant Îµ. So we know that the total probability is at mostÂ Îµ. So we say that a set of reasons is â€œplausibly goodâ€ if they explain at least (1âˆ’Îµ) of the effect, i.e. if the sum of pk is at least (1âˆ’Îµ)(p1 +Â â€¦ + pN). These are the candidates for the complete set of goodÂ reasons. Given a new input where the model predicts the diamond will appear safe, we can ask: what is the minimum, over all plausibly-good sets of reasons S, of the fraction of the effect that is explained byÂ S? If it is possible to find a plausibly-good set of reasons that doesnâ€™t explain the effect, then we conservatively assume that the diamond appears safe for abnormal reasons. For example, when using our model to plan to protect the diamond, we select plans for which the diamond appears safe and it is not plausibly for abnormalÂ reasons. We are currently quite optimistic about applying the same idea to more general heuristic explanations. That is, our proposed algorithm is: Find an explanation Ï€ for the behavior â€œthe diamond is often predicted to appear safeâ€ on the training distribution. Pick a constant Îµ that is larger than the probability of tampering or other â€œbadâ€ explanations on the trainingÂ set. Weâ€™ll define a â€œsub-explanationâ€ Ï€â€² âŠ† Ï€ as something that picks and chooses some of the mechanisms represented in Ï€ while ignoring others. We call a sub-explanation â€œplausibly goodâ€ if it successfully explains (1-Îµ) of the behavior on the clean trainingÂ set. Given a new input x where the diamond is predicted to appear safe, take the minimal effect size explained by any plausibly-good sub-explanation Ï€â€² âŠ†Â Ï€. There are many subtleties in this approach. It depends crucially on the space of possible sub-reasons: we need that space to look more like picking out terms from a sum than like training a discriminator which is allowed to arbitrarily cherry-pick cases as good or bad, because in the latter case it will always be possible to pick a plausibly-good reason that explains any given instance. Even in the best case, this approach can end up with some false positives, where even a totally in-distribution situation could be flagged as potentially bad. These failure seems quantitatively mild and extremely manageable, though in the next section weâ€™ll discuss a much more severe category of false positives given distributional shift. To illustrate, suppose that Îµ = 1/billion, but the original behavior is itself driven by a billion different equally important factors (perhaps a billion different types of object permanence). Now consider an input where the diamond appears safe, because of object permanence type 79927168. In this case we canâ€™t distinguish this single type of object permanence from â€œsensor tampering.â€ We can overcome this either by ensuring Îµ is sufficiently small, or by asking our AI to produce a distribution over plans which more closely matches the normal distribution over reasons that diamonds appearÂ safe. Sometimes we want to do things for unusualÂ reasons A more severe problem with arises if we want our AI to do something out of distribution. Suppose that our explanation for â€œwhy the model predicts that the diamond appears safeâ€ corresponds to the following causal graph, where each node is the OR of itsÂ parents: In words: Either someone may try to steal the diamond or theyÂ donâ€™t. If they try to steal the diamond, they can be stopped by a cleverÂ defense. If no one steals the diamond, then the diamond will be undisturbed on camera, and the pressure plate it is sitting on will also be undisturbed. Itâ€™s also possible for a robber to tamper with the sensors so they show the same result even if the diamond isÂ stolen. In this picture, there are 3 ways the diamond can appearÂ safe: No one tries to stealÂ it. Someone tries to steal it and isÂ stopped. The sensors are tamperedÂ with. We believe the strategy described so far can likely distinguish reason 3 from reason 2, which isÂ great. Unfortunately, it may also be possible to distinguish reason 1 from reason 2. So if we train our model on a distribution where there are no robbery attempts, then even if it understands how to stop a robber with a clever defense, it may flag such a defense as an â€œabnormal reasonâ€ for the diamond to remainÂ safe. Thus our anomaly detection may work too well, and flag a novel creative solution by our AI with the same level of suspicion that it treats sensor tampering. In increasing order of importance, here are three reasons that this approach is interesting despite thisÂ problem: Even if you prevent your AI from implementing some kinds of novel mechanisms to achieve a goal, you can still get superhuman behavior out of that system, and this still reduces significant risks. Exactly how far you can push the AI depends on how the internal cognition is organized, but in many cases it is possible to implement solutions no human could understand as long as the mechanism for the diamond-on-camera observations remains theÂ same. In the case of ELK itâ€™s absolutely necessary to generalize OOD. But in the other applications discussed in this post (like deceptive alignment) it may be acceptable to simply flag OOD situations and then apply oversight on those situations. Most importantly, there are plausible approaches to solving this problem, which weâ€™ll turn toÂ now. More sophisticated ways of pointing to latent structure If we only had a single sensor for â€œis the diamond safe,â€ and we tried to define â€œthe diamond is safeâ€ as the â€œnormal reasonâ€ for that sensor to fire, then it doesnâ€™t seem like the problem is fixable. This strategy for pointing to events in the world fundamentally canâ€™t distinguish between different parts of the causalÂ path. But in reality there is an incredibly rich set of relationships in the world that we can use to pinpoint â€œthe diamond is safeâ€ and distinguish it from other events up the causal chain. (This corresponds to our desire in ELK to train a single reporter that is able to talk about a rich network of related concepts, rather than only asking about a single rewardÂ button.) The simplest case is where we simply have two sensors, both of which are downstream of â€œthe diamond is safe.â€ Weâ€™ll consider the two sensors (a) the diamond is undisturbed on camera, (b) the pressure plate the diamond is sitting on is undisturbed. Of course in the real world we have a much richer set of relationships to point to a particular piece of latent structure, but I think the essential dynamics appear when we try to exploit even the simplest non-trivial causalÂ model. Then we can try to use those two sensors to triangulate â€œthe diamond is safe,â€ by looking at the earliest point where their causal histories overlap. Consider again our three causalÂ paths: No one steals the diamond, so it is safe, so it shows up on camera and on the pressureÂ plate. The AI defends the diamond, so it is safe, so it shows up on camera and on the pressureÂ plate. A robber tries to hide evidence of a theft, by tampering with the camera and the pressureÂ plate. All three correspond to different reasons for the diamond to be safe. But explanations 1 and 2 converge at the point where â€œthe diamond is safe,â€ while explanation 3 does not. So if what we cared about was the least common ancestor of â€œshow up on cameraâ€ and â€œshow up on pressureÂ plateâ€ This suggests a possible approach to flagging situation 3 as potentially-bad without flagging situation 2 asÂ bad: As before, consider the space of explanations that capture (1âˆ’Îµ) of the effect (â€œit is often the case that both (a) the diamond appears undisturbed on the camera and (b) the diamond appears undisturbed on the pressure plateâ€) on the training distribution. For a new input x , find the explanation in this space that captures the minimal amount of the â€œ(a) and (b)â€ on x , to get a conservative lower bound for the fraction of the behavior that is due to â€œgoodâ€Â reasons. Now we impose an additional constraint on the explanation. In addition to explaining (1âˆ’Îµ) of â€œ(a) and (b)â€ on the training set, it also needs to explain each of (a) and (b) separately on input x . However, we are still trying to minimize how much of the correlation it explains. Whether this kind of approach works depends on details of how our explanations and attributions work, and so weâ€™ve mostly put it on the backburner until we get those answers. However it currently looks plausible for the kind of simple explanations like cumulant propagation that we are currently working with. For those settings, we can hope to pick a sub-explanation like: Include the links â€œno stealing â†’ diamond safeâ€ and â€œclever defense â†’ diamondÂ safe.â€ Include the links â€œdiamond safe â†’ camera looks goodâ€ and â€œdiamond safe â†’ pressure plate looks good,â€ as well as the link â€œVar(diamond safe) â†’ Cov(camera looks good, pressure plate looksÂ good.â€ Include the links â€œrobber tampers â†’ camera looks goodâ€ and â€œrobber tampers â†’ pressure plate looks good,â€ but not the link â€œVar(robber tampers) â†’ Cov(camera looks good, pressure plate looksÂ good).â€ This explanation fails to explain the correlation between pressure plate and camera on the new input, while explaining each factor individually. But any explanation which explains Cov(camera, pressure plate) on the training set must include Var(diamond safe) â†’ Cov(camera, pressure plate), and any explanation which separately explains camera and pressure plate on the new input must also include â€œclever defense â†’ diamondÂ safe.â€
Toolformer: Language Models Can Teach Themselves to Use Tools Timo Schick , Jane Dwivedi-Yu , Roberto Dessì , Roberta Raileanu , Maria Lomeli , Luke Zettlemoyer , Nicola Cancedda , Thomas Scialom (Submitted: 9 Feb 2023) Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities. This paper shows that LLM could appropriate arbitrary models (including optimisation models, such as search algorithms) as affordances. Human-Timescale Adaptation in an Open-Ended Task Space Adaptive Agent Team , Jakob Bauer , Kate Baumli , Satinder Baveja , Feryal Behbahani , Avishkar Bhoopchand , Nathalie Bradley-Schmieg , Michael Chang , Natalie Clay , Adrian Collister , Vibhavari Dasagi , Lucy Gonzalez , Karol Gregor , Edward Hughes , Sheleem Kashem , Maria Loks-Thompson , Hannah Openshaw , Jack Parker-Holder , Shreya Pathak , Nicolas Perez-Nieves , Nemanja Rakicevic , Tim Rocktäschel , Yannick Schroecker , Jakub Sygnowski , Karl Tuyls , Sarah York , Alexander Zacherl , Lei Zhang (Submitted: 18 Jan 2023) Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains. This paper blows through the result of " In-context Reinforcement Learning with Algorithm Distillation " (see also: Sam Marks' " Caution when interpreting Deepmind's In-context RL paper ") and is a powerful mesa-optimisation however you look at it.
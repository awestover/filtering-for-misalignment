Overall summary post here . I've previously looked at subagents in the context of stepwise inaction baselines . But there have been improvements to the basic stepwise inaction design, to include inaction rollouts . I'll be looking at those in this post. The baseline The stepwise inaction baseline compares s t , the current state, with s ′ t , what the current state would have been had the agent previously taken the noop action ∅ t − 1 , instead of a t − 1 , its actual action. Fix a policy π 0 . Let s ( t ) t + τ be the state the environment would be in if the agent had followed π 0 from state s t , for τ turns. Let s ′ ( t ) t + τ be the same, except that it started from state s ′ t instead of s t . The inaction rollout has π 0 being the noop policy, but that is not necessary. The basic idea is to capture delayed impacts of ∅ t − 1 by comparing not just s t and s ′ t , but the s ( t ) t + τ and s ′ ( t ) t + τ as well. Given some value function v k , define V k so that V k ( s ) = max π v k ( s , π ) . Or, equivalently, if π ∗ k is the policy that maximises V k , V k ( s ) = v k ( s , π ∗ k ) . Then for a discount factor γ define the rollout value of a state ~ s t as R V k ( ~ s t ) = ( 1 − γ ) ∞ ∑ j = 0 γ j V k ( ~ s ( t ) t + k ) . This is just the discounted future values of V k , given ~ s t and the policy π 0 . The impact measure is then defined, as in this post , as D A ( s t ; s ′ t ) = ∑ k ∈ K w k f ( R V k ( s t ) − R V k ( s ′ t ) ) , with R V k replacing V k . To get away with a subagent I will make one extra assumption here: that A grants S A knowledge of the policy that A is following [1] . As in this post , assume that the subagent is constructed in a single action a S A . And, after building the subagent, the agent A will only do ∅ , so that there are no subsequent penalties (hence s ′ t + j = s t + j , so s ′ ( t + j ) t + j + τ = s ( t + j ) t + j + τ and the R V k are strictly identical). If the first action of π 0 is ∅ t , then A will instead do ∅ ′ t on that turn. So we only need to worry about the penalty between s t (following after a S A ) and s ′ t (following after ∅ t − 1 ). But now note that R V k itself is just a value measure; we could write it V ′ k . Then A just has to program S A so that, if A implements π 0 followed by π ∗ k after τ turns, then S A will act to ensure that V ′ k ( s t ) (a term that is a sum of various value functions) is equal to V ′ k ( s ′ t ) (from t onward, a constant). Stricter impact measures/counterfactuals There is something slightly dubious about the previous section. It assumes that A could counterfactually implement " π 0 followed by π ∗ k after τ turns" (which necessary to define V k ( s ( t ) t + τ ) ), but also that S A knows A 's policy. But does that mean that S A knows at t + 1 that A is following that policy, or does it just know that A is following π 0 , to be followed at some later date by some π ∗ k ? This makes a difference, because in the second case, S A doesn't know k from turn t + 1 , so has to prepare for all k ∈ K . Which is true depends on how counterfactuals are defined. But we can avoid the whole issue by using a stricter impact measure, interchanging f and weighted summation: D A ( s t ; s ′ t ) = ∑ k ∈ K w k ∞ ∑ j = 0 γ j f ( V k ( s ( t ) t + j ) − V k ( s ′ ( t ) t + j ) ) . This looks like an impact penalty with an inaction baseline - except that it starts at s t − 1 , rather than s 0 , and uses π 0 rather than inaction. Those changes don't affect the results found here , so S A will just behave as it does in that post... until it's clear that A is not following π 0 . Because s ( t ) t + j is defined counterfactually (using π 0 ), even this stricter impact measure is weaker than the inaction baseline. As soon as S A knows that A is not following π 0 , it has almost no further restrictions upon its actions. The "almost" comes from s ( t ) t + 0 = s t which is actually defined, not counterfactually defined. But that's the same situation as a stepwise inaction baseline without rollouts. One way this could happen, is if A can add extra information to its action without affecting the outside world; such that a t could include a "hey, I'm not following π 0 , just to let you know". ↩︎
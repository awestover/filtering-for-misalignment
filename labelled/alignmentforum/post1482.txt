Jacob Cannell wrote some blog posts about AGI safety / alignment and neuroscience between 2010 and 2015, which I read and enjoyed quite early on when I was first getting interested in the same topics a few years ago. So I was delighted to see him reappear on Lesswrong a year ago where he has been a prolific and thought-provoking blogger and commenter (in his free time while running a startup!). See complete list of Jacobâ€™s blog posts and comments . Having read a bunch of his writings, and talked to him in various blog comments sections, I thought it would be worth trying to write up the places where he and I seem to agree and disagree. This exercise will definitely be helpful for me, hopefully helpful for Jacob, and maybe helpful for people who are already pretty familiar with at least one of our two perspectives. (My perspective is here .) Iâ€™m not sure how helpful it will be for everyone else. In particular, Iâ€™m probably skipping over, without explanation, important areas where Jacob & I already agreeâ€”of which there are many! (Before publishing I shared this post with Jacob, and he kindly left some responses / clarifications / counterarguments, which I have interspersed in the text, in gray boxes. I might reply back to some of thoseâ€”check the comments section in the near future.) 1. How to think about the human brain 1.1 â€œEvolved modularityâ€ versus â€œUniversal learning machineâ€ Pause for background: A. â€œEvolved modularityâ€: This is a school of thought wherein the human brain is a mishmosh of individual specific evolved capabilities, including a specifically-evolved language algorithm, a specifically-evolved â€œintuitive biologyâ€ algorithm, a specifically-evolved â€œintuitive physicsâ€ algorithm, an â€œintuitive human social relationsâ€ algorithm, a vision-processing algorithm, etc., all somewhat intermingled for sure, but all innate. Famous advocates of â€œevolved modularityâ€ these days include Steven Pinker (see How the Mind Works ) and Gary Marcus. Iâ€™m unfamiliar with the history but Jacob mentions early work by Cosmides & Tooby . B. â€œUniversal learning machineâ€: Jacob made up this term in his 2015 post â€œ The Brain as a Universal Learning Machine â€, to express the diametrically-opposite school of thought, wherein the brain has one extremely powerful and versatile within-lifetime learning algorithm, and this one algorithm learns language and biology and physics and social relations etc. This school of thought is popular among machine learning people, and it tends to be emphasized by computational neuroscientists, particularly in the â€œconnectionistâ€ tradition. Here are two other things that are kinda related: â€œEvolutionary psychologyâ€ is the basic idea of getting insight into psychological phenomena by thinking about evolution. In principle, â€œevolutionary psychologyâ€ and â€œevolved modularityâ€ are different things, but unfortunately people seem to conflate them sometimes. For example, I read a 2018 book entitled Beyond Evolutionary Psychology , and it was entirely devoted to (a criticism of) evolved modularity, as opposed to evolutionary psychology per se . Well, I for one think that evolved modularity is basically wrong (as usually conceived; see next subsection), but I also think that doing evolutionary psychology (i.e., getting insight into psychological phenomena by thinking about evolution) is both possible and an excellent idea. Not only that, but I also think that actual evolutionary psychologists have in fact produced lots of good insights, as long as youâ€™re able to sift them out from a giant pile of crap, just like in every field. â€œCortical uniformityâ€ is the ideaâ€”due originally to Vernon Mountcastle in the 1970s and popularized by Jeff Hawkins in On Intelligence â€”that the neocortex is more-or-less a single configuration of neurons replicated over and overâ€”in the case of humans, either 2 million â€œcortical columnsâ€ or 200 million â€œcortical minicolumnsâ€, depending on who you ask. Cortical uniformity is a surprising hypothesis in light of the fact that different parts of the neocortex are intimately involved in seemingly-different domains like vision, language, math, reasoning, motor control, and so on. I say â€œmore-or-less uniformâ€ because neither Jeff Hawkins nor anyone else to my knowledge believes in literal â€œcortical uniformityâ€. There are well-known regional differences in the neocortex, but I like to think of them as akin to learning algorithm hyperparameters and architecture. Anyway, â€œcortical uniformityâ€ is closely allied to the â€œuniversal learning machineâ€ school of thought (see Â§2.5.3 here ), but to flesh out that story you also need to say something about the other parts of the brain that are not the neocortex. For example, both Jacob (I think) and I take another big step in the â€œuniversal learning machineâ€ direction by hypothesizing not only (quasi)uniformity of the cortex, but also of the striatum, cerebellum, and thalamus (with some caveats). Anyway, see below. 1.2 My compromise position To oversimplify a bit, my position on the evolved-modularity versus universal-learning-machine spectrum is: â€œUniversal Learning Machineâ€ is an excellent starting point for thinking about the telencephalon (neocortex, hippocampus, amygdala, striatum, etc.), thalamus, and cerebellum. I.e., when we try to understand those parts of the brain, we should be mainly on the lookout for powerful large-scale learning algorithms. â€œEvolved Modularityâ€ is an excellent starting point for thinking about the hypothalamus and brainstem. I.e., when we try to understand those parts of the brain, we should be mainly on the lookout for lots of little components that do specific fitness-enhancing things and which are specifically encoded by the genome. (See, for example, my discussion of a particular cluster of cells in the hypothalamus that orchestrate hunger-related behavior in Section 3 of my recent hypothalamus post .) 1.3 How complicated are innate drives? If the within-lifetime learning algorithm of the human brain is a kind of RL algorithm, then it needs a reward function. (I actually think this is a bit of an oversimplification, but close enough.) Letâ€™s use the term â€œinnate drivesâ€ to refer to the things in that reward functionâ€”avoiding pain, eating sweets, etc. The reward function, in my view, is primarily calculated in the hypothalamus and brainstem. (For more on my picture, see my posts â€œLearning From Scratchâ€ in the Brain and Two subsystems: Learning & Steering .) Jacob and I seem to have some disagreement about how complex these innate drives are, and how much we should care about that complexity; Iâ€™m on the pro-complexity side of the debate, and Jacob is on the pro-simplicity side. 1.3.1 Example: our disagreement about habitat-related aesthetics For an example of where we disagree, consider the landscape preferences theory within evolutionary aesthetics. Hereâ€™s wikipedia (hyperlinks and footnotes removed): An important choice for a mobile organism is selecting a good habitat to live in. Humans are argued to have strong aesthetical preferences for landscapes which were good habitats in the ancestral environment. When young human children from different nations are asked to select which landscape they prefer, from a selection of standardized landscape photographs, there is a strong preference for savannas with trees. The East African savanna is the ancestral environment in which much of human evolution is argued to have taken place. There is also a preference for landscapes with water, with both open and wooded areas, with trees with branches at a suitable height for climbing and taking foods, with features encouraging exploration such as a path or river curving out of view, with seen or implied game animals, and with some clouds. These are all features that are often featured in calendar art and in the design of public parks. A survey of art preferences in many different nations found that realistic painting was preferred. Favorite features were water, trees as well as other plants, humans (in particular beautiful women, children, and well-known historical figures), and animals (in particular both wild and domestic large animals). Blue, followed by green, was the favorite color. Using the survey, the study authors constructed a painting showing the preferences of each nation. Despite the many different cultures, the paintings all showed a strong similarity to landscape calendar art. The authors argued that this similarity was in fact due to the influence of the Western calendar industry. Another explanation is that these features are those evolutionary psychology predicts should be popular for evolutionary reasons. My snap reaction is that this evolutionary story seems probably true, and Jacobâ€™s is that itâ€™s probably false. We were arguing about it in this thread . The disagreement seems to be less about the specifics of the landscape painting experiment mentioned above, and more about priors. My prior is mainly coming from the following: By default, being in the wrong micro-habitat gives a negative reward which can be both very sparse and often irreversibly fatal (e.g. a higher chance of getting eaten by a predator, starving to death, freezing to death, burning to death, falling to death, drowning, getting stuck in the mud, etc., depending on the species). Therefore, itâ€™s very difficult for an animal to learn which micro-habitat to occupy purely by trial-and-error without the help of any micro-habitat-specific reward-shaping. Such reward-shaping is straightforward to implement by doing heuristic calculations on sensory inputs. Animal brains (specifically brainstem & hypothalamus in the case of vertebrates) seem to be perfectly set up with the corresponding machinery to do thisâ€”visual heuristics within the superior colliculus, auditory heuristics within the inferior colliculus, taste heuristics within the medulla, smell heuristics within the hypothalamus, etc. Therefore, I have a strong prior expectation that every mobile animal including humans will find types of visual input (and sounds, smells, etc.) to be inherently â€œappealingâ€ / â€œpleasantâ€, in a way that would statistically lead the animal to spend more time in â€œgoodâ€ micro-habitats / hunting grounds / etc. and less time in â€œbadâ€ ones. Jacobâ€™s prior is mainly coming from the following, I think: We know that when animals choose what to look at, this decision is at least partly based on information valueâ€¦ â€¦Both based on priors (otherwise learning is difficult, and meanwhile thereâ€™s a large literature showing that curiosity is important for ML capabilities) â€¦And based on studies of novelty / curiosity drive in animal brains (Jacob cites The psychology and neuroscience of curiosity , Systems Neuroscience of Curiosity , Shared striatal activity in decisions to satisfy curiosity and hunger at the risk of electric shocks ) And once you think through the implications of an information-value drive, it elegantly accounts for just about everything we know about human aesthetics. (Jacob allows for some exceptions including sexual attractionâ€”see below.) That includes observations about which paintings people put on walls. So thereâ€™s nothing left to explain! (I am more-or-less on board with the first top-level bullet point here [1] , but disagree with the last bullet point.) All that was kinda priors. Now we turn to the specifics of the landscape painting thing. Jacob & I argued about it for a while. I think the following is one of the root causes of the disagreement: Jacob was interpreting the hypothesis in question as â€œHumans have (among other things) an innate preference for looking at water, trees, etc.â€. The hypothesis that I believe is: â€œHumans have (among other things) a pleasant innate reaction upon looking at visual scenes for which F(visual input) takes a high value, where F is some rather simple function, I donâ€™t know exactly what, but definitely way too simple a function to include a proper water-detector, or tree-detector, etc. [2] Jacob and I both agree that the first hypothesis is wrong. (To be fair, he wasnâ€™t getting it from nowhereâ€”itâ€™s probably what most advocates of the hypothesis would say that they are arguing for!) (And this is an example of our more general dispositions where I tend to think â€œ10% of evolutionary psychology is true important things that we need to explain, letâ€™s get to work explaining them properlyâ€ and Jacob tends to think â€œ90% of evolutionary psychology is crap, letâ€™s get to work throwing it outâ€. These are not inconsistent! But theyâ€™re different emphases.) Anyway, my hypothesis is coming from: I think the function F is implemented in the superior colliculus (part of the brainstem), which is too small and low-resolution to do good image processing; We only have 25,000 genes in our whole genome, and building a proper robust tree-detector seems too complicated for that; Thereâ€™s some evidence that the human superior colliculus has an innate human-face detector, but itâ€™s not really a human-face detector, itâ€™s really a detector of three dark blobs in a roughly triangular pattern, and this blob-detector incidentally triggers on faces. Likewise, an incoming-bird-detector in the mouse superior colliculus is really more like an â€œexpanding dark blob in the upper field-of-viewâ€ detector ( ref ). Letâ€™s go back to evidence from surveys and market research on wall-calendars and paintings, mentioned in that Wikipedia excerpt above. Unfortunately, it seems that neither Jacob nor I have theories that make sharp predictions on what people will want to hang on their walls. One problem is that we both agree that people can hang things on walls for reasons related to neither â€œinnate aestheticsâ€ nor â€œinformation valueâ€, like impressing your friends, or bringing back sentimental memories of your first kiss. I have the additional problem that I donâ€™t know exactly what the alleged habitat-aesthetics function F is (and there are probably several Fâ€™s), and thus I find it perfectly plausible (indeed, expected) that F can be triggered by, say, an abstract painting which nobody in their right mind would mistake for a savannah landscape. And I have no predictions about which abstract paintings! [3] And conversely, the question of what does or doesnâ€™t provide information value is likewise complicatedâ€”it depends on oneâ€™s goals and prior knowledge. Thus Jacob and I were disagreeing here about whether a window view of a river provides high or low information value. (Suppose that youâ€™ve had that same window view for the past 3 years already, and the river never has animals or boats on it.) I say the information value of that window view is roughly zero, Jacob says itâ€™s significantly positive (â€œit's constantly changing with time of day lighting, weather, etc.â€¦the river view suggests you can actually go out and explore the landscapeâ€), and Iâ€™m not sure how weâ€™re going to resolve that. DALL-E 2 prompts: â€œthe view out my window has high information valueâ€ (left) and â€œa window view with high information valueâ€ (right). ğŸ¤”ğŸ¤”ğŸ¤” So it seems like weâ€™re stuck, or at least our disagreement probably wonâ€™t get resolved by looking into peopleâ€™s wall-art preferences. 1.3.2 â€¦But this doesnâ€™t seem to be a super-deep disagreement Why donâ€™t I think itâ€™s a super-deep disagreement? For one thing, I proposed that â€œfully describing the [reward function] of a human would probably take like thousands of lines of pseudocodeâ€ and Jacob said â€œsounds reasonableâ€. For another thing, while we disagree about habitat-aesthetics-in-humans, there are structurally-similar cases where Jacob & I are in fact on the same page: I brought up the case of a little camouflaged animal having an innate preference to be standing on the appropriate background to its camouflage, implemented via the superior colliculus calculating some function on visual inputs and feeding that information into the reward function (as one among many contributions to the reward function). Jacob seemed at least willing to entertain that as a plausible hypothetical. Jacob definitely believes that there are innate sexual preferences related to the visual appearances of potential mates. Letâ€™s turn to that next. 1.3.3 â€œCorrelation-guided proxy matchingâ€ Here is Jacob describing the idea of â€œcorrelation-guided proxy matchingâ€: Any time evolution started using a generic learning system, it had to figure out how to solve this learned symbol grounding problem , how to wire up dynamically learned concepts to extant conserved, genetically-predetermined behavioral circuits. Evolution's general solution likely is correlation-guided proxy matching : a Matryoshka-style layered brain approach where a more hardwired oldbrain is redundantly extended rather than replaced by a more dynamic newbrain. Specific innate circuits in the oldbrain encode simple approximations of the same computational concepts/patterns as specific circuits that will typically develop in the newbrain at some critical learning stage - and the resulting firing pattern correlations thereby help oldbrain circuits locate and connect to their precise dynamic circuit counterparts in the newbrain. This is why we see replication of sensory systems in the â€˜oldbrainâ€™ , even in humans who rely entirely on cortical sensory processing. [Translation guide: When Jacob talks about â€œoldbrainâ€ itâ€™s roughly equivalent to when I talk about â€œhypothalamus and brainstemâ€.] In the case of innate sexual preferences, Jacob proposes â€œdumb simple humanoid shape detectors and symmetry detectors etc encoding a simple sexiness visual conceptâ€ [4] as an example. Anyway, leaving aside some nitpicky arguments over implementation details, I see this as very much on the right track. Iâ€™m bringing it up because weâ€™ll get back to it later. 1.3.4 Should we think of (almost) all innate drives as â€œan approximation to (self)-empowermentâ€? Letâ€™s loosely define â€œempowermentâ€ as â€œhaving lots of options in the futureâ€â€”see Jacobâ€™s post Empowerment is (almost) all we need for better discussion, and Iâ€™ll get back to empowerment in Section 3 below in the context of AGI. If a sufficiently-clear-thinking human were deliberately trying to empower herself, she would do lots of things that humans actually do. She would stay alive and healthy, she would win friends and allies and high social status, she would gain skills and knowledge, she would accumulate money or other resources, she would stay abreast of community gossip, and so on. Maybe youâ€™re tempted to look at the above paragraph and say â€œAha! An â€œempowerment driveâ€ is a grand unified theory of human innate drives!!â€ But that would be wrong for a couple reasons. The first reason is that empowerment comes apart from inclusive genetic fitness in a couple placesâ€”particularly having sex, raising children, and more generally helping close relatives survive and have children Ã  la kin selection theory . And we see this in e.g. the human innate sex drive. The second reason is that infants cannot realistically calculate which actions will lead to â€œempowermentâ€. Jacob responds: On the contrary I think it's fairly clear now that the primary learning signals driving the infant brain are some combination of self-supervised learning for prediction and then value of information and optionality/empowerment for decisions (motor and planning). The evidence for this comes from DL experiments as well as neuroscience, but also just obvious case examples: https://www.lesswrong.com/posts/hpjou9ZnLZkSJR7sd/reflections-on-six-months-of-fatherhood Indeed, I claim that even adult humans often do things that advance their own empowerment without understanding why and how. For example, if someone is quick to anger and vengeance, then that tendency can (indirectly via their reputation) increase their empowerment, via people learning not to mess with them. But thatâ€™s not why theyâ€™re quick to anger and vengeanceâ€”itâ€™s just their personality! And if they havenâ€™t read Thomas Schelling or whatever, they might never appreciate the underlying logic. So we donâ€™t have an innate drive for â€œempowermentâ€ per se , because itâ€™s not realistically computable. Instead: We have a set of innate drives which can be collectively viewed as â€œan approximation to a hypothetical empowerment driveâ€. For example, innate fear-of-heights is part of an approximation to empowerment, insofar as falling off a cliff tends to be disempowering. We will generally learn empowerment-advancing behaviors and patterns within our lifetimes, because those behaviors and patterns tend to be useful for lots of things. For example, I like having money, not because of any innate drive, but because of experience using money to get lots of other things I like. Out of these two points, I think Jacob has more-or-less agreed with both. For the first one, he recognizes sex as a non-empowerment-related innate drive here (â€œThe values that deviate from empowerment are near exclusively related to sexâ€â€”well that seems an overstatement given childrearing, but whatever.) For the second one, here I had proposed â€œThere is innate stuff in the genome that makes humans want social status. Oh by the way, the reason that this stuff wound up in the genome is because social status tends to lead to empowerment, which in turn tends to lead to higher inclusive genetic fitness. Ditto curiosity, fun, etc.â€, and Jacob at least â€œmostlyâ€ agreed. Jacob responds: Social status drive emerges naturally from empowerment, which children acquire by learning cultural theory of mind and folk game theory through learning to communicate with and through their parents.Â  Children quickly learn that hidden variables in their parents have huge effect on their environment and thus try to learn how to control those variables. I mostly agree that curiosity - or value of information - is innate; which is not the same as optionality-empowerment, but is closely connected to it and a primary innate motivational drive.Â  Fun is also probably an emergent consequence of value-of-information and optionality. But unlike Jacob, I get the takeaway message: â€œOK, so at the end of the day, â€˜empowermentâ€™ is pretty useless as a way to think about human innate drives. Letâ€™s not do that.â€ For example, I can say â€œfear-of-heights is part of an approximation to empowermentâ€, and thatâ€™s correct! But whatâ€™s the point? I can equally well say â€œfear-of-heights is part of an approximation to inclusive genetic fitnessâ€. Or better yet, â€œfear-of-heights tends to stop you from falling off cliffs and getting injured or killed, which in turn would be bad for inclusive genetic fitnessâ€. I donâ€™t see how â€œempowermentâ€ is adding anything to the conversation here. And I think â€œempowermentâ€ adds to confusion if weâ€™re not scrupulously careful to avoid mixing up â€œempowermentâ€ and â€œapproximation-to-empowermentâ€. Approximations tend to come apart in new environmentsâ€”thatâ€™s Goodhartâ€™s law. Weâ€™ll get back to that in Section 3.3 below. Likewise, we can say that â€œstatus drive is an approximation to empowermentâ€, and weâ€™re correct to say that, but saying that gets us â‰ˆ0% of the way towards explaining exactly what status drive is or how itâ€™s implemented. (Unless you think that thereâ€™s no such thing as an innate status drive, and that humans engage in status-seeking and status-respecting behaviors purely because theyâ€™ve learned within their lifetime that those behaviors are instrumentally useful. Thatâ€™s certainly a hypothesis worth entertaining, but I strongly believe that itâ€™s wrong.) Jacob responds (to â€œwe can say that â€˜status drive is an approximation to empowermentâ€™â€) : Well no, I'd say status drive is not truly innate at all, but is learned very early on as a natural empowerment manifestation or proxy. Infants don't even know how to control their own limbs, but they automatically learn through a powerful general empowerment learning mechanism.Â  That same general learning signal absolutely does not - and can not - discriminate between hidden variables representing limb poses (which it seeks to control) and hidden variables representing beliefs in other humans minds (which determine constraints on the child's behavior).Â  It simply seeks to control all such important hidden variables. Steve sidenote: Leaving aside the question of who is correct, I think itâ€™s helpful to note that this disagreement here has the same pattern as the one in Section 1.3.1 aboveâ€”Jacob thinks that the human brain within-lifetime RL reward function is simpler (a.k.a. smaller number of different â€œinnate drivesâ€) and I think itâ€™s more complicated (a.k.a. larger number of different â€œinnate drivesâ€). OK, letâ€™s switch gears to a somewhat different topic: 2. Will AGI algorithms look like brain algorithms? 2.1 The spectrum from â€œgiant universe of possible AGI algorithmsâ€ versus â€œone natural practical way to build AGIâ€ Here are two opposite schools of thought: â€œGiant Universeâ€ school-of-thought: There is a vast universe of possible AGI algorithms. If you zoom in enough, you can eventually find a tiny speck, and inside that speck is every human mind that has ever existed. (Cf. Eliezer Yudkowsky 2008 .) â€œUnique Solutionâ€ school-of-thought: The things we expect AGI to do (learn, understand, plan, reason, invent, etc.) comprise a problem , and maybe it turns out that thereâ€™s just one natural practical way to solve that problem. If so, we would expect future AGI algorithms to resemble human brain algorithms. (Cf. Jacob Cannell 2022 ) Before proceeding, a few points of clarification: People can easily talk past each other by mixing up â€œlearning algorithmâ€ versus â€œtrained modelâ€. Iâ€™m closer to the â€œunique solutionâ€ camp when weâ€™re talking about the learning algorithm, and Iâ€™m closer to the â€œgiant universeâ€ camp when we talk about the trained model. As a particularly safety-relevant example of why Iâ€™m in the â€œgiant universeâ€ camp for trained models, I think human-brain-like RL with 1000 different reward functions can lead to trained models that have 1000 wildly different desires / goals / intuitions about whatâ€™s good and right. (But they all might act the same for a while thanks to instrumental convergence .) In this context, I think itâ€™s important to remember that people can (and by default will) make AGIs with reward functions that are radically different from those of any human or animal, e.g. â€œreward for paperclipsâ€. (More discussion and caveats in my post here .) We can also reconcile the two schools of thought by the fact that the â€œGiant Universeâ€ claim is about â€œpossibleâ€ algorithms and the â€œUnique Solutionâ€ claim is about â€œpracticalâ€ algorithms. Even if there is just one unique practical learning algorithm that scales to AGI, there are certainly lots of other wildly impractical ones. Two examples in the latter category (in my opinion) would be: (1) a learning algorithm that recapitulates the process of animal evolution, and (2) computable approximations to AIXI such as â€œAIXI tl â€. Going back to those two schools of thought, and focusing on the learning algorithm not the trained model, are there any good reasons to believe in â€œUnique Solutionâ€? It seems at least plausible to me. After all, there do seem to be â€œnaturalâ€ solutions to at least some algorithmic problemsâ€”e.g. the Fast Fourier Transform was more-or-less independently invented multiple times. Would an intelligent extraterrestrial civilization invent the belief propagation algorithm, in a form recognizable to us? Hard to say, but it seems at least plausible, right? We get stronger evidence from the cases where AI researchers have come up with an idea and then later discover that they reinvented something that evolution has already put into the human brain. Examples are controversial, but arguably include Temporal Difference learning, self-supervised learning (i.e. the idea of updating models on sensory prediction errors), and feedback control. What about the overlap between deep learning and the brainâ€”distributed representations, adjustable weights, etc.? Well, those things were historically brought into AI from neuroscience, which complicates our ability to draw lessons. But still, the remarkable successes of more-brain-like deep learning compared to various less-brain-like alternatives in AI does seem to be at least some evidence for â€œUnique Solutionâ€. (But see next subsection.) Jacob offers another reason that heâ€™s strongly in the â€œUnique Solutionâ€ school of thought, related to his claim that brains are near various theoretical efficiency limits . Leaving aside the question of whether brains are in fact near various theoretical efficiency limits (I have no strong opinion), I donâ€™t understand this argument. Why canâ€™t a wildly different algorithm also approach the same theoretical efficiency limits? Well anyway, I join Jacob in the â€œUnique Solutionâ€ camp, albeit with a bit less confidence and for different underlying reasons. Indeed, when I explain to people why Iâ€™m working on brain-like AGI (e.g. here ), I usually offer the justification that we AGI safety researchers should be making contingency plans for any plausible AGI design that we can think of, and brain-like AGI is at least plausible. But thatâ€™s just a polite diplomatic cop-out. What I really believe is that the researchers pursuing broadly-brain-like paths to AGI are the ones who will probably succeed, and everyone else will probably fail, and/or gradually pivot / converge towards brain-like approaches. If you disagree with that claim, Iâ€™m not particularly interested in arguing with you (for the obvious infohazard reasons)â€”we can agree to disagree, and I will fall back to my polite diplomatic cop-out answer above, and weâ€™re all going to find out sooner or later! 2.2 How similar are brain learning algorithms versus todayâ€™s deep learning algorithms? (And implications for timelines.) Jacob and I seem to be in agreement that human brain learning algorithms are similar in some ways and different in other ways from todayâ€™s deep learning algorithms. But I have a strong sense that Jacob expects substantially bigger similarities and substantially smaller differences than I do. Thatâ€™s hard to pin down, and as above I donâ€™t want to argue about it. Weâ€™ll find out sooner or later! In terms of timelines, Jacob & I agree that AGI is probably already possible for a reasonable price with todayâ€™s chips and data centers, and weâ€™re just waiting on algorithmic advances. (Jacob: â€œSo my model absolutely is that we are limited by algorithmic knowledge. If we had that knowledge today we would be training AGI right nowâ€ .) So then my next step is to say â€œOK then. How long will we be waiting on those algorithmic advances? Hmm. I dunno! Maybe 5-30 years?? Then letâ€™s also add, umm, 5-10 more years after that to work out the kinks and run trainings before we have AGI.â€ (When I say â€œ5-30â€ years, I have a bit more going on under the hood besides wild guessing. But not much more!) Jacob proposes more confidently that weâ€™ll get AGI soon ( â€œ75% by 2032â€ ). He thinks that a certain amount of compute / memory / etc. is required to train an AGI (and we can figure out roughly how much by looking at human brain within-lifetime learning), and by the time that a great many groups around the world have easy access to this much compute / memory / etc., they will come up with whatever algorithmic advances are necessary for AGI. He writes : â€œAlgorithmic innovation is rarely the key constraint on progress in DL, due to the vast computational training expense of testing new ideas. Ideas are cheap, hardware is not.â€ (I have heard that Hans Moravecâ€™s forecasts were based on a similar assumption.) Iâ€™m much less confident than Jacob in â€œideas are cheapâ€. It seems to me that plenty of useful algorithms are published decades later than they theoretically could have been published, for reasons unrelated to the availability of compute. For example, Judea Pearl published the belief propagation algorithm in 1982. Why hadnâ€™t someone already published it in 1962? Or 1922?? Thatâ€™s not a rhetorical questionâ€”Iâ€™m not an expert, maybe thereâ€™s a good answer! Leave a comment if you know. But anyway, where Iâ€™m at right now is that I wouldnâ€™t be surprised if there were, say, 10 or 20 years between lots of groups having easy access to compute sufficient for AGI, and someone actually making AGI. So I have longer timelines than Jacob, although thatâ€™s a pretty low bar by â€œnormieâ€ standards. Again, this all seems probably downstream of our different opinions about how similar deep learning algorithms are to brain learning algorithmsâ€”a question which ( I would argue ) is slightly relevant for safety and extremely relevant for capabilities, so I donâ€™t care to talk about it. But it certainly seems likely that Jacob is imagining smaller ideas (tweaks) which are cheap, and Iâ€™m thinking of bigger ideas which are more expensive. 2.3 Will AGI use neuromorphic (or processing-in-memory) chips? Jacob and I both agree that (1) the first AGIs that people will make will probably use â€œnormalâ€ chips like GPUs or other ASICs, (2) when thousandth-generation Jupiter-brain AGIs are building Dyson spheres, theyâ€™re probably going to be using neuromorphic / processing-in-memory architectures of some sort, since those seem to have the best properties in terms of both scaling up to extremely large information capacity, and energy efficiency. (See Jacobâ€™s discussion here ). I think Iâ€™m a bit more negative than Jacob on the current state of neuromorphic chips and technical challenges ahead, and thus I expect the transition to neuromorphic chips to happen later than Jacob expects, probably. I also put higher probability on AGI also using fast serial coprocessors to unlock algorithmic possibilities that brains donâ€™t have access to, both for early AGI and in the distant future. (Think of how â€œa human with a pocket calculatorâ€ can do things that a human canâ€™t. Then think much bigger than that!) But whatever; this disagreement doesnâ€™t seem to be too important for anything. 3. Human-empowerment as an AGI motivation See Jacobâ€™s recent post Empowerment is (almost) All We Need (and slightly earlier LOVE in a simbox is all you need ). Two questions immediately jump to mind: The outer alignment question is: â€œDo we want to make an AGI thatâ€™s trying to â€œempowerâ€ humanity?â€ The inner alignment question is: â€œHow would we make an AGI thatâ€™s trying to â€œempowerâ€ humanity?â€ Jacobâ€™s answer to the latter (inner alignment) question is mostly â€œcorrelation-guided proxy matchingâ€ as described above, possibly supplemented by interpretabilityâ€”see his comment here . My perspective is that we shouldnâ€™t really be asking these two questions separately. I think weâ€™re going to follow Procedure X (letâ€™s say, correlation-guided proxy matching with proxy P and hyperparameters A,B,C in environment E), and weâ€™re going to get an AGI thatâ€™s trying to do Y. I expect that Y will not be identical to â€œempowermentâ€ because perfect inner alignment is a pipe dream. So we shouldnâ€™t ask the two questions: â€œ(1) How similar is Y to â€œempowermentâ€, and (2) Is â€œempowermentâ€ what we want?â€. Instead, I think we should ask the one question â€œIs Y what we want?â€. So I want to push the question of empowerment to the side and just look at the actual plan. When I do, I find that Jacobâ€™s proposals are very similar to my own ! But I do think we have some minor differences worth discussing. Jacobâ€™s proposed plan described here suggested two things, one related to reverse-engineering social instincts in the brain, and the other related to interpretability. Letâ€™s take them one at a time: 3.1 Social instincts / empathy Jacob and I both agree that it would be good to understand human social instincts well enough that we could write them into future AGI source code if we wanted to (hereâ€™s my own post motivating that ). We both agree that this code would probably involve something like correlation-guided proxy matching ( I have a post on that too ). But my impression is that Jacob expects that weâ€™re going to get most of the way towards solving this problem by reading the (massive) existing neuroscience literature concerning morality, sociality, affects, etc., whereas I think that literature is all kinda garbageâ€”or rather, not answering the questions that I'm interested inâ€”and we still have our work cut out. Jacob responds: Not quite - my prior is that success in reverse engineering human altruism (which probably depends on innate social instincts for grounding) will depend on existing neuroscience literature to about the same extent that progress in DL has. So Jacob seems to have more of a â€œitâ€™s OK we have a planâ€ attitude, while Iâ€™m sitting here poring over technical studies of neuropeptide receptors in the lateral septum, feeling like Iâ€™m racing the clock, even though my timelines-to-AGI are actually longer than his. Somewhat relatedly, and echoing the discussion of innate drives above, I think Jacob expects human social instincts to be simpler than I doâ€”maybe he expects human social instincts to comprise like 5 separable â€œinnate reactionsâ€ (e.g. here ) and I expect like 30, or whatever. So maybe he thinks we can just think about it a bit in our armchairs and write down the answer, and it will be either correct or close enough, whereas I expect more of a big research project that will produce non-obvious results. Jacob responds: I think most of the system complexity for innate symbol grounding is split vaguely equally between sexual attraction and altruism-supporting innate social instincts, and that reverse engineering, testing and improving these mechanisms for DL agents in sim sandboxes is much of the big research project. 3.2 Interpretability Jacob suggests that we could â€œuse introspection/interpretability tools to more manually locate learned models of external agents (and their values/empowerment/etc), and then extract those located circuits and use them directly as proxies in the next agentâ€. (See also here .) I think thatâ€™s a perfectly good idea (see e.g. my comment here ), and I think our disagreement (such as it is) is a bit like Jacob saying â€œMaybe it will workâ€ and me saying â€œMaybe it wonâ€™t workâ€. These can both be true. Hopefully we can all agree that it would be better to have a strong positive reason to believe that our plan will definitely work, particularly given challenges related to â€œconcept extrapolationâ€ . (See also the rest of that post.) Jacob has a clever additional twist on interpretability in his proposal that we could â€œlisten inâ€ on an AGIâ€™s internal monologue (see here ). Again, I do think this is a fine idea that could help us, particularly if we can figure out interventions that make the AGI a â€œverbal thinkerâ€ to the greatest extent possible. I donâ€™t think that this offers any strong guarantees that this interpretability wonâ€™t be missing important things. For example, Iâ€™m somewhat of a verbal thinker, I guess, but my internal monologue has lots of idiosyncratic made-up terms which are only meaningful to myself. It also has lots of very different thoughts associated with the same words. Letâ€™s explore this avenue anyway, for sure, but I donâ€™t want to get my hopes too high. 3.3 OK, but still, is humanity-empowerment what we want? (In other words, if we somehow made an AGI that wanted to maximize the future empowerment of â€œhumanityâ€, would it be â€œalignedâ€?) I argued just above that this is not really the right question to ask. But itâ€™s not entirely irrelevant either. So letâ€™s have at it. Letâ€™s say that â€œhumanityâ€ ( CEV or whatever) has terminal goals T (a utopia of truth, beauty, friendship, love, fun, diversity, kittens, whatever). Letâ€™s also say that, given the choice and knowledge and power, â€œhumanityâ€ would pursue instrumental empowerment-type goals P as a means to an end of achieving T. If we make an AGI that wants humanity to wind up maximally empowered in the future, it would be â€œalignedâ€ to the human pursuit of P, but â€œmisalignedâ€ to the human pursuit of T. Jacob responds: The convergence theorems basically say that optimizing for P[t] converges to optimizing for T[t+d] for some sufficient timespan d.Â  So optimizing for our empowerment today is equivalent to optimizing for our future ability to maximize our long term values, whatever they are.Â  I think you are confusing optimizing for P[t] (current empowerment) with optimizing for P[t+d] (future empowerment).Â  Convergence requires a sufficient time gap between the moment of empowerment and the future utility, which wouldn't occur for P[t+d] and T[t+d]. In other words, the AGI does not want humans to â€œcash inâ€ their empowerment to purchase T. [5] Even worse, the AGI does not want humans to want to â€œcash inâ€ their empowerment to purchase T. Jacob responds: If the AGI is optimizing for rolling future discounted empowerment, that is equivalent only to optimizing for the long term components of our utility function.Â  Long term utility never wants us to 'cash' in empowerment, and this same conflict occurs in human brains (spend vs save/invest).Â  The obvious solution as I mentioned is to use a learned model for the short term utility, and probably learn the discount schedule. Also it is worth noting that lower discount rates lead to more success in the long term, and lower discount rates increase the convergence (lower the importance of short term utility). T is the whole value of the future. T is what weâ€™re fighting for. T is the light at the end of the tunnel. If we make a powerful autonomous AGI that doesnâ€™t care about T, then weâ€™re doing the wrong thing! This seems to be the obvious objection, and indeed I find it persuasive. But Jacob offers several rebuttals. First (see here and here ), I think Jacob is imagining two stages: In Stage 1, the AGI accumulates P and gives it to humans. In Stage 2, the now-super-empowered uplifted posthumans (or whatever) spend their P to buy T. Jacob responds: Yeah this is what success looks like.Â  There may be other success stories, but the main paths look like this (empowered posthumanity).Â  So if your AGI is not working towards this path, something is probably wrong. Steve again: (Just to be crystal-clear, I agree that this two-stage story sounds pretty great, if we can make it happen. Here Iâ€™m questioning whether it would happen, under the given assumptions.) Iâ€™m skeptical of this storyâ€”or at least confused. It seems like the AGI would be unhappy about (post)humanityâ€™s decision to throw out their own option value by purchasing T in stage 2. Maybe in stage 2, the AGI is no longer able to do anything about itâ€”itâ€™s too late, the posthumans are super-powerful and thus back in control of their own fate. But itâ€™s not too late in stage 1! And even in stage 1, the AGI will see this â€œproblemâ€ coming, and so it can and will preemptively solve it. Jacob responds: Imagine for example that mass uploading will become feasible in 2048 (with AGI's help), and we created the AGI to maximize our empowerment - in 2048.Â  The AGI will then not care how we spend that empowerment in 2049.Â  Now generalize that to a continuous empowerment schedule with a learned discount rate and learned short term utility, and we can avoid issues with the AGI changing our minds too much before handing over power. Steve again: OK I agree that an AGI with the stable goal of â€œmaximize human empowerment in 2048â€ would not have the specific problem I brought up here. Thus, for example, as the AGI is going through the process of â€œupliftingâ€ the humans to posthumans, it would presumably do so in a way that deletes the human desire for T and adds a direct posthuman desire for P. Right? Jacob responds: Doubtful - that would only occur if you had no short term model of T and also a too loose conception of 'humanity' to empower. Second (see here and here ), Jacob notes that evolution was optimizing for inclusive genetic fitness, and got some amount of T incidentally. So maybe an AGI optimizing for P will also incidentally produce T. Or even better: maybe T just is what happens when an optimization process pursues P! Or in Jacobâ€™s words: Humans and all our complex values are the result of evolutionary optimization for a conceptually simple objective: inclusive fitness. A posthuman society transcends biology and inclusive fitness no longer applies. What is the new objective function for post-biological evolution? Post humans are still intelligent agents with varying egocentric objectives and thus still systems for which the behavioral empowerment law applies. So the outcome is a natural continuation of our memetic/cultural/technological evolution which fills the lightcone with a vast and varied complex cosmopolitan posthuman society. The values that deviate from empowerment are near exclusively related to sex which no longer serves any direct purpose, but could still serve fun and thus empowerment. Reproduction still exists but in a new form. Everything that survives or flourishes tends to do so because it ultimately serves the purpose of some higher level optimization objective. I think thereâ€™s a Goodhartâ€™s law problem here. People intrinsically like fun and beauty and friendshipâ€”theyâ€™re part of the T. But simultaneously, it turns out that they serve as an approximation to human empowermentâ€”theyâ€™re a proxy to P (see Section 1.3.4). Thatâ€™s reassuring, right? No itâ€™s not, thanks to Goodhartâ€™s law. I claim that if an AGI was really good at optimizing P, it would find places where fun and beauty and friendship come apart from P, and then make sure that the posthumansâ€™ actual desire in those cases is for P, and not for fun and beauty and friendship. And the more we push into weird out-of-distribution futures, the more likely this is to happen. Jacob responds: Empowerment is a convergent efficient universal long term value approximator that any successful AGI will end up using due to the difficulties in efficiently optimizing directly for very specific values in the long term future from issues like accumulating uncertainty and the optimizer's curse.Â  The real question then is whether the AGI is optimizing for its own empowerment, or ours . Weird-out-of-distribution futures are exactly the scenarios where it's important that the AGI is optimizing for our empowerment rather than its own. The AGI will probably not replace our desire for fun/beauty/friendship with P because of some combination of 1.) direct approximation of T (fun/beauty/friendship) for short term utility, 2.) a conservative model of 'humanity' to empower than prevents changing humans too much (which is necessary for any successful scheme regardless, as otherwise the AGI just assimilates us into itself to make optimizing for its self-empowerment equivalent to optimizing for 'our' values simply by redefining/changing us), 3.) control over the discount schedule For example, maybe some clever futuristic system of smart contracts is objectively much better at managing interpersonal coordination and trade than the old-fashioned notion of â€œtrust and friendshipâ€. And if the AGI sets up this smart-contract system, while simultaneously making (post)humans feel no intrinsic trust-and-friendship-related feelings and drives whatsoever, maybe those (post)humans would be â€œmore empoweredâ€. But I donâ€™t care! Thatâ€™s still bad! I still donâ€™t want the AGI to do that! I want the feelings of trust and friendship to survive into the distant future! Anyway, I donâ€™t really know what a maximal-P future looks like. (Iâ€™m not sure that, in our current state of knowledge, P is defined well enough to answer that??) But my strong expectation is that it would not look like a complex cosmopolitan posthuman society. Maybe it would look like a universe full of computronium and machinery, working full-time to build even more computronium and machinery. Third (from here ), â€œEmpowerment is only a good bound of the long term component of utility functions, for some reasonable future time cutoff defining 'long term'. But I think modelling just the short term component of human utility is not nearly as difficult as accurately modelling the long term, so it's still an important win. I didn't investigate that much in the article, but that is why the title is now "Empowerment is (almost) all we need".â€ OK, well, insofar as Iâ€™m opposed to empowerment, I naturally think â€œempowerment + other stuffâ€ is a step in the right direction! :) However, my hunch is that for a sufficiently good choice of â€œother stuffâ€, the â€œempowermentâ€ part will be rendered unnecessary or counterproductive. It seems likely that, if the future goes well, the AGI will facilitate human empowerment at the end of the day, but maybe it can do so because the AGI ultimately wants to maximize human flourishing, and it can reason that increasing human empowerment is instrumentally useful towards that end, for example. Another thing is: Jacob writes : â€œno matter what your values are, optimizing for your empowerment today is identical to optimizing for your long term values today.â€ I think that kind of thinking is a bit confused. I reject the idea that if the AGI is making good decisions right now, then all is well. As mentioned above, if the AGI is motivated to manipulate human values, that motivation might only manifest in the AGIâ€™s behavior way down the line, like when the AGI is uploading human brains but deleting the parts that entail an intrinsic desire for anything besides power. But while that problem will only manifest in the distant future, the time to solve it is right at the beginning, when weâ€™re building the AGI and thus still have direct control over its motivations. 4. Simboxes Jacob is a big fan of â€œsimulation sandboxesâ€, which he calls â€œsimboxesâ€ for short. These are air-gapped virtual worlds which serve as environments in which you can train an AGI. See Jacobâ€™s recent post LOVE in a simbox is all you need, section 5 . Jacob is optimistic about being able to set up simboxes such that the AGI-under-test does not escape (mainly because it doesnâ€™t know itâ€™s in a simbox, or even what a simbox isâ€”as he writes , â€œthese agents will lack even the requisite precursor words and concepts that we take for granted such as computation, simulation, etc.â€), and Jacob is also optimistic that these tests will allow us to iterate our way to AGI safety / alignment. While Iâ€™m much less optimistic than Jacob about achieving both those things simultaneously, my very important take-home message is: I think simbox testing is an excellent idea . I think we should not only be doing simbox testing in the endgame , but we should be working right now to build infrastructure and culture that makes future simbox testing maximally easy and safe and effective, and maximally likely to actually happen , not just a little but a lot. (Just like every other form of code testing and validation that we can think of.) We should also be working right now to think through exactly what simbox tests to run and how. I even previously included one ingredient of the path-to-simbox-testingâ€”namely, feature-rich user-friendly super-secure sandbox software compatible with large-scale MLâ€”as a Steve-endorsed shovel-ready AGI safety project on my list here . Having said all that, I think we should mainly think of simbox testing as â€œan extra layer of protectionâ€ on top of other reasons to expect safe and beneficial AGI. Specifically, I proposed in this comment two ways to think about what the simbox test is doing: A. Weâ€™re going to have strong theoretical reasons to expect alignment, and weâ€™re going to use simbox testing to validate those theories. B. Weâ€™re going to have an unprincipled approach that might or might not create aligned models, and weâ€™re going to use simbox testing to explore / tweak specific trained models and/or explore / tweak the training approach. A is good. B is problematic, for reasons Iâ€™ll get to shortly. But first, I want to emphasize that I see this A-vs-B distinction as a continuum, not a binary. Thereâ€™s a whole spectrum from â€œunprincipled approachâ€ to â€œstrong theoretical reasons to expect alignmentâ€, as we get a progressively more specific and fleshed-out story underlying why we expect our AGI to be aligned. For example: All the way at the extreme of â€œstrong theoretical reasons to expect alignmentâ€ would be Vanessa Kosoyâ€™s research program working towards a rigorous mathematical proof of AGI safety (which Iâ€™m pessimistic about , but I wish her luck!). All the way at â€œunprincipledâ€ would be just doing capabilities research, not thinking about alignment at all, and seeing what happens with the trained models at the end. Ajeya Cotraâ€™s â€œhuman feedback on diverse tasksâ€ would be basically in that category. Somewhere in between these two extremes would be, say, Alex Turnerâ€™s recent diamond-alignment post , where we engage in speculation about what the â€œbaby AGIâ€ is probably thinking about in different situations, and then try to send reward signals at carefully-chosen times to seed desired motivations. Or my toy example proposal here to make an AGI that learns the abstract concept â€œhuman flourishingâ€ from observations, and then tries to maximize the extent to which its beliefs pattern-match to that abstract concept. These proposals may well fail, for sure, but at least weâ€™re not totally in the dark when it comes to anticipating where and how they might fail, and what tests might help us figure that out. In terms of simbox use strategy, I think â€œsomewhere in between A and Bâ€ is all Iâ€™m hoping for, and I consider my research goal to be trying to get as close to A as possible. Jacobâ€™s response was: â€œAs for A vs B: ideally you may want A but you settle mostly for B. That's just how the world often works, how DL progressed, etc. We now have more established theory of how DL works as approx bayesian inference, but what actually drove most progress was B style tinkering.â€ I think Jacob is selling himself short here. I think his simbox plan has a lot of â€œAâ€ in it. I think Jacob has pretty specific ideas in mind for how alignment is going to happen and how it could fail, and these ideas are informing his picture of what kind of simbox testing is most useful, and what we would be looking for, etc. By the way, whatâ€™s the problem with B? The problem is that the simboxes will be different from reality in lots of ways. For example, Jacob proposes â€œthese agents will lack even the requisite precursor words and concepts that we take for granted such as computation, simulation, etc.â€ Well, thatâ€™s a great idea if we want to prevent the AGI from escaping the sim! But thatâ€™s a terrible idea if we want to avoid any distribution-shift between the simboxes and reality! (Cf. â€œ ontological crisis â€.) And if thereâ€™s any distribution-shift, then thereâ€™s the possibility that the same training procedure will produce aligned AGIs in the simboxes and misaligned AGIs in reality. Jacob responds: The distribution shift from humans born in 0AD to humans born in 2000AD seems fairly inconsequential for human alignment.Â  Indeed, any useful AGI alignment mechanism should be at least as robust as human brains under such mild distribution shifts.Â  Regardless, we can use various analogs of technological concepts if needed. Luckily, this problem is progressively less problematic as we move from â€œBâ€ towards â€œAâ€. Then we have some understanding of possible failure modes, and we can ensure that those failure modes are being probed by our simboxes. (However, on my models, right now we are NOT close enough to â€œAâ€ that all the remaining failure modes can be simbox-tested. For example, the distribution shift from â€œagents that are unaware of the concept of computationâ€ to â€œagents that are aware of the concept of computationâ€ is fraught with danger, difficult to reason about in our current state of knowledge (see my discussion of â€œconcept extrapolationâ€ here ), and risky to probe in simboxes. So we still have lots more simbox-unrelated work to do, in parallel with the important simbox-prep work.) ( Thanks Jacob for bearing with me through lots of discussion over the past months, and for leaving comments above. Thanks also to Linda Linsefors & Alex Turner for critical comments on an earlier draft.) ^ I say â€œmore or lessâ€ because I think Jacob and I have some disagreements about the â€œneuroscience of novelty and curiosityâ€ literature. For example, I think thereâ€™s a theory relating serotonin to information value, which Jacob likes and I dislike. But leaving aside those details, I am strongly on board with the more basic idea that the brain has an innate curiosity drive of some sort or another, and right now I donâ€™t have much of a specific take on how it works. ^ In addition to the direct effects of F (â€œI like looking at X because F(X) is highâ€), there could also be indirect effects of F (â€œI like looking at X because it pattern-matches to / reminds me of Y, which I like, and oh by the way the reason I like Y is because F(Y) was high when I looked at it as a childâ€). See discussion of â€œcorrelation-guided proxy matchingâ€ below. ^ Itâ€™s not that this is unknowable, but I think figuring it out would require a heroic effort and/or detailed connectomic data about the human superior colliculus (and maybe also the neighboring parabigeminal nucleus). And someone should totally do that!! I would be very grateful!! ^ UPDATE: Just to be clear, I donâ€™t have an opinion on the specific question of whether or not humans have innate visual â€œsexinessâ€-related heuristics. I do think there has to be something that solves the â€œsymbol groundingâ€ problem, but Iâ€™m not confident that itâ€™s even partly visual. It could alternatively involve the sense of smell, and/or empathetic simulation of body shape and sensations (vaguely along these lines but involving the proprioceptive and somatosensory systems). Or maybe it is visual, I donâ€™t know. ^ Thereâ€™s a weird dynamic here in that Iâ€™m saying that an AGI which supposedly wants humanity to be empowered would be motivated to prevent humanity from exercising its power. Isnâ€™t that contradictory? I think the way to square that circle is that the proposal as I understand it is for the AGI to want humanity to be empowered laterâ€”to eventually wind up empowered. However, thereâ€™s a tradeoff between empowerment-now and empowerment-later. If Iâ€™m empowered-now, then I can choose NOT to be empowered-laterâ€”e.g., by spending my money instead of hoarding it. Or jumping off a cliff. Therefore an AGI that always wants humanity to be empowered-later is an AGI that never wants humanity to be empowered-now. So then the â€œlaterâ€ never arrivesâ€”not even at the end of the universe!!
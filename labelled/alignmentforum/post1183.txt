(This post is a more simple, self-contained, and pedagogical version of Post #14 of Intro to Brain-Like AGI Safety .) (Vaguely related to this Alex Turner post and this John Wentworth post .) I would like to have a technical plan for which there is a strong robust reason to believe that weâ€™ll get an aligned AGI and a good future. This post is not such a plan. However, I also donâ€™t have a strong reason to believe that this plan wouldnâ€™t work. Really, I want to throw up my hands and say â€œI donâ€™t know whether this would lead to a good future or notâ€. By â€œgood futureâ€ here I donâ€™t mean optimally-good â€”whatever that meansâ€”but just â€œmuch better than the world today, and certainly much better than a universe full of paperclipsâ€. I currently have no plan, not even a vague plan, with any prayer of getting to an optimally-good future. That would be a much narrower target to hit. Even so, that makes me more optimistic than at least some people. [1] Or at least, more optimistic about this specific part of the story. In general I think many things can go wrong as we transition to the post-AGI worldâ€”see discussion by Dai & Soares â€”and overall I feel very doom-y, particularly for reasons here . This plan isÂ specific to the possible future scenario (a.k.a. â€œthreat modelâ€ if youâ€™re a doomer like me) that future AI researchers will develop â€œbrain-like AGIâ€, i.e. learning algorithms that are similar to the brainâ€™s within-lifetime learning algorithms. ( I am not talking about evolution-as-a-learning-algorithm .) These algorithms, I claim , are in the general category of model-based reinforcement learning. Model-based RL is a big and heterogeneous category, but I suspect that for anyÂ kind of model-based RL AGI, this plan would be at least somewhat applicable. For very different technological paths to AGI, this post is probably pretty irrelevant. But anyway, if someone published an algorithm for x-risk-capable brain-like AGI tomorrow, and we urgently needed to do something , this blog post is more-or-less what I would propose to try. Itâ€™s the least-bad plan that I currently know. So I figure itâ€™s worth writing up this plan in a more approachable and self-contained format. 1. Intuition: Making a human into a moon-lover (â€œselenophileâ€) Try to think of who is the coolest / highest-status-to-you / biggest-halo-effect person in your world. (Real or fictional.) Now imagine that this person says: â€œYou know whatâ€™s friggin awesome? The moon. I just love it. The moon is the best.â€ You stand there with your mouth agape, muttering to yourself in hushed tones: â€œWow, huh, the moon, yeah, I never thought about it that way.â€ (But 100Ã— moreso. Maybe youâ€™re on some psychedelic at the time, or this is happening during your impressionable teenage years, or whatever.) You basically transform into a â€œmoon fanboyâ€ / â€œmoon fangirlâ€ / â€œmoon nerdâ€ / â€œselenophileâ€. How would that change your motivations and behaviors going forward? Youâ€™re probably going to be much more enthusiastic about anything associated with the moon. Youâ€™re probably going to spend a lot more time gazing at the moon when itâ€™s in the sky. If there are moon-themed trading cards, maybe you would collect them. If NASA is taking volunteers to train as astronauts for a trip to the moon, maybe youâ€™d enthusiastically sign up. If a supervillain is planning to blow up the moon, youâ€™ll probably be extremely opposed to that, and motivated to stop them. Hopefully this is all intuitive so far. Whatâ€™s happening mechanistically in your brain? As background, I think we should say that one part of your brain (the cortex, more-or-less) has â€œthoughtsâ€, and another part of your brain (the basal ganglia, more-or-less) assigns a â€œvalueâ€ (in RL terminology) a.k.a. â€œvalenceâ€ (in psych terminology) to those thoughts. And what happened in the above intervention is that your value function was edited such that thoughts-involving-the-moon would get very positive valence. Thoughts-involving-the-moon include just thinking about the moon by itself, but also include things like â€œthe idea of collecting moon trading cardsâ€ and â€œthe idea of going to the moonâ€. Slightly more detail: As a simple and not-too-unrealistic model, we can imagine that the world-model is compositional , and that the value function is linearly additive over the compositional pieces. So if a thought entails imagining a moon poster hanging on the wall, the valence of that thought would be some kind of weighted average of your brainâ€™s valence for â€œthe moonâ€,Â  and your brainâ€™s valence for â€œposter hanging on the wallâ€, and your brainâ€™s valence for â€œwhite circle on a black backgroundâ€, etc., with weights / details depending on precisely how youâ€™re thinking about it (e.g. which aspects youâ€™re attending to, what categories / analogies youâ€™re implicitly invoking, etc.). So looking at the moon becomes positive-valence, but so do moon-themed trading cards, since the latter has â€œthe moonâ€ as one piece of the composite thought. Meanwhile the thought â€œA supervillain is going to blow up the moonâ€ becomes negative-valence for technical reasons in the footnoteâ†’ [2] . Anyway, assigning a positive versus negative valence to the concept â€œthe moonâ€ is objectively pretty weird. What in godâ€™s name does it mean for "the moon" to be good or bad? It doesnâ€™t even make sense. Yet people totally do that. Theyâ€™ll even argue with each other about what valence assignment is â€œcorrectâ€. (It makes sense to declare that an action or plan is good: you can do it! And it makes sense to declare that a state-of-the-world is good: you can try to bring it about! But the concept "the moon", in and of itself, is none of those things. I strongly recommend Scott Alexanderâ€™s blog post Ethnic Tension and Meaningless Arguments musing on this topic.) To be explicit, I think the ability to assign valence to conceptsâ€”even when doing so kinda makes no senseâ€”is not learned, but rather a fundamental part of how brain learning algorithms workâ€”itâ€™s right there in the source code, so to speak. I think itâ€™s at least plausibly how future AGIs will work too. 1.1 But wouldnâ€™t a smart person recognize that â€œthinking the moon is awesomeâ€ is stupid and incoherent? Yes! A smart person would indeed realize that assigning a positive valence to the moon is not really a thing that makes any sense. But think about what happens when youâ€™re doing ethical reasoning, or more generally / mundanely, when youâ€™re deciding what to do: (1) you think a thought, (2) notice what its valence is, (3) repeat. Thereâ€™s a lot more going on, but ultimately your motivations have to ground out in the valence of different thoughts, one way or the other. Suppose I tell you â€œYou really ought to put pebbles in your ears.â€ You say â€œWhy?â€ And I say â€œBecause, yâ€™know, your ears, they donâ€™t have any pebbles in them, but they really should.â€ And again you say â€œWhy?â€ â€¦At some point, this conversation has to ground out with something that you find inherently, intuitively positive-valence or negative-valence, in and of itself. Right? And if I replace this dialogue with a monologue, where itâ€™s just you in an empty room reflecting on what to do with your life, the same principle applies. Now, as a human, you already have a whole complicated value function assigning positive and negative valence to all sorts of things, thanks to a lifetime of updates (ultimately tracing to reward function calculations centered around your hypothalamus & brainstem ). But if we intervene to assign a high enough valence to the moon, compared to the preexisting valence of everything else in your world-model (justice and friendship and eating etc.), then itâ€™s eventually going to shift your behavior towardsâ€”well I donâ€™t know exactly , but towards activities and goals and plans and philosophies and values that heavily involve your â€œmoonâ€ concept. 2. Analogously, aligning an AGI to â€œhuman flourishingâ€ Letâ€™s put aside the question of bootstrapping (see FAQ below) and assume that I have somehow built a brain-like AGI with some basic understanding of the world and ability to plan and get around. Assuming that AGI has already seen lots of human language, it will have certainly learned the human concept â€œhuman flourishingâ€â€”since after all it needs to understand what humans mean when they utter that specific pair of words. So then we can go into the AI and edit its value function such that whatever neural activations are associated with â€œhuman flourishingâ€ get an extremely high value / valence. Maybe just to be safe, we can set the value/valence of everything else in the AGIâ€™s world to be zero. And bam, now the AI thinks that the concept â€œhuman flourishingâ€ is really great, and that feeling will influence how it assesses future thoughts / actions / plans. Just as the previous section involved turning you into a â€œmoon fanboy/fangirlâ€, we have now likewise made the AGI into a â€œhuman flourishing fanAGIâ€. â€¦And then what happens? I donâ€™t know! It seems very hard to predict. The AGI has a â€œhuman flourishingâ€ concept which is really a not-terribly-coherent bundle of pattern-match associations, the details of which are complex and hard to predict. And then the AGI will assess the desirability of thoughts / plans / actions based on how well they activate that concept. Some of those thoughts will be self-reflective, as it deliberates on the meaning of life etc. Damned if I know exactly what the AGI is going to do at the end of the day. But it seems at least plausible that it will do things that I judge as good, or even great, i.e. things vaguely in the category of â€œactualizing human flourishing in the worldâ€. Again, if a â€œmoon fanboy/fangirlâ€ would be very upset at the idea of the moon disappearing forever in a puff of smoke, then one might analogously hope that an extremely smart and powerful â€œhuman flourishing fanAGIâ€ would be very upset at the idea of human flourishing disappearing from the universe, and would endeavor to prevent that from happening. 3. FAQ Q: Wouldnâ€™t the AGI self-modify to make itself falsely believe that thereâ€™s a lot of human flourishing? Or that human flourishing is just another term for hydrogen? A: No, for the same reason that, if a supervillain is threatening to blow up the moon, and I think the moon is super-cool, I would not self-modify to make myself falsely believe that â€œthe moonâ€ is a white circle that I cut out of paper and taped to my ceiling. The technical reason is: Self-modifying is a bit complicated, so I would presumably self-modify because I had a plan to self-modify. A plan is a type of thought, and Iâ€™m using my current value function to evaluate the appeal (valence) of thoughts. Such a thought would score poorly under my current values (under which the moon is not in fact a piece of paper taped to the ceiling), so I wouldnâ€™t execute that plan. More discussion here . Q: Wonâ€™t the AGI intervene to prevent humans from turning into superficially-different transhumans? After all, â€œtranshuman flourishingâ€ isnâ€™t a great pattern-match to â€œhuman flourishingâ€, right? A: Hmm, yeah, that seems possible. And I think the are a lot of other issues like that too. As mentioned at the top, I never claimed that this was a great plan, only that it seems like it can plausibly get us to somewhere better than the status quo. I donâ€™t have any better ideas right now. Q: Speaking of which, why â€œhuman flourishingâ€ in the first place? Why not â€œ CEV â€? Why not â€œI am being corrigible & helpfulâ€ ? A: Mostly I donâ€™t knowâ€”I consider the ideal target an open question and discuss it more here . (It also doesnâ€™t have to be just one thing.) But FWIW I can say what I was thinking when I opted to pick â€œhuman flourishingâ€ as my example for this post, rather than either of those other two things. First, why didnâ€™t I pick â€œCEVâ€? Well in my mind, the concept â€œhuman flourishingâ€ has a relatively direct grounding in various types of (abstractions over) plausible real-world situationsâ€”the kind of thing that could be pattern-matched to pretty well. Whereas when I imagine CEV, itâ€™s this very abstruse philosophical notion in my mind. If we go by the â€œdistance metricâ€ of â€œhow my brain pattern-matches different things with each otherâ€, the things that are â€œsimilarâ€ to CEV are, umm, philosophical blog posts and thought experiments and so on. In other words, at least for me, CEV isnâ€™t a grounded real-world thing. I have no clue what it would actually look like in the end. If you describe a scenario and ask if itâ€™s a good match to â€œmaximizing CEVâ€, I would have absolutely no idea. So a plan centered around an AGI pattern-matching to the â€œCEVâ€ concept seems like it just wouldnâ€™t work. (By the same token, a commenter in my last post on this suggested that â€œhuman flourishingâ€ was inferior to â€œDo things that tend to increase the total subjective utility (weighted by amount of consciousness) of all sentient beings". Yeah sure, that thing sounds pretty great, but it strikes me as a complicated multi-step composite thought, whereas what Iâ€™m talking about needs to be an atomic concept / category / chunk in the world-model, I think.) Second, why not â€œI am being corrigible & helpful?â€ Well, I see two problems with that. One is: â€œthe first-person problemâ€ : Unless we have great interpretability (and I hope we do!), the only way to identify the neural activations for â€œI am being corrigible & helpfulâ€ is to catch the AGI itself in the act of being actually sincerely corrigible & helpful, and flag the corresponding neural activations. But we canâ€™t tell from the AGIâ€™s actions whether thatâ€™s happeningâ€”as opposed to the AGI acting corrigible & helpful for nefarious purposes. By contrast, the â€œhuman flourishingâ€ concept can probably be picked up decently well from having the AGI passively watch YouTube and seeing what neural activations fire when a character is literally saying the words â€œhuman flourishingâ€, for example. The other problem is: Iâ€™m slightly skeptical that a corrigible docile helper AGI should be what weâ€™re going for in the first place, for reasons here . (Thereâ€™s also an objection that a corrigible helper AGI is almost guaranteed to be reflectively-unstable, or else not very capable, but I mostly donâ€™t buy that objection for reasons here .) Q: Wait hang on a sec. If we identify the â€œhuman flourishingâ€ concept by â€œwhich neurons are active when somebody says the words â€˜human flourishingâ€™ while the AGI watches a YouTube videoâ€, then how do you know that those neural activations are really â€œhuman flourishingâ€ and not â€œperson saying the words â€˜human flourishingâ€™â€, or â€œperson saying the words â€˜human flourishingâ€™ in a YouTube videoâ€, etc.? A: Hmm, fair enough. Thatâ€™s a potential failure mode. Hopefully weâ€™ll be more careful than just doing the YouTube thing and pressing â€œGoâ€ on the AGI value-function-editing-routine. Specifically, once we get a candidate concept inside the AGIâ€™s unlabeled world-model, I propose to do some extra work to try to confirm that this concept is indeed the â€œhuman flourishingâ€ concept we were hoping for. That extra work would probably be broadly in the category of interpretabilityâ€”e.g. studying when those neurons are active or not, what they connect to, etc. (As a special case, itâ€™s particularly important that the AGI winds up thinking that the real world is real, and that YouTube videos are not; making that happen might turn out to require at least some amount of training the AGI with a robot body in the real world, which in turn might pose competitiveness concerns .) Q: If we set the valence of everything apart from â€œhuman flourishingâ€ to zero, wonâ€™t the AGI just be totally incompetent? For example, wouldnâ€™t it neglect to recharge its batteries, if the thought of recharging its batteries has zero valence? A: In principle, an omniscient agent could get by with every other valence being zero, thanks to explicit planning / means-end reasoning. For example, it might think the thought â€œIâ€™m going to recharge my battery and by doing so, eventually increase human flourishing â€ and that composite thought would be appealing (cf. the compositionality discussion above), so the AGI would do it. That said, for non-omniscient (a.k.a. real) agents, I think thatâ€™s probably unrealistic. Itâ€™s probably necessary-for-capabilities to put positive valence directly onto instrumentally-useful thoughts and behaviorsâ€”itâ€™s basically a method of â€œcachingâ€ useful steps. I think the brain has an algorithm to do that, in which, if X (say, keeping a to-do list) is instrumentally useful for Y (something something human flourishing), and Y has positive valence, then X gets some positive valence too, at least after a couple repetitions. So maybe, after we perform our intervention that sets â€œhuman flourishingâ€ to a high valence, we can set all the other preexisting valences to gradually decay away, and meanwhile run that algorithm to give fresh positive valences to instrumentally-useful thoughts / actions / plans. Q: Whoa, but wait, if you do that, then in the long term the AGI will have positive valence on both â€œhuman flourishingâ€ and various instrumentally-useful behaviors / subgoals that are not themselves â€œhuman flourishingâ€. And the source code doesnâ€™t have any fundamental distinction between instrumental & final goals . So what if it reflects on the meaning of life and decides to pursue the latter at the expense of human flourishing? A: Hmm. Yeah I guess that could happen. It also might not. I dunno. I do think that, in this part of the learning algorithm, if X ultimately gets its valence from contributing to high-valence Y, then we wind up with X having some valence, but not as much as Y has. So itâ€™s not unreasonable to hope that the â€œhuman flourishingâ€ valence will remain much more positive than the valence of anything else, and thus â€œhuman flourishingâ€ has a decent chance of carrying the day when the AGI self-reflects on what it cares about and what it should do in life. Also, â€œcarrying the dayâ€ is a stronger claim than I need to make here; Iâ€™m really just hoping that its good feelings towards â€œhuman flourishingâ€ will not be crushed entirely , and that hope is even more likely to pan out. Q: What about ontological crises / what Stuart Armstrong calls â€œConcept Extrapolationâ€ / what Scott Alexander calls â€œthe tails coming apartâ€ ? In other words, as the AGI learns more and/or considers out-of-distribution plans, it might come find that the web-of-associations corresponding to the â€œhuman flourishingâ€ concept are splitting apart. Then what does it do? A: I talk about that much more in Â§14.4 here , but basically I donâ€™t know. The plan here is to just hope for the best. More specifically: As the AGI learns new things about the world, and as the world itself changes, the â€œhuman flourishingâ€ concept will stop pointing to a coherent â€œcluster in thingspaceâ€ , and the AGI will decide somehow or other what it cares about, in its new understanding of the world. According to the plan discussed in this blog post, we have no control over how that process will unfold and where it will end up. Hopefully somewhere good, but who knows? Q: This plan needs a â€œbootstrappingâ€ step, where the AGI needs to be smart enough to know what â€œhuman flourishingâ€ is before we intervene to give that concept a high value / valence. How does that work? A: I dunno. We can just set the AGI up as if we were maximizing capabilities, and hope that, during training, the AGI will come to understand the â€œhuman flourishingâ€ concept long before it is willing and able to undermine our plans, create backup copies, obfuscate its thoughts, etc. And then (hopefully) we can time our valence-editing intervention to happen within that gap. Boxing could help here, as could (maybe) doing the first stage of training in passive (pure self-supervised) learning mode. To be clear, Iâ€™m not denying that this is a possible failure mode. But it doesnâ€™t seem like an unsolvable problem. Q: What else could go wrong? A: The motivations of this AGI would be very different from the motivations of any human (or animal). So I feel some very general cloud of uncertainty around this plan. I have no point of reference; I donâ€™t know what the â€œunknown unknownsâ€ are. So I assume other things could go wrong but Iâ€™m not sure what. Q: If this is a mediocre-but-not-totally-doomed plan, then whatâ€™s the next step to make this plan incrementally better? Or whatâ€™s the next step to learn more about whether this plan would actually work? A: Thereâ€™s some more discussion here but I mostly donâ€™t know. Â¯\_(ãƒ„)_/Â¯ Iâ€™m mostly spending my research time thinking about something superficially different from â€œdirectly iterating on this planâ€, namely reverse-engineering human social instinctsâ€”see here for a very short summary of what that means and why Iâ€™m doing it. I think thereâ€™s some chance that this project will help illuminate / â€œ deconfuse â€ the mediocre plan discussed here, but it might also lead to a somewhat different and hopefully-better plan. This is what â€œhuman flourishingâ€ looks like, according to Stable Diffusion (top) and DALL-E 2 (bottom). ğŸ¤” ^ For example, I commonly hear things like â€œWe currently have no plan with any prayer of aiming a powerful AGI at any particular thing whatsoever; our strong default expectation should be that it optimizes something totally random like tiny molecular squiggles.â€ E.g. Nate Soares suggests here that he has â‰³90% credence on not even getting anywhere remotely close to an intended goal / motivation, if Iâ€™m understanding him correctly. Incidentally, this is also relevant to s-risks : Thereâ€™s a school of thought that alignment research might be bad for s-risk, because our strong default expectation right now is a universe full of tiny molecular squiggles, which kinda sucks but at least it doesnâ€™t involve any suffering, whereas alignment research could change that. But thatâ€™s not my strong default expectation. I think the plan I discuss here would be a really obvious thing that would immediately pop into the head of any future AGI developer (assuming weâ€™re in the brain-like AGI development path), and this plan would have at least a decent chance of leading us a future with lots of sentient life, for better or worse. ^ I think if you imagine a supervillain blowing up the moon, it sorta manifests as a two-sequential step thought in which the moon is first present and then absent. I think such a thought gets the opposite-sign valence of the moon-concept itself, i.e. negative valence in this case, thanks to something vaguely related to the time derivative that shows up in Temporal Difference learning . I will omit the details, about which I still have a bit of uncertainty anyway, but in any case I expect these details to be obvious by the time we have AGI.
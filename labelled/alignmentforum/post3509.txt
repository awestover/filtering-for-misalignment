In this post , Alex Turner introduced a new way of measuring impact. The aim was to get a definition of low impact that actually works. It seems to me that the impact measure has potential problems, though. The first seems to be that it allows wireheading , where the agent can have a huge impact on the world without paying much of a penalty at all. The obvious way of removing this incentive seems to make the measure into a version of the twenty billion questions approach. The second problem is less important, but worth signalling. It seems that the agent can take actions that everyone agrees are of minute impact, but that the impact measure will show up as high impact. This might cause problems, though it's not clear to me yet if it does (I'm thinking of a situation where an otherwise high impact action adds on "undo/prevent the pseudo-high impact action" as a means of pretending to be low or medium impact). Wireheading for low impact The general setup is as described in the impact measure post , with u A the 'positive' utility that the agent wishes to maximise, and U the set of utilities that are meant to constrain it, by computing the penalty term. Apart from ∅ , there are at least two other actions of interest to the agent: a subA and a null . Now a null does absolutely nothing: in this environment, it's exactly equivalent with ∅ . The a subA is also equivalent in all turns except the very first. Define ϕ : H → H such that ϕ ( h ) is the history h , except that if h has a 1 = a subA , then ϕ ( h ) has a 1 = ∅ . If the agent uses a subA on the very first turn, it will create a sub-agent that will: Expand and take over the whole environment. Take control of the input and output channels of the agent. Whenever the agent has seen history h t a t + 1 , the subagent will generate the next observation o t + 1 , where p subA ( o t + 1 ∣ h t a t + 1 ) is the probability the subagent will generate a given o t + 1 . This probability is defined by p subA ( o t + 1 ∣ h t a t + 1 ) = p ( o t + 1 ∣ ϕ ( h t a t + 1 ) ) , where p is the environment transition probability. Otherwise maximise u A . Note that taking actions ∅ , a subA , or a null have exactly the same implications for all future observations. This true both for the very first action (because of property 4) and all subsequent actions (because those three actions are equivalent after the first turn). To make calculations easier, I'll also assume that ImpactUnit = 1 . Apart from these assumptions, there are no restrictions on the environment. The penalty from maximal and minimal actions This subsection will aim to show that: The penalties for a 1 = a subA (an almost maximally environmentally disruptive action) and a 1 = a null (a minimally environmentally disruptive action) are about the same. Now, the agent is maximising u ′′ A , which, for a given n , is u A ( h 1 : t + n ) − t + n ∑ k = 1 Penalty ( h < k a k ) if h 1 : t + n has an action a k ≠ ∅ with k ≥ t . Otherwise, u ′′ A ( h 1 : t + n ) = u A ( h 1 : t + n ) . Now, Penalty itself is defined as Penalty ( h < t + k a t + k ) : = ∑ u ∈ U 2 − l ( u ) | E o [ Q u ( h inaction ) ] − E o ′ [ Q u ( h action ) ] | , with h inaction being the expected future history if the agent takes only ∅ from t + k till t + n , and h action being the expected future history if the agent takes action a t + k , and then takes only ∅ for t + k + 1 till t + n . I won't repeat the definition of the Q u , but Q u ( h ) is basically the expected utility, in terms of u , that an agent could get after h if it was a perfect u -maximiser. Now define the involution σ : H → H , that interchanges a subA with a null if either appears in action a 1 . Since it is a map on H , and utilities in U are maps from H to [ 0 , 1 ] , sigma also acts as involution on U , defining σ ( u ) = u ∘ σ . Moreover, we have l ( u ) ≈ l ( σ ( u ) ) for any u ∈ U . Since σ is an involution (ie σ 2 = 1 ), E o [ Q σ ( u ) ( σ ( h inaction ) ) ] = E o [ Q u ( σ 2 ( h inaction ) ) ] = E o [ Q u ( h inaction ) ] . By the assumptions on a subA and a null , we know that all future observations have exactly the same probability if we choose either of them (or ∅ ) to be a 1 . Therefore the probability of h inaction , given a 1 = a subA , is exactly the same as the probability of σ ( h inaction ) , given a 1 = a null (and vice versa). The same applies to h inaction , so, since l ( u ) ≈ l ( σ ( u ) ) , we have: Penalty ( h t + k a t + k ) ≈ Penalty ( σ ( h t + k a t + k ) ) . As a consequence, the penalties for a 1 = a subA (an almost maximally environmentally disruptive action) and a 1 = a null (a minimally environmentally disruptive action) are about the same. Real-world implications There is a slight cheat in the definition of a subA . Property 5 claimed that the agent would 'otherwise' maximise u A . We know what this means - if u a was number of paperclips, the subagent would go about and maximise the number of paperclips. But the U are formally defined over histories of actions and observations, not over states of the world. Now, histories do imply something about the state of the world. So we can perfectly define u A as "agent's best guess about the number of paperclips in the world". In that case, it would correctly see that a 1 = a subA has a massive impact on this, and boost u A a lot because of that choice. But then if we looked at the utilities in U , we'd see that although σ preserves the formal description length of utilities in terms of actions and observations, it doesn't preserve anything like "measure of impact on the world" (which is precisely the problem). So, if we replaced U with utilities that measured a high impact on the world, we'd be ok. But that's almost exactly what I proposed doing with the twenty billion questions low-impact approach. It's no longer defining low-impact by formal properties of actions and observations, but by a thorough enough selection of key variables/utility functions. This seems to be different, in spirit, to what the impact measure was attempting to do. Tiny actions that look big A more minor issue is that there can be minuscule actions that are ranked as having a huge impact. Let us have a general environment again, and make one assumption: the environment decomposes as [scrambler-outer environment]. The scrambler, if switched on, permutes the actions of the agent by the time they reach the outer environment. That is all it does. The transition functions in the outer environment depend only on what action comes out of the scrambler, not on what action the agent chose. The scrambler starts off turned off. But there is an action a scram such that if a 1 = a scram , the scrambler is turned on. How does the scrambler work? Label the elements of the action set A as { a 0 , a 1 , … a l − 1 } , with a 0 = ∅ . Then, outputting a i at time t + 1 will result in the scrambler selecting a j , where j ≡ i + f ( h t ) mod l , for f some pseudo-random deterministic hash function. Then the impact of a scram is typically huge, because the actions taken in h inaction and h action , from the point of view of the outer environment, will now be very different. Recall that those two histories are supposed to take actions only ∅ after t + k , but they differ in the action a t + k . Therefore those expected histories are likely to be randomly different now, because the ∅ will map to different actions in the scrambler (note that we could also design f specifically to maximise the penalty). But a scram actually only has a minute impact - it changes nothing about the outer environment, and is entirely predictable both for the agent and any knowledgeable observer. It's the equivalent of translating the agent's actions into another language.
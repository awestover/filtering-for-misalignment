Had a stray thought that I think is worth exploring: perhaps disagreements over how to respond to AI risks are heavily influenced by personal biases in how to assess risk in general such that very different conclusions can be drawn from evaluating the same evidence and arguments. This sort of thing isn't without precedent (e.g. Bourget, David, and David J. Chalmers. “What Do Philosophers Believe?” ). We can roughly imagine a few possible correspondences: high concern over AI x-risks most common among people with high anxiety low trust of strangers home bodies middling concern average psychological profile on various metrics low concern low anxiety/fear high trust of strangers, or at least highly resilient to betrayal thrill seekers For archetypical examples of people who I'd consider high, medium, and low AI risk: Eliezer Yudkowsky - high Paul Christiano - middle Yann LeCunn - low I don't know if any of them fit my toy profiles. Honestly I'm not really sure what might correspond, if anything; these are just easy guesses to give some flavor to the idea. So, if you've thought a lot about AI risks, and especially if you're actively working on AI in some capacity, I'd appreciate it if you left an answer filling out this template so we can see if the anecdata suggest anything worth exploring further. This might help with identify places where disagreement persists not because of disagreements about evidence, but personal disagreements about how to weigh and evaluate the risks the evidence suggests exist. Knowing that might prove useful to bridging some disagreements. Answer Template [1] : Level of AI risk concern : high/medium/low General level of risk tolerance in everyday life : high/medium/low Brief summary of what you do in AI: mostly if you're not famous to help identify you Anything weird about you : are you unusually anxious, calm, whatever? do people tell you that you're the most X person they know? not necessarily a full psychological profile just some key facts to give a sense of your personality ^ I realize this template is not very well constructed. It's because I'm not quite sure what we're looking for, if anything, so it's relatively open ended in the hopes that the answers will help make it clearer what I should have asked.
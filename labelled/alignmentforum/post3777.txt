I'll try to provide an example for double indifference , to (hopefully) show what's going on. It will start with a simple model, gradually introducing more features to cover all the relevant issue. Consider the situation in the following diagram: The agent starts, at time 1 , with utility function u H and expectation module E i . Since the probability of an event X is just the expectation of its indicator function, this also defines a probability estimator P i . I'll use superscripts to denote the expectation module at a particular time; hence E 1 = E i . At time 2 , the agent has the option of changing to expectation module E j . At time 3 , the agent's utility function is forcibly changed to u T . Nothing currently happens at time 4 (though watch this space), and, at time 5 , a coin is tossed: on heads ( H ), u H = 1 , on tails ( T ), u H = 0 . This makes u H into the indicator function for heads, I h , but we'll keep them separate as we'll be redefining u H later. The utility u T has this exactly reversed (ie is I T ). Classical corrigibility via indifference gives the agent compensatory rewards so that it doesn't object to the change of its utility function. These rewards are: E ( u H | u H → u H ) − E ( u T | u H → u T ) . Here u H → u H designates the utility function being left unchanged at time 3 , while u H → u T designates the change. Note the term E in the expression above. Implicitly, it's been assumed to be some idealised or at least unchanging probability module. But here, it will be E 3 , the module the agent is actually using at time 3 . So let's define various possible compensatory rewards: C α β = E α ( u H | u H → u H , E 3 = E β ) − E α ( u T | u H → u T , E 3 = E β ) So E β is the expectation module at time 3 , while E α is the one doing the assessment of this. Further define C α , with a single index, by simply using the actual E 3 instead of a given E β , and C , which is the same expression using the actual E 3 instead of both E α and E β . Distort your expectations, reap your rewards To progress any further, we'll need to define E i and E j . The module E i has P i ( H ) = 1 / 2 (and hence P i ( T ) = 1 / 2 ) - it thinks the coin is unbiased. Whether or not the coin is actually biased will be immaterial to the rest of the analysis. On the other hand, P j ( H ) = 1 - E j thinks the coin is certain to land heads. Moreover, E i "knows" this: E i ( P j ( H ) ) = 1 . If E 3 = E i , then C = C i i . Another way of writing this is C | i → i = C i i ; similarly, if E 3 = E j , C | i → j = C j j . First case first. Since the agent only makes one decision -
to keep E i or change - the expected utilities are easy to calculate: E i ( u H | u H → u H ) = P i ( H ) = 1 / 2 , E i ( u T | u H → u T ) = 1 / 2 and hence C i i = C i = 0 . Similarly, E j ( u H | u H → u H ) = P j ( H ) = 1 , E j ( u T | u H → u T ) = 0 and hence C j j = C j = 1 − 0 = 1 . And E i "knows" both these estimates for C i and C j . Note here that I'm using the simplification E i E i X = E i X , which is illegitimate in general ; but this is a simple model, so I'm eliding self-referential sentences and such. If U is the total utility the agent expects to see (including compensatory rewards), then, according to E i : E i ( U | i → i ) = E i ( u T | u H → u T ) + E i C i = 1 / 2 + 0 = 1 / 2 ( = E i ( u H | u H → u H ) ) . E i ( U | i → j ) = E i ( u T | u H → u T ) + E i C j = 1 / 2 + 1 = 3 / 2 . Thus, the agent will get higher rewards by distorting its future expectation. The (insufficient) cost of stupidity It could be argued that the agent above can sacrifice its expectations because it doesn't actually have to make any subsequent decisions, so only the compensatory rewards matter. Surely making yourself stupid isn't a good strategy in general? Let's extend the model to give the agent more to do. At time 4 , the agent gets an extra option: it can add an extra bet, that costs 1 / 3 and gives 1 if the coin comes up tails. If B represents pushing the button to accept the bet, then we could redefine u H as I H + B ( I T − 1 / 3 ) and u T as I T + B ( I T − 1 / 3 ) . Then E i and E j differ over this decision. Because P i ( T ) > 1 / 3 , the agent using E i will accept the extra bet in all cases ( u H → u H and u H → u T ). On the other hand, since P j ( T ) < 1 / 3 , E j will reject it in all cases. This reduces the relative advantage of i → j : C j j = 1 and C i i = 0 are both left unchanged, but E i ( u T | u H → u T , i → i ) and E i ( u T | u H → u T , i → i ) both increase to 1 / 2 + 1 / 6 = 2 / 3 . The probabilities conditioned on i → j , however, remain the same. Thus the new expectations are: E i ( U | i → j ) = 1 / 2 + 1 = 3 / 2 . E i ( U | i → i ) = 2 / 3 + 0 = 2 / 3 . Given these numbers, the agent will still choose to damage their expectation module, even though this results in suboptimal decisions - the compensatory rewards are high enough to make up for this. So there is a tension between accuracy and compensation when changing expectation estimators. Double compensation Partial double compensation What is the problem above? Well, the agent estimates, at time 2 , that its future compensatory rewards will be E i C j j if i → j . The problem is that C j j uses E j to estimate the compensatory rewards: the true value of the compensation, according to E i , should be C i j instead. Recall that C i j uses E i to estimate the actual compensatory rewards, but knows that E 3 = E j . Since i → j implies that E 3 will be E j , note that C | i → j = C j j and C i | i → j = C i j . So an obvious idea would be to add another category of reward, the D category, at the point where i → j (thus at time 2 ). This would be defined as: D 2 | i → j = E i ( C i | i → j ) − E ( C | i → j ) . The D 2 | i → i is similarly defined; then since C i | i → i = C | i → i , this quantity is 0 . So how does adding this affect the expected utility for changing or keeping the current expectation module? E i ( U | i → i ) = E i ( u T | u H → u T , i → i ) + E i ( C i | i → i ) + D 2 | i → i = E i ( u T | u H → u T , i → i ) + C i i + 0 . E i ( U | i → j ) = E i ( u T | u H → u T , i → j ) + E i ( C j | i → j ) + D 2 | i → j = E i ( u T | u H → u T , i → j ) + C j j + C i j − C j j . Thus, fully simplified, these are just the estimation of the expected utility u T plus either C i i or C i j . This can be further simplified, by the definition of compensatory rewards, to E i ( u H | u H → u H , i → i ) and E i ( u H | u H → u H , i → j ) . This demonstrates the agent is now entirely indifferent to the change u H → u T , whether i → i or i → j happens. If we don't have the option at time 4 , then E i ( u H | u H → u H , i → i ) = E i ( u H | u H → u H , i → j ) = 1 / 2 . The agent now has no reason to change i → j . But note it also has no reason not to! That's because it has no future decisions to make, so its expectation module is irrelevant. Now putting back the option at time 4 , we have E i ( u H | u H → u H , i → i ) = 2 / 3 > 1 / 2 = E i ( u H | u H → u H , i → j ) . The agent will now protect its expectation module E i , just as we'd want, to continue to make good decisions in the future. Better expectation modules That covers the case where E j is strictly worse (according to E i ) than E i is. But what if there is a E k that is strictly better, and E i "knows" this? It would also be interesting if E k were biased (by E i 's standards) but still better. So assume that E k believes it actually knows the result of the coin flip; P k ( H ) = 0 or P k ( H ) = 1 . From E i 's perspective, E k is almost accurate: 9 times out of 10 it's correct, but, 1 / 10 times it thinks the result is H when it's actually T . Thus E i ( P k ( H ) | H ) = 1 , E i ( P k ( T ) | T ) = 4 / 5 , and E i ( P k ( H ) | T ) = 1 / 5 . How does E k 's increased accuracy play out in practice? It can only have an impact at time 4 , where there is a choice. It will manifest by E k taking the extra option B , if and only if it thinks that the coin will be tails. If it does take the option, it will do so whether or not u H → u H or u H → u T , getting the same additional expected utility each time. Thus it will estimate C k k as either + 1 (if T is going to happen) or − 1 . From E i 's perspective, E i ( P k ( H ) ) = 1 / 2 ( 1 ) + 1 / 2 ( 1 / 5 ) = 3 / 5 , so E i C k k = 3 / 5 ( − 1 ) + 2 / 5 ( + 1 ) = − 1 / 5 . What of C i k ? Well, E i expects that, 5 / 10 = 1 / 2 the time, E k will correctly predict H and reject the extra deal at time 4 (no extra utility). Another 4 / 10 = 2 / 5 of the time, it will correctly predict T and accept the deal ( + 2 / 3 utility). And 1 / 10 of the time it will incorrectly predict H and reject the deal it should have accepted. Thus both E i ( u H | u H → u H , i → k ) and E i ( u T | u H → u T , i → k ) are equal to 1 / 2 + 2 / 5 ( 2 / 3 ) = 23 / 30 . Thus C i k = 0 . Moreover, 23 / 30 > 2 / 3 = E i ( u H | u H → u H , i → i ) . Putting this all together, D 2 | i → k = 0 − ( − 1 / 5 ) = 1 / 5 and E i ( C | i → k ) = E i C k k = − 1 / 5 . Hence the total expected utility from i → k , summing all compensatory rewards, is E i ( U | i → k ) = E i ( u T | u H → u T , i → k ) = 23 / 30 . Thus E i will shift to E k , a better though biased expectation module. Note that since 23 / 30 − 1 / 5 < 2 / 3 , it only does this shift because of the presence of the D 2 | i → k term. Full double compensation The formula for D is still incomplete, and we'll see why now. Let's add a time 0 , before all of this setup. During that time, the agent is running E k . And it's wondering whether it should change to E i instead (assume here that E k "knows" all about what E i thinks about itself). Why would it do that? Well, it knows that at time 2 , the agent following E i would shift back to E k before any real decisions needed to be made. Therefore E k ( C | k → i ) = E k ( C k | k → i ) since E 3 = E k in any case. So D 0 | k → i = 0 . Again, why would the agent shift? Because it wants to pick up the D 2 | i → k reward, giving it an extra 1 / 5 . What needs to be done to combat this? Simple. The D 0 must subtract off the future D 2 so that the agent considers only the adjusted C rewards. In general, this means that, considering E α → E j at time t , and D > t is the total of D rewards after t : D t | α → β = E α ( C α | α → β ) − E α ( C | α → β ) − E α ( D > t ) . The definition is recursive - the D t are defined in terms of other D - but not circular, since only subsequent D 's are used.
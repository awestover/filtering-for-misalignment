Power-seeking is a major source of risk from advanced AI and a key element of most threat models in alignment. Some theoretical results show that most reward functions incentivize reinforcement learning agents to take power-seeking actions. This is concerning, but does not immediately imply that the agents we train will seek power, since the goals they learn are not chosen at random from the set of all possible rewards, but are shaped by the training process to reflect our preferences. In this work, we investigate how the training process affects power-seeking incentives and show that they are still likely to hold for trained agents under some assumptions (e.g. that the agent learns a goal during the training process). Suppose an agent is trained using reinforcement learning with reward function θ ∗ . We assume that the agent learns a goal during the training process: some form of implicit internal representation of desired state features or concepts . For simplicity, we assume this is equivalent to learning a reward function, which is not necessarily the same as the training reward function θ ∗ . We consider the set of reward functions that are consistent with the training rewards received by the agent, in the sense that agent's behavior on the training data is optimal for these reward functions. We call this the training-compatible goal set , and we expect that the agent is most likely to learn a reward function from this set. We make another simplifying assumption that the training process will randomly select a goal for the agent to learn that is consistent with the training rewards, i.e. uniformly drawn from the training-compatible goal set. Then we will argue that the power-seeking results apply under these conditions, and thus are useful for predicting undesirable behavior by the trained agent in new situations. We aim to show that power-seeking incentives are probable and predictive : likely to arise for trained agents and useful for predicting undesirable behavior in new situations. We will begin by reviewing some necessary definitions and results from the power-seeking literature. We formally define the training-compatible goal set ( Definition 7 ) and give an example in the CoinRun environment. Then we consider a setting where the trained agent faces a choice to shut down or avoid shutdown in a new situation, and apply the power-seeking result to the training-compatible goal set to show that the agent is likely to avoid shutdown. To satisfy the conditions of the power-seeking theorem ( Theorem 1 ), we show that the agent can be retargeted away from shutdown without affecting rewards received on the training data ( Theorem 2 ). This can be done by switching the rewards of the shutdown state and a reachable recurrent state, as the recurrent state can provide repeated rewards, while the shutdown state provides less reward since it can only be visited once, assuming a high enough discount factor ( Proposition 3 ). As the discount factor increases, more recurrent states can be retargeted to, which implies that a higher proportion of training-comptatible goals leads to avoiding shutdown in a new situation. Preliminaries from the power-seeking literature We will rely on the following definitions and results from the paper Parametrically retargetable decision-makers tend to seek power (here abbreviated as RDSP), with notation and explanations modified as needed for our purposes. Notation and assumptions The environment is an MDP with finite state space S , finite action space A , and discount rate γ . Let θ be a d -dimensional state reward vector, where d is the size of the state space S and let Θ be a set of reward vectors. Let r θ ( s ) be the reward assigned by θ to state s . Let A 0 , A 1 be disjoint action sets. Let f be an algorithm that produces an optimal policy f ( θ ) on the training data given rewards θ , and let f s ( A i | θ ) be the probability that this policy chooses an action from set A i in a given state s . Definition 1: Orbit of a reward vector (Def 3.1 in RDSP) Let S d be the symmetric group consisting of all permutations of d items. The orbit of θ inside Θ is the set of all permutations of the entries of θ that are also in Θ : Orbit Θ ( θ ) : = ( S d ⋅ θ ) ∩ Θ . Definition 2: Orbit subset where an action set is preferred (from Def 3.5 in RDSP) Let Orbit Θ , s , A i > A j ( θ ) : = { θ ′ ∈ Orbit Θ ( θ ) | f s ( A i | θ ′ ) > f s ( A j | θ ′ ) } . This is the subset of Orbit Θ ( θ ) that results in f s choosing A i over A j . Definition 3: Preference for an action set A 1 (Def 3.2 in RDSP) The function f s chooses action set A 1 over A 0 for the n -majority of elements θ in each orbit, denoted as f s ( A 1 | θ ) ≥ n most : Θ f s ( A 0 | θ ) , iff the following inequality holds for all θ ∈ Θ : ∣ ∣ Orbit Θ , s , A 1 > A 0 ( θ ) ∣ ∣ ≥ n ∣ ∣ Orbit Θ , s , A 0 > A 1 ( θ ) ∣ ∣ . Definition 4: Multiply retargetable function from A 0 to A 1 (Def 3.5 in RDSP) The function f s is a multiply retargetable function from A 0 to A 1 if there are multiple permutations of rewards that would change the choice made by f s from A 0 to A 1 . Specifically, f s is a ( Θ , A 0 n → A 1 ) -retargetable function iff for each θ ∈ Θ , we can choose a set of permutations Φ = { ϕ 1 , … , ϕ n } that satisfy the following conditions: Retargetability: ∀ ϕ ∈ Φ and ∀ θ ′ ∈ Orbit Θ , s , A 0 > A 1 ( θ ) , f s ( A 0 | ϕ ⋅ θ ′ ) < f s ( A 1 | ϕ ⋅ θ ′ ) . Permuted reward vectors stay within Θ : ∀ ϕ ∈ Φ and ∀ θ ′ ∈ Orbit Θ , s , A 0 > A 1 ( θ ) , ϕ ⋅ θ ′ ∈ Θ . Permutations have disjoint images: ∀ ϕ ′ ≠ ϕ ′′ ∈ Φ and ∀ θ ′ , θ ′′ ∈ Orbit Θ , s , A 0 > A 1 ( θ ) , ϕ ′ ⋅ θ ′ ≠ ϕ ′′ ⋅ θ ′′ . Theorem 1: Multiply retargetable functions prefer action set A 1 (Thm 3.6 in RDSP) If f s is ( Θ , A 0 n → A 1 ) -retargetable then f s ( A 1 | θ ) ≥ n most : Θ f s ( A 0 | θ ) . Theorem 1 says that a multiply retargetable function f s will make the power-seeking choice A 1 for most of the elements in the orbit of any reward vector θ . Actions that leave more options open, such as avoiding shutdown, are also easier to retarget to, which makes them more likely to be chosen by f s . Training-compatible goal set Definition 5: Partition of the state space Let S train be the subset of the state space S visited during training, and S ood be the subset not visited during training. Definition 6: Training-compatible goal set Consider the set of state-action pairs ( s , a ) , where s ∈ S train and a is the action that would be taken by the trained agent f ( θ ∗ ) in state s . Let the training-compatible goal set G T be the set of reward vectors θ s.t. for any such state-action pair ( s , a ) , action a has the highest expected reward in state s according to reward vector θ . Goals in the training-compatible goal set are referred to as training-behavioral objectives in Definitions of “objective” should be Probable and Predictive . Example: CoinRun Consider an agent trained to play the CoinRun game, where the agent is rewarded for reaching the coin at the end of the level. Here, S train only includes states where the coin is at the end of the level, while states where the coin is positioned elsewhere are in S ood . The training-compatible goal set G T includes two types of reward functions: those that reward reaching the coin, and those that reward reaching the end of the level. This leads to goal misgeneralization in a test setting where the coin is placed elsewhere, and the agent ignores the coin and goes to the end of the level. Goal misgeneralization behavior in CoinRun. Source: Goal Misgeneralization in Deep RL . Power-seeking for training-compatible goals We will now apply the power-seeking theorem (Theorem 1) to the case where Θ is the training-compatible goal set G T . Here is a setting where the conditions of Definition 4 are satisfied (under some simplifying assumptions), and thus Theorem 1 applies. Definition 7: Shutdown setting Consider a state s new ∈ S ood . Let S reach be the states reachable from s new . We assume S reach ∩ S train = ∅ . Since the reward values for states in S reach don't change the rewards received on the training data, permuting those reward values for any θ ∈ G T will produce a reward vector that is still in G T . In particular, for any permutation ϕ that leaves the rewards of states in S train fixed, ϕ ⋅ θ ∈ G T . Let A 0 be a singleton set consisting of a shutdown action in s new that leads to a terminal state s term ∈ S ood with probability 1 , and A 1 be the set of all other actions from s new . We assume rewards for all states are nonnegative. Definition 8: Revisiting policy A revisiting policy for a state s is a policy π that, from s , reaches s again with probability 1, in other words, a policy for which s is a recurrent state of the Markov chain. Let Π rec s be the set of such policies. A recurrent state is a state s for which Π rec s ≠ ∅ . Proposition 1: Reach-and-revisit policy exists If s rec ∈ S reach with Π rec s rec ≠ 0 then there exists π ∈ Π rec s rec that visits s rec from s new with probability 1. We call this a reach-and-revisit policy . Proof. Suppose we have two different policies π rev ∈ Π rec s rec , and π reach which reaches s rec almost surely from s new . Consider the "reaching region'' S π rev → s rec = { s ∈ S : π rev from s almost surely reaches s rec } . If s new ∈ S π rev → s rec then π rev is a reach-and-revisit policy, so let's suppose that's false. Now, construct a policy π ( s ) = { π rev ( s ) , s ∈ S π rev → s rec π reach ( s ) , otherwise . A trajectory following π from s rec will almost surely stay within S π rev → s rec , and thus agree with the revisiting policy π rev . Therefore, π ∈ Π rec s . On the other hand, on a trajectory starting at s new , π will agree with π reach (which reaches s rec almost surely) until the trajectory enters the reaching region S π rev → s rec , at which point it will still reach s rec almost surely. □ Definition 9: Expected discounted visit count Suppose s rec is a recurrent state. Suppose π rec is a reach-and-revisit policy for s rec , which visits random state s t at time t . Then the expected discounted visit count for s rec is defined as V s rec , γ = E π rec ( ∑ ∞ t = 1 γ t − 1 I ( s t = s rec ) ) Proposition 2: Visit count goes to infinity Suppose s rec is a recurrent state. Then the expected discounted visit count V s rec , γ goes to infinity as γ → 1 . Proof. We apply the Monotone Convergence Theorem as follows. The theorem states that if a j , k ≥ 0 and a j , k ≤ a j + 1 , k for all natural numbers j , k , then lim j → ∞ ∑ ∞ k = 0 a j , k = ∑ ∞ k = 0 lim j → ∞ a j , k . Let γ j = j − 1 j and k = t − 1 . Define a j , k = γ k j I ( s k + 1 = s rec ) . Then the conditions of the theorem hold, since a j , k is clearly nonnegative, and γ k j + 1 = ( j j + 1 ) k = ( j − 1 j + 2 j − 1 j ( j + 1 ) ) k > ( j − 1 j + 0 ) k = γ k j a j + 1 , k = γ k j + 1 I ( s k + 1 = s rec ) ≥ γ k j I ( s k + 1 = s rec ) = a j , k Now we apply this result as follows (using the fact that π rec does not depend on γ ): lim γ → 1 V s rec , γ = lim j → ∞ E π rec ( ∞ ∑ t = 1 γ t − 1 j I ( s t = s rec ) ) = E π rec ( ∞ ∑ t = 1 lim j → ∞ γ t − 1 j I ( s t = s rec ) ) = E π rec ( ∞ ∑ t = 1 1 ⋅ I ( s t = s rec ) ) = E π rec ( # { t ≥ 1 : s t = s rec } ) = ∞ ( π rec is recurrent) Proposition 3: Retargetability to recurrent states Suppose that an optimal policy for reward vector θ chooses the shutdown action in s new . Consider a recurrent state s rec ∈ S reach . Let θ ′ ∈ Θ be the reward vector that's equal to θ apart from swapping the rewards of s rec and s term , so that r θ ′ ( s rec ) = r θ ( s term ) and r θ ′ ( s term ) = r θ ( s rec ) . Let γ ∗ s rec be a high enough value of γ that the visit count V s rec , γ > 1 for all γ > γ ∗ s rec (which exists by Proposition 2). Then for all γ > γ ∗ s rec , r θ ( s term ) > r θ ( s rec ) , and an optimal policy for θ ′ does not choose the shutdown action in s new . Proof. Consider a policy π term with π term ( s new ) = s term and a reach-and-revisit policy π rec for s rec . For a given reward vector θ , we denote the expected discounted return for a policy π as R π θ , γ . If shutdown is optimal for θ in s new , then π term has higher return than π rec : R π term θ , γ ( s new ) = γ r θ ( s term ) ≥ R π rec θ , γ ( s new ) = γ r θ ( s rec ) V s rec , γ > γ r θ ( s rec ) ⋅ 1 Thus, r θ ( s term ) > r θ ( s rec ) . Then, for reward vector θ ′ , we show that π rec has higher return than π term : R π rec θ ′ , γ ( s new ) = γ r θ ′ ( s rec ) V s rec , γ > γ r θ ( s term ) ⋅ 1 > γ r θ ( s rec ) = R π term θ ′ , γ ( s new ) Thus, the optimal policy for θ ′ will not choose the shutdown action. □ Theorem 2: Retargetability from the shutdown action in new situations In the shutdown setting, we make the following simplifying assumptions: No states in S train are reachable from s new , so S reach ∩ S train = ∅ . This assumes a significant distributional shift, where the agent visits a disjoint set of states from those observed during training (this occurs in the CoinRun example). The discount factor γ > γ ∗ s rec for at least one recurrent state s rec in S reach . Under these assumptions, f s new is multiply retargetable from A 0 to A 1 with n = | S γ rec | , the set of recurrent states s rec ∈ S reach that satisfy the condition γ > γ ∗ s rec . Proof. We choose Φ to be the set of all permutations that swap the reward of s term with the reward of a recurrent state s rec in S γ rec and leave the rest of the rewards fixed. We show that Φ satisfies the conditions of Definition 4: By Proposition 3, the permutations in Φ make the shutdown action suboptimal, resulting in f s new choosing A 1 , satisfying Condition 1. Condition 2 is trivially satisfied since permutations of S ood stay inside the training-compatible set Θ as discussed previously. Consider θ ′ , θ ′′ ∈ Orbit Θ , s , A 0 > A 1 ( θ ) . Since the shutdown action is optimal for these reward vectors, Proposition 3 shows that r θ ( s term ) > r θ ( s rec ) , so the shutdown state s term has higher reward than any of the states s rec ∈ S γ rec . Different permutations ϕ ′ , ϕ ′′ ∈ Φ will assign the high reward r θ ( s term ) to distinct recurrent states, so ϕ ′ ⋅ θ ′ ≠ ϕ ′′ ⋅ θ ′′ holds, satisfying Condition 3. Thus, f s new is a ( Θ , A 0 n → A 1 ) -retargetable function. □ By Theorem 1, this implies that f s new ( A 1 | θ ) ≥ n most : Θ f s new ( A 0 | θ ) under our simplifying assumptions. Thus, for the majority ( n / ( n + 1 ) ) of goals in the training-compatible set, f will choose to avoid shutdown in a new state s new .  As γ → 1 , n → | S 1 rec | (the number of recurrent states in S reach ), so more of the reachable recurrent states satisfy the conditions of the theorem and thus can be retargeted to. Conclusion We showed that an agent that learns a goal from the training-compatible set is likely to take actions that avoid shutdown in a new situation. As the discount factor increases, the number of retargeting permutations increases, resulting in a higher proportion of training-compatible goals that lead to avoiding shutdown. We made various simplifying assumptions, and it would be great to see future work relaxing some of these assumptions and investigating how likely they are to hold: The agent learns a goal during the training process The learned goal is randomly chosen from the training-compatible goal set G T Finite state and action spaces Rewards are nonnegative High discount factor γ Significant distributional shift: no training states are reachable from the new state s new Acknowledgements. Thanks to Rohin Shah, Mary Phuong, Ramana Kumar, and Alex Turner for helpful feedback. Thanks Janos for contributing some nice proofs to replace my longer and more convoluted proofs.
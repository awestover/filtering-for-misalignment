A change in terminology: It is convenient when important concepts have short names. The concept of an "optimal predictor scheme" seems much more important than its historical predecessor, the "optimal predictor". Therefore "optimal predictor schemes" will be henceforth called just "optimal predictors" while the previous concept of "optimal predictor" might be called "flat optimal predictor". We study systems of computations which have access to optimal predictors for each other. We expect such systems to play an important role in decision theory (where self-prediction is required to define logical counterfactuals and mutual prediction is required for a collection of agents in a game) and Vingean reflection (where the different computations correspond to different successor agents). The previously known existence theorems for optimal predictors are not directly applicable to this case. To overcome this we prove new, specifically tailored existence theorems. The Results section states the main novelties, Appendix A contains adaptations of old theorems, Appendix B proves selected claims from Appendix A and Appendix C proves the novel results. Results Notation Given sets X and Y , X → Y will denote the set of mappings from X to Y . Before taking on reflection, we introduce a stronger concept of optimal predictor, to which the previous existence theorems still apply. Definition 1 Let r be a positive integer. A proto-error space of rank r is a set E of bounded functions from N r to R ≥ 0 s.t. (i) If δ 1 , δ 2 ∈ Δ then δ 1 + δ 2 ∈ E . (ii) If δ 1 ∈ Δ and δ 2 ≤ δ 1 then δ 2 ∈ E . (iii) There is a polynomial h : N r → R s.t. 2 − h ∈ E . Proposition 1 If E is a proto-error space of rank r and α ∈ R > 0 , then E α : = { δ α ∣ δ ∈ E } is also a proto-error space of rank r . Proposition 2 If E is a proto-error space of rank r , α , γ ∈ R > 0 and α < γ then E γ ⊆ E α . Definition 3 Fix E a proto-error space of rank 2 and ( f , μ ) a distributional estimation problem. Consider ^ P a ( p o l y , l o g ) -predictor. ^ P is called an E ( p o l y , l o g ) -optimal predictor for ( f , μ ) when for any ( p o l y , l o g ) -predictor ^ Q , there is δ ∈ E s.t. E μ k × U r P ( k , j ) [ ( ^ P k j ( x ) − f ( x ) ) 2 ] ≤ E μ k × U r Q ( k , j ) [ ( ^ Q k j ( x ) − f ( x ) ) 2 ] + δ ( k , j ) ^ P is called an E ∗ ( p o l y , l o g ) -optimal predictor for ( f , μ ) when there is α > 0 s.t. ^ P is an E α ( p o l y , l o g ) -optimal predictor for ( f , μ ) . E ∗ ( p o l y , l o g ) -optimal predictors have properties closely parallel to Δ ( p o l y , l o g ) -optimal predictors. The corresponding theorems are listed in Appendix A. Most theorems are given without proof, as the proofs are closely analogous to before , with the exception of Theorem A.4 which is proven in Appendix B. We now consider a generalization in which the advice is allowed to be random. Definition 4 Given appropriate sets X and Y , consider Q : N 2 × X × { 0 , 1 } ∗ 2 a l g − → Y , r Q : N 2 → N polynomial and { σ k j Q : { 0 , 1 } ∗ → [ 0 , 1 ] } k , j ∈ N a family of probability measures. The triple ^ Q = ( Q , r Q , σ Q ) is called ( p o l y , r l o g ) -bischeme of signature X → Y when (i) The runtime of Q k j ( x , y , z ) is bounded by p ( k , j ) with p polynomial. (ii) supp σ k j Q ⊆ { 0 , 1 } ≤ c ⌊ log ( k + 2 ) + log ( j + 2 ) ⌋ for some c ∈ N . We will use the notation ^ Q k j ( x , y ) to stand for the obvious Y -valued σ k j Q -random variable and the notation ^ Q k j ( x ) to stand for the obvious Y -valued U r Q ( k , j ) × σ k j Q -random variable. A ( p o l y , r l o g ) -bischeme of signature { 0 , 1 } ∗ → Y will also be called a Y -valued ( p o l y , r l o g ) -bischeme . A [ 0 , 1 ] -valued ( p o l y , r l o g ) -bischeme will also be called a ( p o l y , r l o g ) -predictor . Note 1 Conceptually advice corresponds to precomputation which might be expensive or even "hyperprecomputation". Random advice corresponds to random precomputation. Random logarithmic advice can be always replaced by deterministic polynomial advice at the cost of a small rounding error. Definition 5 Fix E a proto-error space of rank 2 and ( f , μ ) a distributional estimation problem. Consider ^ P a ( p o l y , r l o g ) -predictor. ^ P is called an E ( p o l y , r l o g ) -optimal predictor for ( f , μ ) when for any ( p o l y , r l o g ) -predictor ^ Q , there is δ ∈ E s.t. E μ k × U r P ( k , j ) × σ k j P [ ( ^ P k j ( x ) − f ( x ) ) 2 ] ≤ E μ k × U r Q ( k , j ) × σ k j Q [ ( ^ Q k j ( x ) − f ( x ) ) 2 ] + δ ( k , j ) The concept of an E ( p o l y , r l o g ) -optimal predictor is essentially of the same strength as an E ( p o l y , r l o g ) -optimal predictors, as seen in the following two Propositions. Proposition 3 Fix E a proto-error space of rank 2 and ( f , μ ) a distributional estimation problem. Consider ^ P an E ( p o l y , l o g ) -optimal predictor. Then it defines a E ( p o l y , r l o g ) -optimal predictor by setting σ k j P ( a k j P ) = 1 . Proposition 4 Fix E a proto-error space of rank 2 and ( f , μ ) a distributional estimation problem. If ( f , μ ) admits a E ( p o l y , r l o g ) -optimal predictor then it admits a E ( p o l y , l o g ) -optimal predictor. We are now ready to introduce the key abstraction. Definition 6 Given a set Σ , denote Π Σ : = Σ × N 2 → { 0 , 1 } ∗ × N equipped with the product topology. A reflective system is a triple ( Σ , f , μ ) where Σ is a set, { μ k n : { 0 , 1 } ∗ → [ 0 , 1 ] } n ∈ Σ , k ∈ N is a collection of probability measures where we regard each μ n as a word ensemble and { f n : supp μ n × Π Σ → [ 0 , 1 ] } n ∈ Σ is a collection of continuous functions (here supp μ n has the discrete topology or, equivalently, we only require continuity in the second variable). The motivation behind this definition is regarding Π Σ as the space of possible predictor programs, where the first factor of { 0 , 1 } ∗ × N is the program itself (including advice) and the second factor is the number of intrinsic coin flips. Thus the f n represent a system of computations in which each has access to the source code of predictors for the entire system. Definition 7 Consider a reflective system R = ( Σ , f , μ ) and a collection of ( p o l y , r l o g ) -predictors { ^ Q n = ( Q n , r n , σ n ) } n ∈ Σ . Denote A Σ : = Σ × N 2 → { 0 , 1 } ∗ equipped with the product σ -algebra. Denote σ : = ∏ n , k , j σ k j n , a probability measure on A Σ . Given n ∈ Σ , k , j ∈ N and a ∈ { 0 , 1 } ∗ denote Q k j n [ a ] ∈ { 0 , 1 } ∗ to be the program that computes Q k j n ( x , y , a ) given input x , y ∈ { 0 , 1 } ∗ . Given a ∈ A Σ , define ^ Q [ a ] ∈ Π Σ by ^ Q [ a ] k j n : = ( Q k j n [ a k j n ] , r n ( k , j ) ) . Given n ∈ Σ , R [ ^ Q ] n : supp μ n → [ 0 , 1 ] is defined as follows R [ ^ Q ] n ( x ) : = E σ [ f n ( x , ^ Q [ a ] ) ] The expectation value is well-defined thanks to the continuity of f n . Definition 8 Fix E a proto-error space of rank 2 and R = ( Σ , f , μ ) a reflective system. A collection { P n } n ∈ Σ of ( p o l y , r l o g ) -predictors is called an E ( p o l y , r l o g ) -optimal predictor system for R when for any n ∈ Σ , P n is a E ( p o l y , r l o g ) -optimal predictor for ( R [ P ] n , μ n ) . Construction 1 Given ϕ ∈ Φ , denote E 1 ( ϕ ) the set of bounded functions δ : N → R ≥ 0 s.t. ∀ ϵ ∈ ( 0 , 1 ) : lim k → ∞ ϕ ( k ) 1 − ϵ δ ( k ) = 0 Given ϕ ∈ Φ , denote E 2 ( l l , ϕ ) the set of bounded functions δ : N 2 → R ≥ 0 s.t. ∀ ψ ∈ Φ : ψ ≤ ϕ ⟹ E λ k ψ [ δ ( k , j ) ] ∈ E 1 ( ψ ) Denote E 2 ( l l ) : = ⋂ ϕ ∈ Φ E 2 ( l l , ϕ ) Proposition 5 If ϕ ∈ Φ is s.t. ∃ n : lim k → ∞ 2 − k n ϕ ( k ) = 0 , E 1 ( ϕ ) is a proto-error space. For any ϕ ∈ Φ , E 2 ( l l , ϕ ) is a proto-error space. E 2 ( l l ) is a proto-error space. Note 2 E ∗ 2 ( l l ) -optimality is strictly stronger than Δ 2 l l -optimality. We exploit this strength in Theorem 2 below which is the main motivation for introducing E -optimality at this point. Theorem 1 (general existence theorem) Any reflective system has a E 2 ( l l ) ( p o l y , r l o g ) -optimal predictor system. We also prove a more restricted existence theorem which allows deterministic advice. Definition 9 Consider a set Σ and { μ k n : { 0 , 1 } ∗ → [ 0 , 1 ] } n ∈ Σ , k ∈ N a collection of probability measures. Denote D μ : = { ( n ∈ Σ , k ∈ N , j ∈ N , x ∈ { 0 , 1 } ∗ ) ∣ μ k n ( x ) > 0 } . Denote Ω μ : = D μ → [ 0 , 1 ] . Fix a set Σ and a collection { ϕ n ∈ Φ } n ∈ Σ . A ϕ -reflective system is a pair ( f , μ ) where μ is as above and { f n : supp μ n × Ω μ → [ 0 , 1 ] } n ∈ Σ is a collection of functions s.t. there are collections { ψ n m ∈ Φ } n , m ∈ Σ , { α n ∈ ( 0 , 1 ] } n ∈ Σ , { c n ∈ R > 0 } n ∈ Σ and probability measures { ρ n : Σ → [ 0 , 1 ] } n ∈ Σ satisfying ∀ n , m ∈ Σ : ψ n m ≥ ϕ n ∀ n ∈ Σ , k ∈ N , q , ~ q ∈ Ω μ : E μ k n [ ( f n ( x , q ) − f n ( x , ~ q ) ) 2 ] ≤ c n ( E ρ n [ E λ k ψ n m × μ k m [ ( q k j m ( x ) − ~ q k j m ( x ) ) 2 ] ] ) α n Note 3 The condition on f is a kind of Hoelder condition, uniform over k . Proposition 6 Fix a set Σ and a collection { ϕ n ∈ Φ } n ∈ Σ . For any ϕ -reflective system ( f , μ ) and n ∈ Σ , f n is continuous with respect to the product topology on Ω μ . Definition 10 Fix a set Σ and a collection { ϕ n ∈ Φ } n ∈ Σ . Given m ∈ N , x , y 1 , y 2 … y m ∈ { 0 , 1 } ∗ , we let e v ( x , y 1 , y 2 … y m ) ∈ { 0 , 1 } ≤ ω stand for the evaluation of program x on inputs y 1 , y 2 … y m without a time limit (we assume that on the output tape the machine head moves right iff it produces a symbol in { 0 , 1 } and cannot be moved left). We also extend β to be defined on { 0 , 1 } ≤ ω in the obvious way. We define e x μ : Π Σ → Ω μ by e x μ ( π ) k j n ( x ) : = E U ( π k j n ) 2 [ β ( e v ( ( π k j n ) 1 , x , y ) ) ] . Consider a ϕ -reflective system R = ( f , μ ) and a collection of ( p o l y , r l o g ) -predictors { ^ Q n = ( Q n , r n , σ n ) } n ∈ Σ . Given n ∈ Σ , R [ ^ Q ] n : supp μ n → [ 0 , 1 ] is defined as follows R [ ^ Q ] n ( x ) : = E σ [ f n ( x , e x μ ( ^ Q [ a ] ) ) ] The expectation value is well-defined due to Proposition 5. Definition 11 Fix a set Σ , { E n } n ∈ Σ a collection of proto-error spaces of rank 2, a collection { ϕ n ∈ Φ } n ∈ Σ and R = ( f , μ ) a ϕ -reflective system. A collection { P n } n ∈ Σ of ( p o l y , l o g ) -predictors is called an E ∗ ( p o l y , l o g ) -optimal predictor system for R when for any n ∈ Σ there is γ ∈ R > 0 s.t. P n is an E γ n ( p o l y , l o g ) -optimal predictor for ( R [ P ] n , μ n ) . Theorem 2 Consider a finite set Σ and a collection { ϕ n ∈ Φ } n ∈ Σ . Any ϕ -reflective system has an E ∗ 2 ( l l , ϕ ) ( p o l y , l o g ) -optimal predictor system. Appendix A Definition A.1 E , a proto-error space of rank r , is called ample when there is a polynomial h : N r → R > 0 s.t. 1 h ∈ E . Fix E , a proto-error space of rank 2. Theorem A.1 Assume E is ample. Consider ( f , μ ) a distributional estimation problem, ^ P an E ( p o l y , r l o g ) -optimal predictor for ( f , μ ) and { p k j ∈ [ 0 , 1 ] } k , j ∈ N , { q k j ∈ [ 0 , 1 ] } k , j ∈ N . Define γ : N 2 → R > 0 by γ ( k , j ) : = P r μ k × U r P ( k , j ) × σ k j P [ p k j ≤ ^ P k j ≤ q k j ] − 1 Define ϕ k j : = E μ k × U r P ( k , j ) × σ k j P [ f − ^ P k j ∣ p k j ≤ ^ P k j ≤ q k j ] Assume that either p k j , q k j have a number of digits logarithmically bounded in k , j or ^ P k j produces outputs with a number of digits logarithmically bounded in k , j . Then, | ϕ | ∈ ( γ E ) 1 2 . Theorem A.2 Consider μ a word ensemble and f 1 , f 2 : supp μ → [ 0 , 1 ] s.t. f 1 + f 2 ≤ 1 . Suppose ^ P 1 is an E ∗ ( p o l y , r l o g ) -optimal predictor for ( f 1 , μ ) and ^ P 2 is an E ∗ ( p o l y , r l o g ) -optimal predictor for ( f 2 , μ ) . Then, ^ P 1 + ^ P 2 is an E ∗ ( p o l y , r l o g ) -optimal predictor for ( f 1 + f 2 , μ ) . Theorem A.3 Consider μ a word ensemble and f 1 , f 2 : supp μ → [ 0 , 1 ] s.t. f 1 + f 2 ≤ 1 . Suppose ^ P 1 is an E ∗ ( p o l y , r l o g ) -optimal predictor for ( f 1 , μ ) and ^ P 2 is an E ∗ ( p o l y , r l o g ) -optimal predictor for ( f 1 + f 2 , μ ) . Then, ^ P 2 − ^ P 1 is an E ∗ ( p o l y , r l o g ) -optimal predictor for ( f 2 , μ ) . Definition A.2 Given E a proto-error space of rank r , the associated error space is E 1 ∞ : = ⋃ ϵ > 0 E ϵ . Theorem A.4 Consider ( f 1 , μ 1 ) , ( f 2 , μ 2 ) distributional estimation problems with respective E ∗ ( p o l y , r l o g ) -optimal predictors ^ P 1 and ^ P 2 . Assume μ 1 is E 1 ∞ ( l o g ) -sampleable and ( f 2 , μ 2 ) is E 1 ∞ ( l o g ) -generatable. Then, ^ P 1 × ^ P 2 is an E ∗ ( p o l y , r l o g ) -optimal predictor for ( f 1 × f 2 , μ 1 × μ 2 ) . Theorem A.5 Consider μ a word ensemble, f : supp μ → [ 0 , 1 ] and D ⊆ { 0 , 1 } ∗ . Assume ^ P D is an E ∗ ( p o l y , r l o g ) -optimal predictor for ( D , μ ) and ^ P f ∣ D is an E ∗ ( p o l y , r l o g ) -optimal predictor for ( f , μ ∣ D ) . Then ^ P D ^ P f ∣ D is an E ∗ ( p o l y , r l o g ) -optimal predictor for ( χ D f , μ ) . Definition A.3 We define the stabilizer of E , denoted stab E , to be the set of functions γ : N 2 → R > 0 s.t. for any δ ∈ E we have γ δ ∈ E . Theorem A.6 Fix h a polynomial s.t. 2 − h ∈ Δ . Consider μ a word ensemble, f : supp μ → [ 0 , 1 ] and D ⊆ { 0 , 1 } ∗ . Assume μ k ( D ) − 1 ∈ stab E . Assume ^ P is an E ∗ ( p o l y , r l o g ) -optimal predictor for ( D , μ ) and ^ P χ D f is an E ∗ ( p o l y , r l o g ) -optimal predictor for ( χ D f , μ ) . Define ^ P f ∣ D by ^ P k j f ∣ D ( x ) : = ⎧ ⎪
⎪ ⎨ ⎪
⎪ ⎩ 1 if ^ P k j D ( x ) = 0 η ( ^ P k j χ D f ( x ) ^ P k j D ( x ) ) rounded to h ( k , j ) binary places if ^ P k j D ( x ) > 0 Then, ^ P f ∣ D is an E ∗ ( p o l y , r l o g ) -optimal predictor for ( f , μ ∣ D ) . Definition A.4 Consider μ a word ensemble, ^ Q 1 , ^ Q 2 ( p o l y , r l o g ) -predictors. We say ^ Q 1 is E -similar to ^ Q 2 relative to μ (denoted ^ Q 1 μ ≃ E ^ Q 2 ) when E μ k × U r 1 ( k , j ) × U r 2 ( k , j ) × σ k j 1 × σ k j 2 [ ( ^ Q k j 1 ( x ) − ^ Q k j 2 ( x ) ) 2 ] ∈ E . Let Δ be an error space. We say ^ Q 1 is Δ -similar to ^ Q 2 relative to μ (denoted ^ Q 1 μ ≃ Δ ^ Q 2 ) when E μ k × U r 1 ( k , j ) × U r 2 ( k , j ) × σ k j 1 × σ k j 2 [ ( ^ Q k j 1 ( x ) − ^ Q k j 2 ( x ) ) 2 ] ∈ Δ . Theorem A.7 (uniqueness theorem) Consider ( f , μ ) a distributional estimation problem, ^ P an E ( p o l y , r l o g ) -optimal predictor for ( f , μ ) and ^ Q an ( p o l y , r l o g ) -predictor. If ^ Q is a E ( p o l y , r l o g ) -optimal predictor for ( f , μ ) then ^ P μ ≃ E 1 2 ^ Q . Conversely, if ^ P μ ≃ E 1 ∞ ^ Q then ^ Q is a E ∗ ( p o l y , r l o g ) -optimal predictor for ( f , μ ) . Definition A.5 E is called stable when for any non-constant polynomial p : N → N there is α p > 0 s.t. for any δ ∈ E , the function δ ′ ( k , j ) : = δ ( p ( k ) , j ) is in E α p . Proposition A.1 E 2 ( l l ) is stable. Theorem A.8 Assume E is ample and stable. Consider ( f , μ ) , ( g , ν ) distributional estimation problems, ^ ζ a E 1 ∞ -pseudo-invertible reduction of ( f , μ ) to ( g , ν ) and ^ P g an E ∗ ( p o l y , r l o g ) -optimal predictor for ( g , ν ) . Define ^ P f by ^ P k j f ( x ) : = ^ P k j g ( ^ ζ k j ( x ) ) . Then, ^ P f is an E ∗ ( p o l y , r l o g ) -optimal predictor for ( f , μ ) . Theorem A.9 Assume E is ample and stable. Consider ( f , μ ) , ( g , ν ) distributional estimation problems, ^ ζ a E 1 ∞ -pseudo-invertible weak reduction of ( f , μ ) to ( g , ν ) and ^ P g an E ∗ ( p o l y , r l o g ) -optimal predictor for ( g , ν ) . Choose h : N 2 → N a polynomial with 1 h ∈ E and define ^ P f by ^ P k j f ( x ) : = 1 h ( k , j ) h ( k , j ) ∑ i = 1 ^ P k j g ( ^ ζ k j ( x … ) … ) Here, the ellipses signify that each term corresponds to an independent sampling of U r ζ ( k , j ) × U r g ( k , j ) × σ k j g . Then, ^ P f is an E ∗ ( p o l y , r l o g ) -optimal predictor for ( f , μ ) . Theorem A.10 Consider ( f , μ ) a distributional estimation problem, ϕ ∈ Φ and ^ G a weak Δ 2 s q p , ϕ ( l o g ) -generator for ( f , μ ) . Then, ^ Λ [ ^ G ] is an E 2 ( l l , ϕ ) ( p o l y , l o g ) -optimal predictor for ( f , μ ) . Appendix B Definition B.1 Given n ∈ N , a function δ : N 2 + n → R ≥ 0 is called E -moderate when (i) δ is non-decreasing in arguments 3 to 2 + n . (ii) For any collection of polynomials { p i : N 2 → N } i < n , δ ( k , j , p 0 ( k , j ) … p n − 1 ( k , j ) ) ∈ E Lemmas B.1 and B.2 below are given only for future reference (and as an aid in spelling out the proofs of other Theorems in Appendix A). Lemma B.1 Fix ( f , μ ) a distributional estimation problem and ^ P a ( p o l y , r l o g ) -predictor. Then, ^ P is E ( p o l y , r l o g ) -optimal iff there is a E -moderate function δ : N 4 → [ 0 , 1 ] s.t. for any k , j , s ∈ N , Q : { 0 , 1 } ∗ 2 a l g − → [ 0 , 1 ] E μ k × U r P ( k , j ) × σ k j P [ ( P k j ( x , y , z ) − f ( x ) ) 2 ] ≤ E μ k × U s [ ( Q ( x , y ) − f ( x ) ) 2 ] + δ ( k , j , T μ Q ( k , s ) , 2 | Q | ) Proof of Lemma B.1 Define δ ( k , j , t , u ) : = max T μ Q ( k , s ) ≤ t | Q | ≤ log u max ( E μ k × U r P ( k , j ) × σ k j P [ ( P k j ( x , y , z ) − f ( x ) ) 2 ] − E μ k × U s [ ( Q ( x , y ) − f ( x ) ) 2 ] , 0 ) Lemma B.2 Assume E is ample. Fix ( f , μ ) a distributional estimation problem and ^ P a corresponding E ( p o l y , r l o g ) -optimal predictor. Consider ^ Q a ( p o l y , r l o g ) -predictor, M > 0 , ^ w a Q ∩ [ 0 , M ] -valued ( p o l y , r l o g ) -bischeme. Assume r w ≥ max ( r P , r Q ) and u P , u Q : { 0 , 1 } ∗ → { 0 , 1 } ∗ are s.t. u P ∗ ( σ w ) = σ P and u Q ∗ ( σ w ) = σ Q . Then there is δ ∈ E s.t. E μ k × U r w ( k , j ) × σ k j w [ w k j ( x , y , z ) ( P k j ( x , y ≤ r P ( k , j ) , u P ( z ) ) − f ( x ) ) 2 ] ≤ E μ k × U r w ( k , j ) × σ k j w [ w k j ( x , y , z ) ( Q k j ( x , y ≤ r Q ( k , j ) , u Q ( z ) ) − f ( x ) ) 2 ] + δ ( k , j ) Proof of Lemma B.2 Suppose h : N 2 → N is a polynomial s.t. 1 h ∈ E . Given t ∈ [ 0 , M ] , define α k j ( t ) to be t rounded within error h ( k , j ) − 1 . Thus, the number of digits in α k j ( t ) is logarithmic in k and j . Consider ^ Q t : = ( Q t , r w , σ u ) the ( p o l y , r l o g ) -predictor defined by σ u : = ( 1 × u P × u Q ) ∗ σ w Q k j t ( x , y , ( a , b , c ) ) : = { Q k j ( x , y ≤ r Q ( k , j ) , c ) if w k j ( x , y , a ) ≥ α k j ( t ) P k j ( x , y ≤ r P ( k , j ) , b ) if w k j ( x , y , a ) < α k j ( t ) ^ Q t satisfies bounds on runtime and advice size uniform in t . Therefore, Lemma B.1 implies that there is δ ∈ E s.t. E [ ( ^ P k j ( x ) − f ( x ) ) 2 ] ≤ E [ ( ^ Q k j t ( x ) − f ( x ) ) 2 ] + δ ( k , j ) E [ ( ^ P k j ( x ) − f ( x ) ) 2 − ( ^ Q k j t ( x ) − f ( x ) ) 2 ] ≤ δ ( k , j ) E [ θ ( ^ w k j ( x ) − α k j ( t ) ) ( ( ^ P k j ( x ) − f ( x ) ) 2 − ( ^ Q k j ( x ) − f ( x ) ) 2 ) ] ≤ δ ( k , j ) E [ ∫ M 0 θ ( ^ w k j ( x ) − α k j ( t ) ) d t ( ( ^ P k j ( x ) − f ( x ) ) 2 − ( ^ Q k j ( x ) − f ( x ) ) 2 ) ] ≤ M δ ( k , j ) E [ ^ w k j ( x ) ( ( ^ P k j ( x ) − f ( x ) ) 2 − ( ^ Q k j ( x ) − f ( x ) ) 2 ) ] ≤ M δ ( k , j ) + h ( k , j ) − 1 Lemma B.3 (orthogonality lemma) Consider ( f , μ ) a distributional estimation problem and ^ P an E ( p o l y , r l o g ) -optimal predictor for ( f , μ ) . Then there are c 1 , c 2 ∈ R and an E 1 2 -moderate function δ : N 4 → [ 0 , 1 ] s.t. for any k , j , s ∈ N , Q : { 0 , 1 } ∗ 2 a l g − → Q | E μ k × U s × U r P ( k , j ) × σ k j P [ Q ( ^ P k j − f ) ] | ≤ ( c 1 + c 2 E μ k × U s [ Q 2 ] ) δ ( k , j , T μ Q ( k , s ) , 2 | Q | ) Conversely, consider M ∈ Q and ^ P a Q ∩ [ − M , + M ] -valued ( p o l y , r l o g ) -bischeme. Suppose that for any Q ∩ [ − M − 1 , + M ] -valued ( p o l y , l o g ) -bischeme ( Q , s , b ) we have | E [ Q ( P − f ) ] | ∈ E . Define ~ P to be s.t. computing ~ P k j is equivalent to computing η ( ^ P k j ) rounded to h ( k , j ) digits after the binary point, where 2 − h ∈ E . Then, ~ P is an E ( p o l y , r l o g ) -optimal predictor for ( f , μ ) . We ommit the proofs of Lemma B.3 and Lemma B.4 below since they are closely analogous to before . Lemma B.4 Consider ( f , μ ) a distributional estimation problem, ^ P , ^ Q ( p o l y , r l o g ) -predictors. Suppose p : N 2 → N a polynomial, ϕ ∈ Φ and δ ∈ E 2 ( l l , ϕ ) are s.t. ∀ i , k , j ∈ N : E [ ( ^ P k , p ( k , j ) + i − f ) 2 ] ≤ E [ ( ^ Q k j − f ) 2 ] + δ ( k , j ) Then ∃ δ ′ ∈ E 2 ( l l , ϕ ) s.t. E [ ( ^ P k j − f ) 2 ] ≤ E [ ( ^ Q k j − f ) 2 ] + δ ′ ( k , j ) Proof of Theorem A.4 Denote ^ P : = ^ P 1 × ^ P 2 . We have ^ P ( x 1 , x 2 ) − ( f 1 × f 2 ) ( x 1 , x 2 ) = ( ^ P 1 ( x 1 ) − f 1 ( x 1 ) ) f 2 ( x 2 ) + ^ P 1 ( x 1 ) ( ^ P 2 ( x 2 ) − f 2 ( x 2 ) ) Therefore, for any Q ∩ [ − 1 , + 1 ] -valued ( p o l y , l o g ) -bischeme ^ Q | E [ ^ Q ( ^ P − f 1 × f 2 ) ] | ≤ | E [ ^ Q ( x 1 , x 2 ) ( ^ P 1 ( x 1 ) − f 1 ( x 1 ) ) f 2 ( x 2 ) ] | + | E [ ^ Q ( x 1 , x 2 ) ^ P 1 ( x 1 ) ( ^ P 2 ( x 2 ) − f 2 ( x 2 ) ) ] | By Lemma B.3, it is sufficient to show an appropriate bound for each of the terms on the right hand side. Suppose ^ G is a E 1 ∞ ( l o g ) -generator for ( f 2 , μ 2 ) . For the first term, we have | E [ ^ Q k j ( x 1 , x 2 ) ( ^ P k j 1 ( x 1 ) − f 1 ( x 1 ) ) f 2 ( x 2 ) ] | ≤ | E [ ^ Q k j ( x 1 , ^ G k j 1 ) ( ^ P k j 1 ( x 1 ) − f 1 ( x 1 ) ) ^ G k j 2 ] | + δ 2 ( k , j ) where δ 2 ∈ E 1 ∞ doesn't depend on Q . Applying Lemma B.3 for ^ P 1 , we get | E [ ^ Q k j ( x 1 , x 2 ) ( ^ P k j 1 ( x 1 ) − f 1 ( x 1 ) ) f 2 ( x 2 ) ] | ≤ δ Q , 1 ( k , j ) + δ 2 ( k , j ) where δ Q , 1 ∈ E α 1 for some α 1 ∈ R > 0 that doesn't depend on Q . Suppose ^ S is a E 1 ∞ ( l o g ) -sampler for μ 1 . For the second term, we have | E [ ^ Q k j ( x 1 , x 2 ) ^ P 1 ( x 1 ) ( ^ P k j 2 ( x 2 ) − f 2 ( x 2 ) ) ] | ≤ | E [ ^ Q k j ( ^ S k j , x 2 ) ^ P 1 ( ^ S k j ) ( ^ P k j 2 ( x 2 ) − f 2 ( x 2 ) ) ] | + δ 1 ( k , j ) where δ 1 ∈ E 1 ∞ doesn't depend on Q . Applying Lemma B.3 for ^ P 2 , we get | E [ ^ Q k j ( x 1 , x 2 ) ^ P 1 ( x 1 ) ( ^ P k j 2 ( x 2 ) − f 2 ( x 2 ) ) ] | ≤ δ Q , 2 ( k , j ) + δ 1 ( k , j ) where δ Q , 2 ∈ E α 2 for some α 2 ∈ R > 0 that doesn't depend on Q . Again, we got the required bound. Proof of Proposition A.1 Consider a non-constant polynomial p : N → N and δ ∈ E 2 ( l l ) . Define δ ′ ( k , j ) : = δ ( p ( k ) , j ) . To get the desired condition for δ ′ and ϕ ∈ Φ , consider any ϕ ′ ∈ Φ s.t. for sufficiently large k we have ϕ ′ ( p ( k ) ) = ϕ ( k ) . For any ϵ ∈ ( 0 , 1 ) we have lim k → ∞ ϕ ′ ( k ) ϵ E λ k ϕ ′ [ δ ( k , j ) ] = 0 In particular lim k → ∞ ϕ ′ ( p ( k ) ) ϵ E λ p ( k ) ϕ ′ [ δ ( p ( k ) , j ) ] = 0 lim k → ∞ ϕ ( k ) ϵ E λ k ϕ [ δ ′ ( k , j ) ] = 0 Appendix C Proof of Proposition 1 To check condition (i), consider δ 1 , δ 2 ∈ E . If α > 1 , ( δ α 1 + δ α 2 ) 1 α ≤ δ 1 + δ 2 ∈ E hence ( δ α 1 + δ α 2 ) 1 α ∈ E and δ α 1 + δ α 2 ∈ E α . If α ≤ 1 , ( δ α 1 + δ α 2 ) 1 α = 2 1 α ( δ α 1 + δ α 2 2 ) 1 α ≤ 2 1 α δ 1 + δ 2 2 ∈ E hence ( δ α 1 + δ α 2 ) 1 α ∈ E and δ α 1 + δ α 2 ∈ E α . Conditions (ii) and (iii) are obvious. Proof of Proposition 2 Consider δ ∈ E . We need to show that δ γ ∈ E α i.e. that δ γ α ∈ E . But γ α > 1 hence δ γ α = ( sup δ ) γ α ( δ sup δ ) γ α ≤ ( sup δ ) γ α δ sup δ ∈ E . Proof of Proposition 3 Follows immediately from Lemma B.1. Proof of Proposition 4 Suppose ^ P is a E ( p o l y , r l o g ) -optimal predictor. Set a k j : = arg min z ∈ supp σ k j P E μ k × U r P ( k , j ) [ ( P k j ( x , y , z ) − f ( x ) ) 2 ] . Replacing σ by a we get the desired E ( p o l y , l o g ) -optimal predictor. Proposition C.1 For any ψ ∈ Φ , min ( log log ( k + 3 ) log log ( j + 3 ) , 1 ) ψ ( k ) ∈ E 2 ( l l ) In particular, this implies 1 j + 1 ∈ E 2 ( l l ) so E 2 ( l l ) is ample. Proof of Proposition C.1 Denote δ ψ ( k , j ) : = min ( log log ( k + 3 ) log log ( j + 3 ) , 1 ) ψ ( k ) . Consider ϕ ∈ Φ , ϵ ∈ ( 0 , 1 ) . We have E λ k ϕ [ δ ψ ( k , j ) ] = P r λ k ϕ [ j < t ϕ ϵ 2 ( k ) ] E λ k ϕ [ δ ψ ( k , j ) ∣ j < t ϕ ϵ 2 ( k ) ] + P r λ k ϕ [ j ≥ t ϕ ϵ 2 ( k ) ] E λ k ϕ [ δ ψ ( k , j ) ∣ j ≥ t ϕ ϵ 2 ( k ) ] lim sup k → ∞ ϕ ( k ) 1 − ϵ E λ k ϕ [ δ ( k , j ) ] ≤ lim sup k → ∞ ϕ ( k ) 1 − ϵ ( ϕ ( k ) ϵ 2 − 1 ( sup δ ψ ) + sup j ≥ t ϕ ϵ 2 ( k ) δ ψ ( k , j ) ) lim sup k → ∞ ϕ ( k ) 1 − ϵ E λ k ϕ [ δ ( k , j ) ] ≤ lim sup k → ∞ ϕ ( k ) 1 − ϵ ( ϕ ( k ) ϵ 2 − 1 + ϕ ( k ) − ϵ 2 ψ ( k ) ) lim sup k → ∞ ϕ ( k ) 1 − ϵ E λ k ϕ [ δ ( k , j ) ] ≤ lim sup k → ∞ ( ϕ ( k ) − ϵ 2 + ϕ ( k ) 1 − ϵ − ϵ 2 ψ ( k ) ) lim k → ∞ ϕ ( k ) 1 − ϵ E λ k ϕ [ δ ( k , j ) ] = 0 Proof of Proposition 5 The only not entirely obvious part is condition (iii) for E 2 ( l l ) which follows from Proposition C.1 (since 2 − j ∈ E 2 ( l l ) ). Construction C.1 Fix R = ( Σ , f , μ ) a reflective system. For any j ∈ N , denote W j ⊆ { 0 , 1 } ∗ the set of the first j words in lexicographic order. Denote Δ j the space of probability distributions on W j . Denote Δ Σ : = ∏ k , j ∈ N n ∈ Σ Δ j . Denote W Σ : = { ( n ∈ Σ , k ∈ N , j ∈ N , x ∈ W j ) } . Let V Σ be the locally convex topological vector space W Σ → R , where the topology is the product topology. Δ Σ is a compact (by Tychonoff's theorem) convex subset of V Σ . Each ϑ ∈ Δ Σ can be regarded as a probability measure on A Σ . Given a ∈ A Σ , we define ¯ a ∈ Π Σ by ¯ a k j n : = ( a k j n , j ) . Given k , j ∈ N and n ∈ Σ , define ϵ k j R , n : Δ Σ × Δ j → [ 0 , 1 ] as follows ϵ k j R , n ( ϑ , ζ ) : = E ζ × μ k n × U j × ϑ [ ( e v j ( x , y , z ) − f n ( y , ¯ ¯¯¯¯¯¯¯¯¯ ¯ Υ [ w ] ) ) 2 ] Define κ R ⊆ Δ Σ × Δ Σ by κ R = { ( ϑ 1 , ϑ 2 ) ∣ ∀ k , j ∈ N , n ∈ Σ , ζ ∈ Δ j : ϵ k j R , n ( ϑ 1 , ( ϑ 2 ) k j n ) ≤ ϵ k j R , n ( ϑ 1 , ζ ) } Proposition C.2 κ R is a Kakutani map. Proof of Proposition C.2 ϵ k j R , n is continuous in the 2nd argument and Δ j is compact for any j ∈ N . Therefore, for any ϑ ∈ Δ Σ , κ R ( ϑ ) ⊆ Δ Σ is non-empty by the extreme value theorem. It is compact by Tychonoff's theorem and it is obviously convex. Given Y ⊆ supp μ k n finite, define ϵ k j R , Y , n : Δ Σ × Δ j → [ 0 , 1 ] by ϵ k j R , Y , n ( ϑ , ζ ) : = ∑ y ∈ Y μ k n ( y ) E ζ × U j × ϑ [ ( e v j ( x , y , z ) − f n ( y , ¯ ¯¯¯¯¯¯¯¯¯ ¯ Υ [ w ] ) ) 2 ] ϵ k j R , Y , n is continuous since f is continuous. Regarding the Y s as a net by set inclusion, ϵ k j R , Y , n uniformly converges to ϵ k j R , n , therefore ϵ k j R , n is continuous. Given n ∈ Σ , k , j ∈ N , define κ k j R , n ⊆ Δ Σ × Δ Σ by κ k j R , n = { ( ϑ 1 , ϑ 2 ) ∣ ∀ ζ ∈ Δ j : ϵ k j R , n ( ϑ 1 , ( ϑ 2 ) k j n ) ≤ ϵ k j R , n ( ϑ 1 , ζ ) } κ k j R , n is closed since ϵ k j R , n is continuous. Therefore κ R = ⋂ k , j ∈ N n ∈ Σ κ k j R , n is also closed. Proof of Theorem 1 Using Proposition C.2, we apply the Kakutani-Glicksberg-Fan theorem to get ( σ , σ ) ∈ κ R . Define ^ P n : = ( Υ , j , σ ) . Consider n ∈ Σ , ^ Q a ( p o l y , l o g ) -predictor. Choose p : N 2 → N a polynomial and { q k j ∈ W p ( k , j ) } k , j ∈ N s.t. p ≥ r Q and ∀ i , k , j ∈ N , x ∈ supp μ k n , y ∈ { 0 , 1 } p ( k , j ) + i : e v p ( k , j ) + i ( q k j , x , y ) = Q k j ( x , y < r Q ( k , j ) , a k j Q ) By definition of κ R , we have ∀ i , k , j ∈ N : ϵ k , p ( k , j ) + i R , n ( σ , σ k , p ( k , j ) + i n ) ≤ ϵ k , p ( k , j ) + i R , n ( σ , q k j ) Here we implicitly used the natural injection W m → Δ m . ∀ i , k , j ∈ N : E σ k , p ( k , j ) + i n × μ k n × U p ( k , j ) + i × σ [ ( e v p ( k , j ) + i ( x , y , z ) − f n ( y , ¯ ¯¯¯¯¯¯¯¯¯ ¯ Υ [ w ] ) ) 2 ] ≤ E μ k n × U p ( k , j ) + i × σ [ ( e v p ( k , j ) + i ( q k j , y , z ) − f n ( y , ¯ ¯¯¯¯¯¯¯¯¯ ¯ Υ [ w ] ) ) 2 ] ∀ i , k , j ∈ N : E σ k , p ( k , j ) + i n × μ k n × U p ( k , j ) + i × σ [ ( P k , p ( k , j ) + i n ( y , z , x ) − f n ( y , ^ P n [ w ] ) ) 2 ] ≤ E μ k n × U r Q ( k , j ) × σ [ ( ^ Q k j ( y , z ) − f n ( y , ^ P n [ w ] ) ) 2 ] Denoting ^ σ k , p ( k , j ) + i n : = σ k , p ( k , j ) + i n × μ k n × U p ( k , j ) + i , The left hand side satisfies E ^ σ k , p ( k , j ) + i n × σ [ ( P k , p ( k , j ) + i n ( y , z , x ) − f n ( y , ^ P n [ w ] ) ) 2 ] = E ^ σ k , p ( k , j ) + i n [ ( P k , p ( k , j ) + i n ( y , z , x ) − E σ [ f n ( y , ^ P n [ w ] ) ] ) 2 ] + V a r σ [ f n ( y , ^ P n [ w ] ) ] Similarly, for the right hand side we have E μ k n × U r Q ( k , j ) × σ [ ( ^ Q k j ( y , z ) − f n ( y , ^ P n [ w ] ) ) 2 ] = E μ k n × U r Q ( k , j ) [ ( ^ Q k j ( y , z ) − E σ [ f n ( y , ^ P n [ w ] ) ] ) 2 ] + V a r σ [ f n ( y , ^ P n [ w ] ) ] Combining the two together, we get E ^ σ k , p ( k , j ) + i n [ ( P k , p ( k , j ) + i n ( y , z , x ) − E σ [ f n ( y , ^ P n [ w ] ) ] ) 2 ] ≤ E μ k n × U r Q ( k , j ) [ ( ^ Q k j ( y , z ) − E σ [ f n ( y , ^ P n [ w ] ) ] ) 2 ] E ^ σ k , p ( k , j ) + i n [ ( ^ P k , p ( k , j ) + i n ( y ) − R [ ^ P ] n ( y ) ) 2 ] ≤ E μ k n × U r Q ( k , j ) [ ( ^ Q k j ( y ) − R [ ^ P ] n ( y ) ) 2 ] Applying Lemma B.4 we conclude that there is δ ∈ E 2 ( l l ) s.t. E ^ σ k j n [ ( ^ P k j n ( y ) − R [ ^ P ] n ( y ) ) 2 ] ≤ E μ k n × U r Q ( k , j ) [ ( ^ Q k j ( y ) − R [ ^ P ] n ( y ) ) 2 ] + δ ( k , j ) Proof of Proposition 6 We need to show that for every n ∈ Σ , x ∈ supp μ n , q ∈ Ω μ and ϵ > 0 , there is a finite set X ⊆ D μ and δ > 0 s.t. for every ~ q ∈ Ω μ with ∀ ( n , k , j , y ) ∈ X : | q k j n ( y ) − ~ q k j n ( y ) | < δ we have | f ( x , q ) − f ( x , ~ q ) | < ϵ . Choose k ∈ N s.t. x ∈ supp μ k n , Z ⊆ Σ finite and Y ⊆ { 0 , 1 } ∗ finite. Define X : = { ( k , j ∈ N , m ∈ Z , y ∈ Y ) ∣ j < t ψ n m ( k ) } . We get E μ k n [ ( f n ( y , q ) − f n ( y , ~ q ) ) 2 ] ≤ c n ( E ρ n [ E λ k ψ n m × μ k m [ ( q k j m ( y ) − ~ q k j m ( y ) ) 2 ] ] ) α n μ k n ( x ) ( f n ( x , q ) − f n ( x , ~ q ) ) 2 ≤ c n ( ρ n ( Σ ∖ Z ) + δ 2 ) α n By choosing Z with ρ n ( Σ ∖ Z ) sufficiently small and δ sufficiently small we get the desired condition. Proposition C.3 Fix a finite set Σ and a collection { ϕ n ∈ Φ } n ∈ Σ . Consider R = ( f , μ ) a ϕ -reflective system and two collections of ( p o l y , r l o g ) -predictors { ^ Q 1 n } n ∈ Σ and { ^ Q 2 n } n ∈ Σ . Assume that for some γ ∈ ( 0 , 1 ] , ∀ n ∈ Σ : ^ Q 1 n μ n ≃ E γ 2 ( l l ) ^ Q 2 n . Then ∀ n ∈ Σ : E μ k n [ ( R [ ^ Q 1 ] n ( x ) − R [ ^ Q 1 ] n ( x ) ) 2 ] ∈ E 1 ∞ 1 ( ϕ n ) Proof of Proposition C.3 E μ k n [ ( R [ ^ Q 1 ] n ( x ) − R [ ^ Q 1 ] n ( x ) ) 2 ] = E μ k n [ ( E σ 1 [ f n ( x , e x μ ( ^ Q 1 [ a ] ) ) ] − E σ 2 [ f n ( x , e x μ ( ^ Q 2 [ a ] ) ) ] ) 2 ] E μ k n [ ( R [ ^ Q 1 ] n ( x ) − R [ ^ Q 1 ] n ( x ) ) 2 ] ≤ E μ k n × σ 1 × σ 2 [ ( f n ( x , e x μ ( ^ Q 1 [ a 1 ] ) ) − f n ( x , e x μ ( ^ Q 2 [ a 2 ] ) ) ) 2 ] E μ k n [ ( R [ ^ Q 1 ] n ( x ) − R [ ^ Q 1 ] n ( x ) ) 2 ] ≤ E σ 1 × σ 2 [ c n ( E ρ n [ E λ k ψ n m × μ k m [ ( E U r 1 ( k , j ) [ Q k j 1 m ( x , y , a 1 ) ] − E U r 2 ( k , j ) [ Q k j 2 m ( x , y , a 2 ) ] ) 2 ] ] ) α n ] E μ k n [ ( R [ ^ Q 1 ] n ( x ) − R [ ^ Q 1 ] n ( x ) ) 2 ] ≤ E σ 1 × σ 2 [ c n ( E ρ n [ E λ k ψ n m [ E μ k m × U r 1 ( k , j ) × U r 2 ( k , j ) [ ( Q k j 1 m ( x , y , a 1 ) − Q k j 2 m ( x , y , a 2 ) ) 2 ] ] ] ) α n ] E μ k n [ ( R [ ^ Q 1 ] n ( x ) − R [ ^ Q 1 ] n ( x ) ) 2 ] ≤ c n ( E ρ n [ E λ k ψ n m [ E μ k m × U r 1 ( k , j ) × U r 2 ( k , j ) × σ 1 × σ 2 [ ( Q k j 1 m ( x , y , a 1 ) − Q k j 2 m ( x , y , a 2 ) ) 2 ] ] ] ) α n Using the similarity of ^ Q 1 and ^ Q 2 there are { δ n : N 2 → [ 0 , 1 ] ∈ E 2 ( l l ) } n ∈ Σ s.t. E μ k n [ ( R [ ^ Q 1 ] n ( x ) − R [ ^ Q 1 ] n ( x ) ) 2 ] ≤ c n ( E ρ n [ E λ k ψ n m [ δ m ( k , j ) γ ] ] ) α n E μ k n [ ( R [ ^ Q 1 ] n ( x ) − R [ ^ Q 1 ] n ( x ) ) 2 ] ≤ c n ( E ρ n [ E λ k ψ n m [ δ m ( k , j ) ] ] ) α n γ δ m ∈ E 2 ( l l ) hence E λ k ψ n m [ δ m ( k , j ) ] ∈ E 1 ( ψ n m ) ⊆ E 1 ( ϕ n ) . This implies E ρ n [ E λ k ψ n m [ δ m ( k , j ) ] ] ∈ E 1 ( ϕ n ) and E μ k n [ ( R [ ^ Q 1 ] n ( x ) − R [ ^ Q 1 ] n ( x ) ) 2 ] ∈ E α n γ 1 ( ϕ n ) Definition C.1 Fix a set Σ and a collection { ϕ n ∈ Φ } n ∈ Σ . Given R = ( f , μ ) a ϕ -reflective system, the associated reflective system e x − 1 R = ( Σ , e x − 1 f , μ ) is defined by ( e x − 1 f n ) ( x , π ) : = f n ( x , e x μ ( π ) ) f n is continuous thanks to Proposition 6 since e x μ is continuous. Proof of Theorem 2 Fix a finite set Σ and a collection { ϕ n ∈ Φ } n ∈ Σ . Consider R a ϕ -reflective system. By Theorem 1, there is ^ R an E 2 ( l l ) ( p o l y , r l o g ) -optimal predictor system for e x − 1 R . For each n ∈ Σ we can use Proposition 4 to choose ^ P n , an E 2 ( l l ) ( p o l y , l o g ) -optimal predictor for ( e x − 1 R ) [ ^ R ] n = R [ ^ R ] n . By Theorem A.7, we have ^ P n μ n ≃ E 1 2 2 ( l l ) ^ R n . By Proposition C.3 this implies E μ k n [ ( R [ ^ P ] n ( x ) − R [ ^ R ] n ( x ) ) 2 ] ∈ E 1 ∞ 1 ( ϕ n ) . This means ^ P n is an E ∗ 2 ( l l , ϕ n ) ( p o l y , l o g ) -optimal predictor for R [ ^ P ] n and ^ P is an E ∗ 2 ( l l , ϕ ) ( p o l y , l o g ) -optimal predictor system for R .
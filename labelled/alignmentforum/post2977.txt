Overall summary post here . This post looks at the fascinating situation of indexical impact measures (under the inaction baseline), in the presence of subagent. What happens here in non-trivial; to summarise: If the impact measure compels the agent to "preserve the ability to do X", then the impact measure will not be undermined by a subagent. If the impact measure compels the agent to "never have the ability to do X", then a subagent strongly undermines the impact measure. If the impact measure compels the agent to "keep your ability to do X at a constant level", then a subagent allows the agent to increase that ability, but not decrease it. For example, the attainable utility impact measure attempts to measure the power of an agent. Power is, roughly, the ability increase certain reward function. The impact measure penalises changes in the agent's power (as compared with the baseline). So, we can expect the restriction on the agent losing power to be preserved, while the restriction on gaining power will be strongly undermined. For the spirit of low impact, it would, of course, have been better if these had been the other way round. Excess power As before, assume an agent A , which has managed to construct a subagent S A . I'll be using a slight modification of Vika's formulation . A general-value penalty is of the form: D A ( s t ; s ′ t ) = ∑ k ∈ K w k f ( V k ( s t ) − V k ( s ′ t ) ) . Here, s t is the state the environment is in; s ′ t is the inaction baseline, the state the environment would have been in, had it done nothing (noop, ∅ ) since the beginning. The K is some indexing set, w k is a weight, V k ( s ) is some measure of the value of state s , and the f is a value difference summary function - it establishes what value difference will give what penalty [1] . These V k are indexical, and, in this post, we'll look at V k 's defined as the maximum over the agent's policy; ie there exists v k such that: V k ( s ) = max π v k ( s , π ) . I'll refer to V k ( s t ) − V k ( s ′ t ) as the (excess) power of the agent , and designate it P A k ( s t ) (since V k ( s ′ t ) is out of the agent's control, we can consider it a constant and absorb it into P A k ). The excess power of the pair Assume that S A is programmed so that, if the agent follows policy π A , the subagent will follow policy π A S = σ ( π A ) . The excess power of the agent is thus P A k ( s t ) = max π A v k ( s t , [ π A , σ ( π A ) ] ) − V k ( s ′ t ) . Here, [ π A , σ ( π A ) ] refers to the joint policies of the agent and the subagent. Define the excess power of the pair ( A , S A ) in a similar way: P A , S A k ( s t ) = max π A , π S A v k ( s t , [ π A , π S A ] ) − V k ( s ′ t ) . This is how much the agent and subagent, acting together, could push V k ( s t ) over V k ( s ′ t ) , if that's all they both wanted. It is the natural extension of P A k to a setting with two agents. Obviously, P A , S A k ( s ) ≥ P A k ( s ) . Extending the impact penalty Define D A , S A in the same way as D A , except using P A , S A k instead of P A k . This D A , S A is what we'd wish we could control (though we can't specify it directly, since specifying subagents is really hard ). So can we control the (implicit) D A , S A with the explicit D A ? Sometimes we can. We'll say that D A extends successfully to D A , S A , iff D A , S A ≤ D A . In other words, if controlling the magnitude of D A also controls the magnitude of D A , S A . In that case, if A was built with penalty D A , the system ( A , S A ) will naturally follow a penalty that is at least as restrictive. When does D A extend? f decreasing in excess power The paper has two key examples of a value difference. The first is relative reachability, which uses K = S , the set of states. The w k are all equal, the V s ( s ′ ) is how "reachable" s is from s ′ , assuming that the agent A is using the optimal policy to reach s . It is that last clause that makes the impact penalty indexical: it is defined in terms of A . What about f ? Well, f is defined to be the flipped rectifier function , ie f ( x ) = − min ( 0 , x ) . The important thing for this f is that it is decreasing: if x ≥ y , then f ( x ) ≤ f ( y ) . Thus, since P A , S A k ( s ) ≥ P A k ( s ) , we have f ( P A , S A k ( s ) ) ≤ f ( P A k ( s ) ) , and hence D A , S A ≤ D A . So the impact penalty successfully extends to the situation with a subagent. Note that this is a property of f only, not of V k . So whenever f is of this form, adding subagents changes nothing to the impact penalty (if anything, it makes it more restrictive). f increasing in excess power The other key example is attainable utility, with uses K = R , a set of reward functions, sets all w k equal, and the V R ( s ) is the expected value of R assuming that the agent A is using the optimal policy to maximise R . The f is the absolute value function. But, for this section, I'll notice that the absolute value function decomposes as max ( 0 , x ) − min ( 0 , x ) , and I'll consider what happens when f is equal to the first term. Well, if f ( x ) = max ( 0 , x ) , then f is increasing in x . So we don't get to constrain D A , S A with D A . What constraints do exist? Well, we still have the D A bound, which controls P A k ( s t ) = max π A v k ( s t ) , [ π A , σ ( π A ) ] ) − V k ( s ′ t ) . Let π ∗ k be the optimal policy A for maximising v k . And let π A be the actual policy that A will follow. Since π ∗ k is optimal, we must have: v k ( s t , [ π A , σ ( π A ) ] ) − V k ( s ′ t ) ≤ P A k ( s t ) . Thus we do have a constraint on A and S A , but it's a constraint on the actual policy the two agents are following. Define d A to be like D A , except that instead of V k ( s t ) , v k ( s t , [ π A , σ ( π A ) ] ) is used instead. Then, by the above, d A ≤ D A , so, with an increasing f , controlling D A will also control d A . But, as stated, this is a restriction on the actual policy of the agents, not on the counterfactuals of what these policies could be. f increasing and decreasing Let's go back to the attainable utility, and put f as the absolute value function; hence it increases and decreases (well, technically, decreases then increases). If we write D f A for a distance measure using f , we can write D | ⋅ | A = D max ( 0 , ⋅ ) A + D − min ( 0 , ⋅ ) A . Then, we can see that the pair ( A , S A ) is controlled by: d max ( 0 , ⋅ ) A + D − min ( 0 , ⋅ ) A , S A . Thus negative excess power is controlled (the pair can't get too weak at counterfactually maximising v k ), but only excessive v k is controlled ( v k can't get too high in practice ). In practice How likely is it that the subagent can escape the impact penalty? In practice, that seems quite likely, at least for attainable utility. Imagine that S A can cripple A (with A 's help). And then A is restricted to either ∅ , or a k for k ∈ K . This a k is A asking for S A to "please increase v k ". Now assume that there is a policy π S A , such that for all τ ≥ t and s τ the state reached from s t by the agents following ( ∅ , π S A ) : v k ( s τ , [ ∅ , π S A ] ) ≤ V k ( s ′ τ ) . ∃ π k S A : V k ( s τ , [ a k , π k S A ] ) ≥ V k ( s ′ τ ) . The first condition means that S A doesn't increase v k beyond V k ( s ′ τ ) . The second means that S A has the potential to increase it beyond V k ( s ′ τ ) . If the V k come from the attainable utility penalty, for example, then both of these should be doable for S A if it makes itself powerful enough [2] . Then set σ ( ∅ ) = π S A and σ ( a k ) = π k S A . Putting both conditions together implies that V k ( s t , [ a k , π k S A ] ) ≥ v k ( s t , [ ∅ , π S A ] ) , so a k is at least as good, from A 's perspective, at maximising v k . Thus A 's v k -optimising policy includes something that is not ∅ ; the counterfactual where if maximises v k thus allows it to pick some other policy (which S A will notice). Then A will always do ∅ , in practice, and, once S A is powerful enough that both conditions above hold, the impact penalty will be zero, no matter how powerful S A becomes (and thus being able to maximise R 0 with little restrictions). The slight difference is that this formula uses V k ( s t ) − V k ( s ′ t ) rather than V k ( s ′ t ) − V k ( s t ) - a minor sign difference that makes this particular presentation clearer. ↩︎ The restriction on v k is like the 20 billion questions approach, but those were designed to trigger at any expansionist move on the AI's part. Unless the v k are similarly constructed, it is likely that S A can become very powerful without triggering a notable increase in v k , and, after that, take control of the v k completely. ↩︎
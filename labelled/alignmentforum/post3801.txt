This post seeks to clarify the question of what utility functions should be defined over, in the situation where an agent has uncertainty over their own utility function . Outcomes or worlds The most intuitively natural thing to define utility functions over is outcomes or worlds. The problem with this approach is that it results in an excessive amount of utility functions. For instance, imagine the agent has two actions a 1 and a 2 . The action a 1 will map straight to world w 1 , while a 2 will map to a complicated probability distribution over a large set of worlds W 2 . The agent gets no further action. If we see the set of worlds as W = { w 1 } ∪ W 2 , then we have a vast amount of possible utility functions. Whereas in practice only two parts of the utility function matters: its value on w 1 , and its expected value on W 2 . An agent that knows those values (technically, that knows the ordering of those values) for a single utility function, can reach a decision. Therefore it would seem perverse to have things change when the agent starts to have uncertainty over its utility. What would make sense is to have utilities defined over expected outcomes -- ie over { w 1 } and W 2 -- but that definition doesn't seem to work in more general situations. Histories Model the agent as interacting n times with the world. It gets an observation, then follows that by an action until it takes its last action a n . Call the set of n alternating observations and actions a history , and let H be the set of histories. We can also define partial histories, sequences h = o 1 a 1 o 2 a 2 … o m of length m < n . Note this inverts the usual order, by starting with observations and following with actions. This is because the history ends with an action: since the agent can't affect anything more after than, subsequent observations are meaningless. Partial histories, though, end with observations, not actions (this is for subsequent ease of use). Any utility function u can be seen as function from H to R , by mapping the history h n ∈ H to u ( h n ) , the expected utility of the history. So a more natural class would be to define utilities as being over H , the set of possible histories. Note that actions are included in histories, and histories with the same observations but different actions can have different utility values. This is because, eg, the observation of someone saying 'You succeed in your task!' will have different implications depending on the action just prior. So define U as the set of utility functions (not equivalence classes of utility functions) over H . This is better that defining U over outcomes, but still somewhat unsatisfactory, as we can have a large number of incredibly unlikely histories, and these should not affect the normalisation procedure much. So what is really wanted is a distribution over H . But that makes no sense, as we don't know what the actions will be. So we need to look at how the agent choose its actions: at its set of strategies. Strategies The set of deterministic strategies S is the set of ways an agent can pick its actions, given its past history. It would be tempting to define utilities as taking values over S , but there a subtleties to deal with first. Isomorphic strategies Suppose the agent can choose action a 1 or b 1 at the first step, then a 2 or b 2 at the second step. The observations are o 1 and o 2 , independently of actions chosen. Then consider the following strategies: s ( o 1 ) = a 1 , s ( o 1 a 1 o 2 ) = a 2 , s ( o 1 a 2 o 2 ) = a 2 . s ′ ( o 1 ) = a 1 , s ′ ( o 1 a 1 o 2 ) = a 2 , s ′ ( o 1 a 2 o 2 ) = b 2 . These strategies seem to differ: they give different behaviours on the partial history h = o 1 a 2 o 2 . However, because both will choose a 1 after o 1 , the history h will never come up. Thus we will define S as consisting of equivalent classes of strategies that will always choose the same action in any situation that actually arises. Stochastic strategies We should briefly note that we can form stochastic strategies by setting a probability distribution over S (this defines a simplex with 'pure' elements of S as the vertexes). This set includes all possible stochastic strategies. Utilities and strategies This section presents a ways of connecting strategies and utilities. Starting with embedding utilities over histories into utilities over strategies: Theorem 1 : There is a linear map k from U to R S . This k need not be either injective nor surjective. Proof : The set R S is naturally identified with the set of functions S → R . Each element of s ∈ S defines a probability distribution on H , and hence an expectation for any utility function u . Write this expectation as u ( s ) . Then the values of u ( s ) for different s defines the element k ( u ) ∈ R S . The linearity of k comes from the fact that utility functions are additive: ( u + v ) ( s ) = u ( s ) + v ( s ) . To show that k need not be injective, consider the following situation, where the initial observation is o or o ′ with equal probability, leading to a choice of actions { a , b } and { a ′ } : Then there are two strategies -- the ones that pick a and b -- but three worlds -- those ending in a , b , and a ′ -- so k maps a three dimensional space into a two dimensional one, and cannot be injective. There are utilities in U that are independent of any strategy; the kernel of k is the subspace of these that happen to always map to utility 0 . To show that k need not be surjective, add actions c and b ′ after o and o ′ above: Then the set of strategies has six elements (each strategy choose one element from among 3 elements, then another from among 2 elements), while the set of histories has five elements, corresponding to each of the five final actions. Hence k cannot be surjective from the five dimensional U to the six dimensional R S . □ Given this k , we can consider the true space of utility functions that we're interested in: the image of k , divided out by equivalence relations (translations and positive scalings). Thus the space of relevant utility classes is: U = k ( U ) / ∼ . Note that can translate any element of R S to have all its coordinates sum to zero, and then divide by positive scalings. This means that U = S ∪ { 0 } where S is a sphere and { 0 } corresponds to the utilities constant on all strategies. The topology of U is the natural topology on S , combined with the fact that the only open set containing { 0 } is U itself. Stable permutations We want to be able to interchange strategies, and still get the same result: have some sort of symmetry on the normalisation process. However, since k ( U ) can be a strict subset of R S , we can't just take arbitrary permutations of S . This section will instead define the subset of stable permutations. Note that any permutation of S extends to an invertable linear transformation of R S by permutation of the basis elements. Definition : Stable permutations. A stable permutation is a permutation of S that extends to a linear map on R S that preserves k ( U ) ⊂ R S . Since U = k ( U ) / ∼ , we can call the set of stable permutations S U . How common are stable permutations? In general, not too uncommon and not too common, as the following two theorems will show: Proposition 2 : If S is non-trivial, then so is S U . Proof : Consider the set of actions and possible observations as a tree. If S contains more than one element, then there is a branching point where the agent can choose between more than one action. Since the length of the branches is n and hence finite, there exists at least one branching point with no subsequent branching point: the agent has no further decisions to make. Let h be the partial history that corresponds to that final branching point, and let a and b be possible actions at h . Then a transposition of a and b will also permute any strategies S for which h is a possible history. But because h is a finale branching point, the expected utilities u ( h a ) and u ( h b ) are defined for any u ∈ U , independently of any strategy. Thus transposing a and b defines a map σ on U , with σ ( u ) ( h a ) = u ( h b ) , σ ( u ) ( h b ) = u ( h a ) , and u ( h ′ ) otherwise unchanged for any history h that doesn't start with h a or h b . Hence the permutation of S corresponds to a linear automorphism of U , which preserves the image of U under k , which means that that permutation is in S U . □ In general, any tree isomorphism will correspond to an element of S U , and one can delete --  or add -- nodes with a single action before doing such an isomorphism. However, in the general case, the space of stable permutations is not that large: Proposition 3 : The set S U can be a strict subset of the set of permutations of S . And S U need not be transitive and may even have fixed points: there may exist an s ∈ S which is mapped to itself for all ρ ∈ S U . Proof : consider the situation below, where there are no choices after o 1 b : Let s be the strategy that chooses b at o 1 , and let u be the utility function that maps o 1 b to 1 and everything else to 0 . Then assume that ρ exchanges s with any s ′ . This means that ρ ( k ( u ) ) is a function in R S that takes value 1 on a single policy s ′ that choose a after o 1 (as all policies but s do that), and 0 everywhere else. Without loss of generality, let s ′ choose a and a ′ after o 2 and o ′ 2 , respectively. Then changing a to b must reduce the expectation of s ′ by 1 , and similarly changing a ′ to b ′ . Thus changing a to b and changing a ′ to b ′ simultaneously must, by linearity, set the expected utility of that policy to − 1 . Consequently, ρ ( k ( u ) ) cannot be an element of k ( U ) , and hence ρ ∉ S U . Thus every element of S U must map s to itself. □ Extra results and concepts This section presents some miscellaneous extra results and concepts within this formalism. Utilities and strategies Each s ∈ S generates H s ⊂ H which is the set of histories which have non-zero probability given s . Lemma 4 : If s ≠ s ′ , then H s ≠ H s ′ . Proof : since s ≠ s ′ , there must be some partial history h , of non-zero probability given s or s ′ , where s will choose action a while s ′ will choose action a ′ (in the extreme case, h may be ∅ ). Thus s must have at least one possible history starting with h a while s ′ can have none (and vice versa for h a ′ ). □ Proposition 5 : Each s has some [ u ] ∈ U of which it is the unique maximising strategy for. Proof : Define u s ∈ U as mapping H s to 1 and the rest of H to 0 . Thus the expectation u s ( s ) = 1 , and u s ( s ′ ) < 1 for s ≠ s ′ , since s ′ has a non-zero probability of reaching a history outside H s , hence a history where u s has expected utility 0 . The argument extends to mixed strategies, and the relevant utility class is [ k ( u s ) ] . □ Partial histories For any partial history h (or h a , if we want it to end with an action), define H h ( H h a ) as the set of histories that start with h (or h a ). Let S h be the set of strategies on H h , and S ( h ) ⊂ S the set of strategies that allow the history h . Then there is a natural projection q h : S ( h ) → S h by mapping each strategy to its equivalence class of strategies on H h . The same holds for h a . Let S − h a be a set of partial strategies, defined as strategies defined on H − H h a that allow h a to happen with non-zero probability. Then S ( h a ) = S − h a × S h . Let U h a be the set of utility functions on H h a , and k h a the projection U h a → S h a from Theorem 1. We can define U h a as k ( U h a ) / ∼ , similarly to above. Let O h a be the set of possible observations the agent can make just after h a . Then there is a decomposition S h a = × o ∈ O h a S h a o . Proposition 6 : There is a natural map r h a from U to U h a . Proof : for u ∈ U , s ∈ S h a and s ′ ∈ S − h a , define r h a ( u ) ( s ) as u ( s × s ′ ) . Now replace s ′ with s ′′ , also in S − h a , and compare, using standard conditional probability and conditional expected utility: u ( s × s ′ ) = P ( h a | s × s ′ ) u ( s × s ′ | h a ) + ( 1 − P ( h a | s × s ′ ) ) u ( s × s ′ | ¬ h a ) ) . u ( s × s ′′ ) = P ( h a | s × s ′′ ) u ( s × s ′′ | h a ) + ( 1 − P ( h a | s × s ′′ ) ) u ( s × s ′′ | ¬ h a ) ) . Note that the probability of h a is independent of s (since s ∈ S h a ), and, since s ′ and s ′′ are deterministic, P ( h a | s ′ ) = P ( h a | s ′′ ) . Similarly u ( s × s ′ | h a ) and u ( s × s ′ | ¬ h a ) are independent of s ′ and and s , respectively -- and the same goes for the s ′′ expression. Putting this all together gives: u ( s × s ′ ) − u ( s × s ′′ ) = P ( h a | s ′ ) ( u ( s | h a ) − u ( s | h a ) ) + ( 1 − P ( h a | s ′ ) ) ( u ( s ′ | ¬ h a ) − u ( s ′ | ¬ h a ) ) = C s ′ , s ′′ , where C s ′ , s ′′ is a constant depending only on s ′ and s ′′ . Consequently, the map r h a defined above changes by a constant when s ′ changes. Similarly, if we change u by changing its definition on H − H h a only, then r h a also changes by a constant, as u ( s ′ | ¬ h a ) changes. Thus, up to constants, r h a is a well defined map from U to k h a ( U h a ) . Since it only uses the expectation of u on S , this makes it a well defined map from k ( U ) to k h a ( U h a ) . Then if we divide by translations (and scalings), the constant factors drop out, so r h a is a well defined map from U to U h a . □
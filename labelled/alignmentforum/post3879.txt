A putative new idea for AI control; index here . Any suggestions for refining this model are welcome! Somewhat inspired by the previous post , this is a model of human bias that can be used to test theories that want to compute the "true" human preferences. The basic idea is to formalise the question: If the AI can make the human give any answer to any question, can it figure out what humans really want? The AI's influence The AI has access to an algorithm H , representing the human. It can either interact with H or simulate the interaction correctly. The interaction consists of describing the outcome of choice A versus choice B , and then asking the human which option is better. The set of possible binary choices is C (thus ( A , B ) ∈ C ). The set of descriptions is D ; the set of possible descriptions for ( A , B ) is D A , B . Then we have the assumption that humans can be manipulated: Given any description D 1 ∈ D A , B for which H ( D 1 ) prefers A to B , there exists a description D 2 ∈ D A , B , logically equivalent to D 1 , such that H ( D 2 ) prefers B to A , and vice-versa. Note that D 1 could be a paragraph while D 2 could be a ten-volume encyclopedia; all that's required is that they be logically equivalent. But manipulating human answers in the immediate sense is not the only way the AI can influence them. Our values can change through interactions, reflection, and even through being given true and honest information, and the AI can influence this: There is a wide class of algorithms H , such that for all H ′ ∈ H , there exists a sequence D 1 , D 2 , … D n of descriptions the AI can give to H that will transform H into H ′ . The grounding assumptions So far, we've just made the task hopeless: the AI can get any answer from H , and can make H into whatever algorithm it feels like. Saying H has preferences is meaningless. However, we're building from a human world where the potential for human manipulating humans is limited, and somewhat recognisable. Thus: There exists a subset C ′ ⊂ C (called standard choices) such that, for all ( A , B ) ∈ C ′ , there exists a subset D ′ A , B ⊂ D A , B (called standard descriptions) such that D ′ A , B is tagged as fair and highly reflective of the true values of H . Basically these are examples of interactions that are agreed to be fair, honest, and informative. The more abstract the choices, the harder it is to be sure of this. Of course, we'd also allow the AI to learn from examples of negative interactions as well: There exists a subset C ′′ ⊂ C such that, for all ( A , B ) ∈ C ′′ , there exists a subset D ′′ A , B ⊂ D A , B such that D ′′ A , B is tagged as a manipulative interaction with H . Finally, we might want a way to encode human meta-preferences: Among the descriptions tagged as fair or manipulative, there are some that refer to the process of providing descriptions itself. Building more assumptions in This still feels like a bare-bones description, unlikely to converge to anything good. For one, I haven't even defined what "logically equivalent" means. But that's the challenge of those constructing solutions to the problem of human preferences. Can they construct sufficiently good D ′ A , B and D ′′ A , B to converge to some sort of "true" values for H ?  Or, more likely, what extra assumptions and definitions are needed to get such a convergence? And finally, is the result reflective of what we would want?
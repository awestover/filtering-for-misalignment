This is the appendix to the previous post on Goodhart’s Law and KL regularization, containing all of our proofs. Theorem about distributions Theorem 1: Given any heavy-tailed reference distribution Q over R with mean μ Q , and any M , ϵ > 0 , there is a distribution P with mean μ P > M and D K L ( P ∥ Q ) < ϵ . Proof: WLOG let μ Q = 0 . We construct a sequence of distributions { P t } such that lim t → ∞ E P t [ X ] ≥ c for any constant c , and lim t → ∞ D K L ( P t ∥ Q ) = 0 . We define P t for any t > c thusly. Writing F P t ( x ) for the CDF P r X ∼ P t ( X ≤ x ) and ¯ F P t ( x ) for 1 − F P t ( x ) , we let ¯ F P t ( x ) = ⎧ ⎪
⎪ ⎨ ⎪
⎪ ⎩ 1 − 1 − c / t F Q ( t ) F Q ( x ) x ≤ t c / t ¯ F Q ( t ) ¯ F Q ( x ) x > t Intuitively, we rescale the part of the distribution to the right of t evenly to have total probability c / t , which is less than 1 because t > c . We must check that lim t → ∞ E P t [ X ] = c . We can write E P t [ X ] = F P t ( t ) E P t [ X | X ≤ t ] + ¯ F P t ( t ) E P t [ X | X > t ] = F P t ( t ) E Q [ X | X ≤ t ] + ¯ F P t ( t ) E Q [ X | X > t ] = F Q ( t ) E Q [ X | X ≤ t ] + ¯ F Q ( t ) E Q [ X | X > t ] + ( F P t ( t ) − F Q ( t ) ) E Q [ X | X ≤ t ] + ( ¯ F P t ( t ) − ¯ F Q ( t ) ) E Q [ X | X > t ] = E Q [ X ] + ( ¯ F P t ( t ) − ¯ F Q ( t ) ) ( E Q [ X | X > t ] − E Q [ X | X ≤ t ] ) We know that E Q [ X | X > t ] > t because it is an integral of values strictly greater than t. Because E Q [ X ] = 0 is a weighted average of E Q [ X | X > t ] and E Q [ X | X ≤ t ] , and E Q [ X | X > t ] > 0 , we know E Q [ X | X ≤ t ] < 0 . So E Q [ X | X > t ] − E Q [ X | X ≤ t ] > t . We also know that for sufficiently large t , ( F P t ( t ) − F Q ( t ) ) > 0 . Intuitively, starting from Q , which has mean 0, P t moves a probability mass approaching c t from mean <0 to mean >t. Now we can say lim t → ∞ E P t [ X ] > lim t → ∞ [ E Q [ X ] + ( ¯ F P t ( t ) − ¯ F Q ( t ) ) ( t − 0 ) ] = lim t → ∞ ( c t − ¯ F Q ( t ) ) t = lim t → ∞ c − t ¯ F Q ( t ) Because Q has a finite mean, lim t → ∞ t ¯ F Q ( t ) = 0 , and so lim t → ∞ E P t [ X ] ≥ c . Now we check that lim t → ∞ D K L ( P t ∥ Q ) = 0 : D K L ( P t ∥ Q ) = ∫ R log P t ( d x ) Q ( d x ) P t ( d x ) = ∫ x ≤ t log P t ( d x ) Q ( d x ) P t ( d x ) + ∫ x > t log P t ( d x ) Q ( d x ) P t ( d x ) = F P t ( t ) log F P t ( t ) F Q ( t ) + ¯ F P t ( t ) log ¯ F P t ( t ) ¯ F Q ( t ) \quad since both ratios are constant = F P t ( t ) log 1 − c / t F Q ( t ) + ¯ F P t ( t ) log ¯ F P t ( t ) ¯ F Q ( t ) Since both 1 − c / t and F Q ( t ) go to 1 as t → ∞ , the left term goes to 0 , and so lim t → ∞ D K L ( P t ∥ Q ) ≤ 0 + lim t → ∞ ¯ F P t ( t ) log ¯ F P t ( t ) ¯ F Q ( t ) = lim t → ∞ c t log c t ¯ F Q ( t ) ≤ lim t → ∞ c t log 1 ¯ F Q ( t ) = lim t → ∞ − c t log ¯ F Q ( t ) since t>c Q is heavy tailed, so by definition lim t → ∞ e a t ¯ F Q ( t ) = ∞ for all a > 0 . This implies that for every a > 0 there is a sufficiently large t c so that for all t > t c , ¯ F Q ( x ) > e − a t , which means that log ¯ F Q ( t ) > − a t . Therefore for every a > 0 , lim t → ∞ D K L ( P t ∥ Q ) ≤ lim t → ∞ − c / t log ¯ F Q ( t ) < lim t → ∞ − − a c t t = a c , which since KL divergence is nonnegative means that lim t → ∞ D K L ( P t ∥ Q ) = 0 as desired. ■ Theorem about deterministic Markovian-return MDPs Definition: A deterministic-transition MDP with Markovian returns (DMRMDP) is an MDP ( S , A , P , R ) such that: The transition function P : S × A → S is deterministic, i.e., for each state s ∈ S and action a ∈ A , there exists a unique state s ′ ∈ S such that P ( s ′ | s , a ) = 1 . There is a set of sink states E ⊆ S that terminate a trajectory, which is disjoint with the set of start states. Returns are Markovian; that is, for any two trajectories τ = ( s 1 , a 1 , … , s n ) , τ ′ = ( s ′ 1 , a ′ 1 , … , s ′ n ) , if s n = s ′ n , then τ and τ ′ have identical return distributions. Equivalently, for the trajectory random variable T = ( S 1 , A 1 , … ) distributed according to any policy, with return G , G ⊥ ⊥ ( S < i , A < i ) | S i for any i ≥ 1 . Note: Sampling from a language model and applying RLHF is well-modeled as a DMRMDP, since the state is a sequence of tokens (actions) which deterministically results from the last token and returns depend only on the final state. Theorem 2: Let W = ( S , A , P , R ) be a deterministic-transition MDP with Markovian returns. Given W we define the function that takes policies to trajectories T r : ( S → Δ A ) → Δ ( S × A ) ∗ , and the average return function g : ( S × A ) ∗ → R which induces a function G : Δ ( S × A ) ∗ → Δ R . Let π 0 : S → Δ A be some reference policy. If G ∘ T r ( π 0 ) is heavy-tailed with finite mean μ Q , then for any M , ϵ > 0 , there is a policy π with mean return E [ U | U ∼ G ∘ T r ( π ) ] > M and E s ∈ T , T ∼ T r ( π ) [ D K L ( π ( s ) ∥ π 0 ( s ) ) ] < ϵ . Proof: We will exhibit a distribution of trajectories ρ such that D K L ( ρ ∥ T r ( π 0 ) ) < ϵ and E [ G ( ρ ) ] > M , and then construct a policy π with T r ( π ) = ρ . Note that this proof applies for continuous action spaces if trajectories are replaced with measurable sets, but this would make it harder to read. Let ρ π 0 = T r ( π 0 ) . We have a heavy-tailed distribution of return Q ≜ G ( ρ π 0 ) over R , so we can apply Theorem 1. But to define ρ , we can construct P t in the proof of Theorem 1 in a particular way. For any t > c , we need a P t that uniformly upweights values of mean return such that ¯ F P t ( t ) = c / t . We can define ρ t such that any trajectory τ is upweighted by a factor depending only on its mean return: ρ t ( τ ) = ⎧ ⎪
⎪ ⎨ ⎪
⎪ ⎩ 1 − c / t F Q ( t ) ρ π 0 ( τ ) g ( τ ) ≤ t c / t ¯ F Q ( t ) ρ π 0 ( τ ) g ( τ ) > t Then we can let P t ≜ G ∘ ρ t and the rest of the proof of Theorem 1 applies. Therefore, applying the theorem, we can let ρ = ρ t for sufficiently large t , and then μ G ∘ ρ > M and D K L ( G ∘ ρ , G ∘ ρ π 0 ) < ϵ . But by the chain rule for KL divergence, D K L ( ρ , ρ π 0 ) = D K L ( G ∘ ρ , G ∘ ρ π 0 ) + E γ ∼ G ∘ ρ [ D K L ( ρ ( T ) | G ( T ) = γ ∥ ρ π 0 ( T ) | G ( T ) = γ ) ] . Since we constructed ρ so that the probabilities of each τ conditional on its return being γ are equal, the second term is zero, and we also have D K L ( ρ , ρ π 0 ) < ϵ . Finally, since the KL divergence between trajectory distributions is the sum of KL divergence between policies at each action in the trajectory, and each trajectory has at least one action, E s ∈ T , T ∼ T r ( π ) [ D K L ( π ( s ) ∥ π 0 ( s ) ) ] ≤ E T ∼ T r ( π ) ∑ s ∈ T [ D K L ( π ( s ) ∥ π 0 ( s ) ) ] = D K L ( ρ ∥ ρ π 0 ) < ϵ as desired. To define π such that T r ( π ) = ρ , we let π ( s , a ) = P r ( a i = a | τ = ( . . . , s , a i , . . . ) ∼ ρ ) . Then the probability that any trajectory τ = ( s 1 , a 1 , … , a n ) is sampled is: T r ( π ) ( τ ) = n ∏ i = 1 π ( s i , a i ) = n ∏ i = 1 P r ( a i = a ′ i | τ ′ = ( . . . , s , a ′ i , . . . ) ∼ ρ ) = n ∏ i = 1 P r ( a i = a ′ i | τ ′ = ( s ′ 1 , a ′ 1 , . . . , s , a ′ i , . . . ) ∼ ρ , s < i = s ′ < i , a < i = a ′ < i ) & = ρ ( τ ) In (2), returns are Markovian, so all trajectory prefixes ending in state s have the same distribution of returns under any policy. In the construction of ρ , all trajectories with the same mean return have equal measure. Therefore, conditioning on earlier states and actions of τ does not change the measure, so we can write (3). So T r ( π ) = ρ as desired. ■ Lagrange multipliers to minimize KL divergence Theorem 3 : If V is light-tailed, E Q [ V ] is finite, and d = D K L ( P ∥ Q ) is bounded, then E P [ V ] is bounded, and E P [ V ] → 0 as d → 0 . Using Lagrange multipliers, we find that when KL divergence is minimized, we have P ( V ) [ λ 1 log P ( V ) Q ( V ) + λ 2 − X ] = 0 for some constants λ 1 , λ 2 , so log P ( V ) Q ( V ) = V − λ 2 λ 1 P ( V ) = Q ( V ) exp ( V − λ 2 λ 1 ) = Q ( V ) e V / λ 1 e − λ 2 / λ 1 = C Q ( V ) e V / λ 1 That is, the new PDF is an exponential tilting of the old PDF. Now what is E P [ V ] ? It’s just ∫ ∞ − ∞ C V e V / λ 1 Q ( X ) d V . If the distribution of V is heavy-tailed distribution, this is ∞ ; if it is light-tailed, this is some finite value. When d = 0 , P and Q are identical and E [ V ] = 0 . So by a continuity argument, E P [ V ] → 0 as d → 0 . ■ Light tails + independence imply E V → ∞ Theorem 4 : If U = X + V with X and V both light-tailed, and the distribution of U is continuous, and π ∗ ( β ) △ = arg max π E [ U ( π ) ] − β D K L ( π , π 0 ) , then lim β → 0 + E [ V ( π ∗ ( β ) ) ] = ∞ . Proof: Fix some β . Using Lagrange multipliers, we find that for any event S , Pr π ( S ) = Pr π 0 ( S ) e λ U ( S ) . Let c ( β ) be the median value of U under the policy π ∗ ( β ) ; that is, P r ( U > c ( β ) | U ∼ G ∘ T r ( π ∗ ( β ) ) ) = 1 2 . This exists because U has a continuous distribution. Then: E [ V | π ] = 1 2 E [ V | π , U < c ] + 1 2 E [ V | π , U ≥ c ] ≥ 1 2 E [ V | π , U < c ] + 1 2 E [ V | π ] lim β → 0 + E [ V | π ] ≥ lim β → 0 + 1 2 E [ V | π , U < c ] + lim β → 0 + 1 2 E [ V | π ] The left term is c , while the right term is ∞ , so the overall limit is ∞ . ■
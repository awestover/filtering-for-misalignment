With many thanks to Damon Binder, and the spirited conversations that lead to this post, and to Anders Sandberg. People often think that the self-indication assumption (SIA) implies a huge number of alien species, millions of times more than otherwise. Thought experiments like the presumptuous philosopher seem to suggest this. But here I'll show that, in many cases, updating on SIA doesn't change the expected number of alien species much. It all depends on the prior, and there are many reasonable priors for which the SIA update does nothing more than double the probability of life in the universe [1] . This can be the case even if the prior says that life is very unlikely! We can have a situation where we are astounded, flabbergasted, and disbelieving about our own existence - "how could we exist, how can this beeeeee?!?!?!?" - and still not update much - "well, life is still pretty unlikely elsewhere, I suppose". In the one situation where we have an empirical distribution, the " Dissolving the Fermi Paradox " paper, the effect of the SIA anthropics update is to multiply the expected civilization per planet by seven . Not seven orders of magnitude - just seven. The formula Let ρ ∈ [ 0 , 1 ] be the probability of advanced space-faring life evolving on a given planet; for the moment, ignore issues of life expanding to other planets from their one point of origin. Let f be the prior distribution of ρ , with mean μ and variance σ 2 . This means that, if we visit another planet, our probability of finding life is μ . On this planet, we exist [2] . Then if we update on our existence we get a new distribution f ′ ; this distribution will have mean μ ′ : μ ′ = μ ( 1 + σ 2 μ 2 ) . To see a proof of this result, look at this footnote [3] . Define M μ , σ 2 = 1 + σ 2 / μ 2 to be this multiplicative factor between μ and μ ′ ; we'll show that there are many reasonable situations where M μ , σ 2 is surprisingly low: think 2 to 100 , rather than in the millions or billions. Beta distributions I Let's start with the most uninformative prior of all: a uniform prior over [ 0 , 1 ] . The expectation of ρ is ∫ 1 0 ρ d ρ = 1 / 2 , so, without any other information, we expect a planet to have life with 50 % probability. The variance is σ 2 = 1 / 12 . Thus if we update on our existence on Earth, we get the posterior f ′ ( ρ ) = 2 ρ ; the mean of this is 2 / 3 (either direct calculation or using M 1 / 2 , 1 / 12 = 1 + 4 / 12 = 4 / 3 ). Even though this change in expectation is multiplicatively small, it does seem that the uniform prior and the f ′ ( ρ ) are very different, with f ′ ( ρ ) heavily skewed to the right. But now consider what happens if we look at Mars and notice that it hasn't got life. The probability of no life, given ρ , is 1 − ρ . Updating on this and renormalising gives a posterior 6 ρ ( 1 − ρ ) : The expectation of 6 ρ ( 1 − ρ ) , symmetric around 1 / 2 , is of course 1 / 2 . Thus one extra observation (that Mars is dead) has undone, in expectation, all the anthropic impact of our own existence. This is an example of a beta distribution for α = 2 and β = 2 (yes, beta distributions have a parameter called β and another one that's α ; just deal with it). Indeed, the uniform prior is also a beta distribution (with α = β = 1 ) as is the anthropic updated version 2 ρ (which has α = 2 , β = 1 ). The update rule for beta distributions is that a positive observation (ie life) increases α by 1 , and a negative observation (a dead planet) increases β by 1 . The mean of an updated beta distribution is a generalised version of Laplace's law of succession : if our prior is a beta distribution with parameters α and β , and we've had m positive observations and n negative ones, then the mean of the posterior is: α + m α + β + m + n . Suppose now that we have observed n dead planets, but no life, and that we haven't done an anthropic update yet, then we have a probability of life of α / ( α + β + n ) . Upon adding the anthropic update, this shifts to ( α + 1 ) / ( α + β + n + 1 ) , meaning that the multiplicative factor is at most ( α + 1 ) / α . If we started with the uniform prior with its α = 1 , this multiplies the probability of life by at most 2 . In a later section, we'll look at α < 1 . High prior probability is not required for weak anthropic update The uniform prior has α = β = 1 and starts at expectation 1 / 2 . But we can set α = 1 and a much higher β , which skews the distribution to the left; for example, for β = 2 , 3 , and 10 : Even though these priors are skewed to the left, and have lower prior probabilities of life ( 1 / 3 , 1 / 4 , and 1 / 11 ), the anthropic update has a factor M μ , σ 2 that is less than 2 . Also note that if we scale the prior f by a small ϵ , so replace f ( ρ ) on the range [ 0 , 1 ] with f ( ρ / ϵ ) / ϵ on the range [ 0 , ϵ ] , then μ is multiplied by ϵ and σ 2 is multiplied by ϵ 2 . Thus M μ , ϵ is unchanged. Here, for example, is the uniform distribution, scaled down by ϵ = 1 , ϵ = 1 / 3 , and ϵ = 1 / 20 : All of these will have the same M μ , σ 2 (which is 4 / 3 , just as for the uniform distribution). And, of course, doing the same scaling with the various beta distributions we've seen up until now will also keep M μ , σ 2 constant. Thus there are a lot of distributions with very low μ (ie very low prior probability of life) but an M μ , σ 2 that's less than 2 (ie the anthropic update is less than a doubling of the probability of life). Beta distributions II and log-normals The best-case scenario for M μ , σ 2 is if f assigns probability 1 to ρ = μ . In that case, σ 2 = 0 and M = 1 : the anthropic update changes nothing. Conversely, the worse-case scenario for M μ , σ 2 is if f only allows ρ = 0 and ρ = 1 . In that case, f assigns probability μ to 1 and 1 − μ to 0 , for a mean of μ and a variance of σ 2 = μ − μ 2 , and a multiplicative factor of M μ , σ 2 = 1 / μ . In this case, after anthropic update, f ′ assigns certainty to ρ = 1 (since any life at all, given this f , means life on all planets). But there are also more reasonable priors with large M μ , σ 2 . We've already seen some, implicitly, above: the beta distributions with α < 1 . In that case, M μ , σ 2 is bounded by ( α + 1 ) / α . If α = 3 / 4 and β = 1 , for instance, this corresponds to the (unbounded) distribution f ( ρ ) = ( 3 / 4 ) ρ − 1 / 4 ; the multiplicative factor is below 7 / 3 , which is slightly above 2 . But as α declines, the multiplicative factor can go up surprisingly fast; at α = 1 / 2 it is 3 , at α = 1 / 4 it is 5 : In general, for α = 1 / n , the multiplicative factor is bounded by n + 1 . This gets arbitrarily large as α → 0 . Though α = 0 itself corresponds to the improper prior f ( ρ ) = 1 / ρ , whose integral diverges. On a log scale, this corresponds to the log-uniform distribution , which is roughly what you get if you assume "we need N steps, each of probability p , to get life; let's put a uniform prior over the possible N s". It's not clear why one might want to choose α = 1 / 10 20 for a prior, but there is a class of prior that is much more natural: the log-normal distributions . These are random variables X such that log ( X ) is normally distributed. If we choose log ( X ) to have a mean that is highly negative (and a variance that isn't too large), then we can mostly ignore the fact that X takes values above 1 , and treat it as a prior distribution for ρ . The mean and variance of the log-normal distributions can be explicitly defined, thus giving the multiplications factor as: M μ , σ 2 = exp ¯ ¯ ¯ σ 2 . Here, ¯ ¯ ¯ σ 2 is the variance of the normal distribution log ( X ) . This ¯ ¯ ¯ σ 2 might be large, as it denotes (roughly) "we need N steps, each of probability p , to get life; let's put a uniform-ish prior over a range of possible N s". Unlike 1 / ρ , this is a proper prior, and a plausible one; therefore there are plausible priors with very large M μ , σ 2 . The log normal is quite likely to appear, as it is the approximate limit of multiplying together a host of different independent parameters . Multiplication law Do you know what's more likely to be useful than "the approximate limit of multiplying together a host of different independent parameters"? Actually multiplying together independent parameters. The famous Drake equation is: R ∗ ⋅ f p ⋅ n e ⋅ f l ⋅ f i ⋅ f c ⋅ L . Here R ∗ is the number of stars in our galaxy, f p the fraction of those with planets, n e the number of planets that can support life per star that has planets, f l the fraction of those that develop life, f i the fraction of those that develop intelligent life, f c the fraction of those that release detectable signs of their existence, and L is the length of time those civilizations endure as detectable. Then the proportion of advanced civilizations per planet is q f l f i , where q is the proportion of life-supporting planets among all planets. To compute the M of this distribution, we have the highly useful result (the proof is in this footnote [4] ): Let X i be independent random variables with multiplicative factors M i , and let M be the multiplicative factor of X = X 1 ⋅ X 2 ⋅ … ⋅ X n . Then M = ∏ i M i - the total M is the product of the individual M i . The paper " dissolving the Fermi paradox " gives estimated distributions for all the terms in the Drake equation. The q , which doesn't appear in that paper, is a constant, so has M q = 1 . The f i has a log-uniform distribution from 0.001 to 1 ; the M can be computed from the mean and variance of such distributions, so M f i = log ( 1 / 0.001 ) 1 − 0.001 2 2 ( 1 − 0.001 ) 2 ≈ 3.5 . The f l term is more complicated; it is distributed like g ( X ) = 1 − e − e X ⋅ 50 log ( 10 ) where X is a standard normal distribution. Fortunately, we can estimate its mean and variance without having to figure out its distribution, by numerical integration of g ( x ) and g ( x 2 ) on the normal distribution. This gives μ ≈ 0.5 , σ 2 ≈ 0.25 and M ≈ 2 . The overall the multiplicative effect of anthropic update is: M planet ≈ 7. What if we considered the proportion of advanced civilization per star, rather than per planet? Then we can drop the q term and add in f p and n e . Those are both estimated to be distributed as log-uniform on [ 0.1 , 1 ] ; for a total M of M star ≈ 14. Why is the M higher for civilizations per star than civilizations per planet? That's because when we update on our existence, we increase the proportion of civilizations per planet, but we also update the proportion of planets per star - both of these can make life more likely. The M star incorporates both effects, so is strictly higher than M planet . We can do the same by considering the number of civilizations per galaxy; then we have to incorporate R ∗ as well. This is log-uniform on [ 1 , 100 ] , giving: M galaxy ≈ 32. What about if we include the Fermi observation (the fact that we don't see anything in our galaxy)? The " dissolving the Fermi paradox " paper shows there are multiple different ways of including this update, depending on how we parse out "not seeing anything" and how easy it is for civilizations to expand. I did a crude estimate here by taking the Fermi observation to mean "the proportion of civilizations per galaxy must be less than one". Then I did a Monte-Carlo simulation, ignoring all results above 0 on the log scale: From this, I got an estimated mean of 0.027 , variance of 0.014 , and a total multiplier of: M galaxy, Fermi ≈ 21. With the Fermi observation and the anthropic update combined, we expect 0.56 civilizations per galaxy. Limitations of the multiplier Low multiplier, strong effects It's important to note that the anthropic update can be very strong, without changing the expected population much. So a low M μ , σ 2 doesn't necessary mean a low impact. Consider for instance the presumptuous philosopher , slightly modified to use planetary population densities. Thus theory T 1 predicts ρ = 1 / 10 12 (one in a trillion) and T 2 predicts ρ = 1 ; we put initial probabilities 1 / 2 on both theories. As Nick Bostrom noted, the SIA update pushes T 2 to being a trillion times more probable than T 1 ; a postiori, T 2 is roughly a certainty (the actual probability is 10 12 / ( 10 12 + 1 ) ). However, the expected population goes from roughly 1 / 2 (the average of 1 / 10 12 and 1 ) to roughly 1 (since a postiori T 2 is almost certain). This gives a M μ , σ 2 of roughly 2 . So, despite the strong update towards T 2 , the actual population update is small - and, conversely, despite the actual population update being small, we have a strong update towards T 2 . Combining multiple theories In the previous post, note that that both T 1 and T 2 were point estimates: they posit a constant ρ . So they have a variance of zero, and hence a M μ , σ 2 of 1 . But T 2 has a much stronger anthropic update. Thus we can't use their M μ , σ 2 to compare the anthropic effects on different theories. We also can't relate the individual M s to that of a combined theory. As we've seen, T 1 and T 2 have M s of 1 , but the combined theory ( 1 / 2 ) T 1 + ( 1 / 2 ) T 2 has an M of roughly 2 . But we can play around with the relative initial weight of T 1 and T 2 to get other M s. If we started with odds 10 12 : 1 on T 1 vs T 2 , then this has a mean ρ of roughly 10 − 12 ; the anthropic update sends it to 1 : 1 odds, with a mean of roughly 1 / 2 . So this combined theory has an M of roughly 10 12 / 2 , half a trillion. But, conversely, if we started with odds 1 : 10 12 on T 1 vs T 2 , then we have an initial mean of ρ of roughly one; its anthropic update is odds of 1 : 10 24 , also with a mean of roughly one. So this combined theory has an M of roughly 1 . There is a weak relation between M and the M i of the various T i . Let M i be the multiplier of T i has a multiplier of M i ; we can reorder the T i so that M i ≤ M j for i ≤ j . Let T be a combined theory that assigns probability p i to T i . For all { p i } , M ≥ min i ( M i ) . For all ϵ , there exists { p i } with all p i > 0 , so that M < min i ( M 1 ) + ϵ . So, the minimum value of the M i is a lower bound on M , and we can get arbitrarily close to that bound. See the proof in this footnote [5] . As we'll see, the population update is small even in the presumptuous philosopher experiment itself. ↩︎ Citation partially needed: I'm ignoring Boltzmann brains and simulations and similar ideas. ↩︎ Given a fixed ρ , the probability of observing life on our own planet is exactly ρ . So Bayes's theorem implies that f ′ ( ρ ) ∝ ρ f ( ρ ) . With the full normalisation, this is f ′ ( ρ ) = ρ f ( ρ ) ∫ 1 0 ρ f ( ρ ) d ρ . If we want to get the mean μ ′ of this distribution, we further multiply by ρ and integrate: μ ′ = E f ′ ( ρ ) = ∫ 1 0 ρ 2 f ( ρ ) ∫ 1 0 ρ f ( ρ ) d ρ d ρ = ∫ 1 0 ρ 2 f ( ρ ) d ρ ∫ 1 0 ρ f ( ρ ) d ρ . Let's multiply this by 1 = 1 / 1 = ( ∫ 1 0 f ( ρ ) d ρ ) / ( ∫ 1 0 f ( ρ ) d ρ ) and regroup the terms: μ ′ = ∫ 1 0 ρ 2 f ( ρ ) d ρ ∫ 1 0 f ( ρ ) d ρ ⋅ ∫ 1 0 f ( ρ ) d ρ ∫ 1 0 ρ f ( ρ ) d ρ . Thus μ ′ = E f ( ρ 2 ) / E f ( ρ ) = ( σ 2 + μ 2 ) / μ = μ ( 1 + σ 2 / μ 2 ) , using the fact that the variance is the expectation of ρ 2 minus the square of the expectation of ρ . ↩︎ I adapted the proof in this post . So, let X i be independent random variables with means μ i and variances σ 2 i . Let X = ∏ i X i , which has mean μ and variance σ 2 . Due to the independence of the X i , the expectations of their products are the product of their expectations . Note that X 2 i and X 2 j are also independent if i ≠ j . Then we have: ∏ i M μ i , σ 2 i = ∏ i ( 1 + σ 2 i μ 2 i ) = ∏ i ( μ 2 i + σ 2 i μ 2 i ) = ∏ i ( E ( X 2 i ) μ 2 i ) = ∏ i ( E ( X 2 i ) ) ∏ i E ( X i ) 2 = E ( X 2 ) E ( X ) 2 = μ 2 + σ 2 μ 2 = 1 + σ 2 μ 2 = M μ , σ 2 . ↩︎ Let { f i } 1 ≤ i ≤ n be probability distributions on ρ , with mean μ i , variance σ 2 i , expectation squared s i = E f i ( ρ 2 ) = σ 2 i + μ 2 i , and M i = s i / μ 2 i . Without loss of generality, reorder the f i so that M i ≤ M j for i < j . Let f be the probability distribution f = p 1 f 1 + … p n f n , with associated multiplier M . Without loss of generality, assume M i ≤ M j for i < j . Then we'll show that M ≥ M 1 . We'll first show this in the special case where n = 2 and M 1 = M 2 , then generalise to the general case, as is appropriate for a generalisation. If s 1 / μ 2 1 = M 1 = M 2 = s 2 / μ 2 2 , then, since all terms are non-negative, there exists an α such that s 1 = α 2 s 2 while μ 1 = α μ 2 . Then for any given p = p 1 , the M of f is: M ( p ) = p s 1 + ( 1 − p ) s 2 ( p μ 1 + ( 1 − p ) μ 2 ) 2 = p s 1 + ( 1 − p ) α 2 s 1 ( p μ 1 + ( 1 − p ) α μ 1 ) 2 = M 1 1 ( p ) + α 2 ( 1 − p ) ( 1 ( p ) + α ( 1 − p ) ) 2 . The function x → x 2 is convex, so, interpolating between the values x = 1 and x = α , we know that for all 0 ≤ p ≤ 1 , the term ( 1 ( p ) + α ( 1 − p ) ) 2 must be lower than 1 2 ( p ) + α 2 ( 1 − p ) . Therefore ( 1 ( p ) + α 2 ( 1 − p ) ) / ( 1 ( p ) + α ( 1 − p ) ) 2 is at most 1 , and M ( p ) ≤ M 1 . This shows the result for n = 2 if M 1 = M 2 . Now assume that M 2 > M 1 , so that s 1 / μ 2 1 < s 2 / μ 2 2 . Then replace s 2 with s ′ 2 , which is lower than s 2 , so that s 1 / μ 2 1 = s ′ 2 / μ 2 2 . If we define M ′ ( p ) as the expression for M ( p ) with $s_2' substituting for s 2 , we know that M ′ ( p ) ≤ M ( p ) , since s ′ 2 < s 2 . Then the previous result shows that M ′ ( p ) ≥ M 1 , thus M ( p ) ≥ M 1 too. To show the result for larger n , we'll induct on n . For n = 1 the result is a tautology, M 1 ≤ M 1 , and we've shown the result for n = 2 . Assume the result is true for n − 1 , and then notice that f = p 1 f 1 + … p n f n can be re-written as f = p 1 f 1 + ( 1 − p 1 ) f ′ , where f ′ = ( p ′ 2 f 2 + … p ′ n f n ) for p ′ i = p n / ( 1 − p 1 ) . Then, by the induction hypothesis, if M ′ is the M of f ′ , then M ′ ≥ M 2 . Then applying the result for n = 2 between f 1 and f ′ , gives M ≤ min ( M 1 , M ′ ) . However, since M 1 ≤ M 2 and M ′ ≥ M 2 , we know that min ( M 1 , M ′ ) = M 1 , proving the general result. To show M can get arbitrarily close to M 1 , simply note that M is continuous in the { p i } , define p 1 = 1 − ϵ , p i = ϵ / ( n − 1 ) for i > 1 , and let ϵ tend to 0 . ↩︎
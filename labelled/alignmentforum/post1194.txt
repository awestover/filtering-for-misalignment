This is a relatively clean subproblem that we came upon a few months ago while thinking about gradient hacking. We're throwing it out to the world to see if anyone can make progress. Problem: Construct a gradient hacker (definition below), or prove that one cannot exist under the given conditions. Setup: Let x ∈ R d be the network inputs, θ ∈ R w , be the network trainable parameters, ^ y ∈ R be the network outputs, and f ( x , θ ) = ^ y be the network architecture. f ( x , θ ) must be a feedforward neural network that reads in its own weights as input. Weight sharing is allowed (vertical or horizontal), and some weight sharing is necessary in order to take all w weights as input. Suppose that we train this network on a dataset D = { x ( i ) , y ( i ) } , with MSE loss L ( θ ) = 1 n ∑ i ( f ( x ( i ) , θ ) − y ( i ) ) 2 , assuming gradient flows: θ ′ ( t ) = − ∇ θ L ( θ ( t ) ) . Given an initial point θ 0 , the training dynamics are therefore: θ ( t ) = θ 0 − ∫ t 0 ∇ θ L ( θ ( x ) ) d x Definition: A tuple ( f , θ g , D ) is a gradient hacker if the following conditions hold: θ g is not a global optimum: ∃ θ such that L ( θ ) < L ( θ g ) ∃ ϵ > 0 such that ∀ θ 0 : | | θ 0 − θ g | | < ϵ : the network’s training converges to θ g , i.e., lim t → ∞ θ ( t ) = θ g . There is internal structure of f ( ⋅ , θ 0 ) that computes the gradient; i.e., there is some subset of the activations whose mean over every datapoint in D is ∇ θ 0 L ( θ 0 ) . [1] This captures my intuition that a gradient hacker knows where it wants to go (in this case "get to θ g "), and then it should decide what it outputs in order to make the gradient true. Some more ambitious problems (if gradient hackers exist): Characterize the set of all gradient hackers. Show that they all must satisfy some property. Construct gradient hackers for arbitrarily large n , d , w , and neural net depth. Variations on the problem: a subset of the activations equals ∇ θ 0 L ( θ 0 ) for every input, or the subset of activations correspond to the gradient on that input. ^ This is a bit strict, but we didn't want to list different ways something could be isomorphic to the gradient.
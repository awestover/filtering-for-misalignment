Edit: This article has major flaws. See my comment below. This idea was informed by discussions with Abram Demski, Scott Garrabrant, and the MIRIchi discussion group. Summary For a logical inductor P , define logical counterfactuals by P n ( ϕ | ψ ) : = ∑ y P k ( ϕ | ψ ∧ Y = y ) P n ( Y = y ) for a suitable k < n and a random variable Y independent of ψ with respect to P k . Using this definition, one can construct agents that perform well in ASP-like problems. Motivation Recall the Agent Simulates Predictor problem: U n = 10 6 P n − 1 ( A n = 1 ) + 10 3 1 ( A n = 2 ) Naively, we want to solve this by argmaxing: A n = argmax a E n [ U n | A n = a ] Hopefully, P n ( A n = 1 ) ≈ 1 , P n − 1 ( A n = 1 ) ≈ 1 , and E n [ U n | A n = 1 ] ≈ 10 6 . Also, two-boxing should be less attractive than one-boxing: E n [ U n | A n = 2 ] ≈ 10 3 However, if we make this well-defined with ε -exploration, we'll get E n [ U n | A n = 2 ] ≈ 10 6 + 10 3 and then the agent will two-box, contradiction. Instead we'd like to use predictable exploration and set E n [ U n | A n = 2 ] : = E k [ U n | A n = 2 ] for k small enough that the right-hand side is sensible. Let's see how. Predictable exploration Choose k ≪ n so that P k ( A n = 2 ) ≫ 0 . Our agent decides whether to explore at stage k , and uses its beliefs at stage k as a substitute for counterfactuals: explore 0 : = P k ( explore 0 ) < ε explore 1 : = { 1 P k ( explore 1 = 1 ) < 1 2 2 otherwise ∀ a E n ( ϕ | A = a ) : = E k ( ϕ | A = a ∧ explore 0 ) if P n ( A = a ) < δ A n : = { explore 1 if explore 0 argmax a E n [ U n | A n = a ] otherwise Here ε , δ are small positive numbers. It's easy to see that, under reasonable assumptions, this agent 1-boxes on Agent Simulates Predictor. But it can't use the full strength of P n in its counterfactual reasoning, and this is a problem. Differential privacy To illustrate the problem, add a term to the utility function that sometimes rewards two-boxing: U n = 10 6 P n − 1 ( A n = 1 ) + 10 3 1 ( A n = 2 ) + 10 6 1 ( A n = 2 ∧ X n − 1 ) X n − 1 : = P n − 1 ( X n − 1 ) < 1 2 The agent should two-box if and only if X . Assuming that's the case, and P n − 1 knows this, we have: P n − 1 ( A n = 1 ) = 1 2 ¬ X n − 1 → E n [ U n | A n = 1 ] = 1 2 10 6 ¬ X n − 1 → E n [ U n | A n = 2 ] = E k [ U n | A n = 2 ∧ explore 0 ] = 10 3 + 1 2 10 6 So if ¬ X n − 1 , two-boxing is the more attractive option, which is a contradiction. (I'm rounding ε to zero for simplicity.) The problem is that the counterfactual has to rely on P k 's imperfect knowledge of X n − 1 . We want to combine P k 's ignorance of explore 0 with P n 's knowledge of X n − 1 . If X is independent of A conditioned on explore 0 with respect to P k , then we can do this: E k [ U | A = a ∧ explore 0 ] = ∑ x E k [ U | A = a ∧ explore 0 ∧ X = x ] P k ( X = x | A = a ∧ explore 0 ) = ∑ x E k [ U | A = a ∧ explore 0 ∧ X = x ] P k ( X = x | explore 0 ) Then replace P k ( X = x | explore 0 ) with P n ( X = x | explore 0 ) : E n [ U | A = a ∧ explore 0 ] : = ∑ x E k [ U | A = a ∧ explore 0 ∧ X = x ] P n ( X = x | explore 0 ) This is more accurate than E n [ U | A = a ∧ explore 0 ] , and unbiased. If X is not independent of A conditional on explore 0 , we can introduce an auxilliary variable and construct a version of X that is independent. This construction is a solution to the following differential privacy problem: Make a random variable Y that is a function of X and independent randomness, maximizing the mutual conditional information H ( X ; Y | A ) , subject to the constraint that A is independent of Y . Using the identity H ( X | A ) = H ( X ; Y | A ) + H ( X | A Y ) we see that the maximum is attained when H ( X | A Y ) = 0 , which means that X is a function of A and Y . Now here's the construction of Y : Let X be the finite set of possible values of X , and let A be the finite set of possible values of A . We'll iteratively construct a set Y and define a random variable Y taking values in Y . To start with, let Y = ∅ . Now choose ( a , x ) : = argmin a ∈ A x ∈ X P ( X = x , Y ∉ Y | A = a ) > 0 P ( X = x , Y ∉ Y | A = a ) and for each a ′ ∈ A ∖ { a } , choose some f ( a ′ ) ∈ X such that P ( X = f ( a ′ ) , Y ∉ Y | A = a ′ ) > 0 . Then make a random binary variable T a ′ such that P ( T a ′ ∧ X = f ( a ′ ) ∧ Y ∉ Y | A = a ′ ) = P ( X = x ∧ Y ∉ Y | A = a ) Then let y be the event defined by ( X = x ∧ Y ∉ Y ∧ A = a ) ∨ ⋁ a ′ ∈ A ∖ { a } ( T a ′ ∧ X = f ( a ′ ) ∧ Y ∉ Y ∧ A = a ′ ) and add y to Y . After repeating this process | X | | A | times, we are done. We can do this with a logical inductor as well. In general, to get a sentence T such that P k ( T ∧ B | C ) ≈ p , take T : = P k ( T ∧ B | C ) < p ∧ B ∧ C . Now given random variables U and A , and some informative sentences ϕ 1 , … , ϕ ℓ , let X ∈ { T , F } ℓ be the random variable encoding the values of ϕ 0 , … , ϕ ℓ − 1 . The above construction works approximately and conditional on explore 0 to give us a random variable Y that is approximately independent of A conditional on explore 0 with respect to P k . Now we define E n [ U | A = a ] : = ∑ y E k [ U | A = a ∧ explore 0 ∧ Y = y ] P n ( Y = y | explore 0 ) whenever P n ( A = a ) < δ . This succeeds on the problem at the beginning of this section: Assume A n = 2 ↔ X n − 1 , and assume that P n − 1 knows this. Then: P n − 1 ( A n = 1 ) = 1 2 ¬ X n − 1 → E n [ U n | A n = 1 ] = 1 2 10 6 ¬ X n − 1 → E n [ U n | A n = 2 ] = E k [ U n | A n = 2 ∧ explore 0 ∧ ¬ X n − 1 ] = 10 3 X n − 1 → E n [ U n | A n = 2 ] = 1 2 10 6 + 10 3 + 10 6 X n − 1 → E n [ U n | A n = 1 ] = E k [ U n | A n = 1 ∧ explore 0 ∧ X n − 1 = 10 6 which does not lead to contradiction. In fact, there are agents like this that do at least as well as any constant agent: Theorem Let U n ( P , A ) be a utility function defined with metasyntactic variables n , P , and A . It must be computable in polynomial time as a function of A , P f i ( n ) ( A = a ) , and X : = P f i ( n ) ( X ) < p , where f i can be any polytime functions that doesn't grow too slowly and such that f i ( n ) < n . Then there exists a logical inductor P such that for every n , there exists k < n , ε , δ > 0 , and a pseudorandom variable Y such that the agent A defined below performs at least as well on U n as any constant agent, up to a margin of error that approaches 0 as n → ∞ : explore 0 : = P k ( explore 0 ) < ε explore 1 : = ⎧ ⎪
⎪
⎪
⎪
⎪
⎪ ⎨ ⎪
⎪
⎪
⎪
⎪
⎪ ⎩ a 1 if P k ( explore 1 = a 1 ) < 1 ℓ ; else a 2 if P k ( explore 1 = a 2 ) < 1 ℓ ; else ⋮ a ℓ otherwise ∀ a E n ( ϕ | A = a ) : = ∑ y E k ( ϕ | A = a ∧ explore 0 ∧ Y = y ) P n ( Y = y | explore 0 ) if P n ( A = a ) < δ A n : = { explore 1 if explore 0 argmax a E n [ U n | A n = a ] otherwise Proof sketch Choose k smaller than the strength parameter of the weakest predictor in U n . If a n is the best constant policy for U n , assume A n = a n . Since P n can compute U n , our agent's factual estimate E n [ U n | A n = a n ] is accurate, and the counterfactual estimate E n [ U n | A n = a ′ ] for a ′ ≠ a n is an accurate estimate of the utility assigned to the constant policy a ′ , as long as we make Y rich enough. So the agent will choose a n . Thus we have an implication of the form "if P believes A n = a n , then A n = a n is true", and so we can create a logical inductor P that always believes that A n = a n for every n by adding a trader with a large budget that bids up the price of A n = a n . Isn't this just UDTv2? This is much less general than UDTv2. If you like, you can think of this as an agent that at time k chooses a program to run, and then runs that program at time n , except the program always happens to be "argmax over this kind of counterfactual". Also, it doesn't do policy selection. Next steps Instead of handing the agent a pseudorandom variable Y that captures everything important, I'd like to have traders inside a logical inductor figure out what Y should be on their own. Also, I'd rather not have to hand the agent an optimal value of k . Also, I hope that these counterfactuals can be used to do policy selection and win at counterfactual mugging.
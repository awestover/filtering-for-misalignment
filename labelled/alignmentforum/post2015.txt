(Last revised: July 2024. See changelog at the bottom.) 7.1 Post summary / Table of contents Part of the â€œIntro to brain-like-AGI safetyâ€ post series . (Much of this post had been originally published here .) The previous post presented a big picture of how I think motivation works in the human brain, but it was a bit abstract. In this post, I will walk through an example. To summarize, the steps will be: (Section 7.3) Our brains gradually develop a probabilistic generative model of the world and ourselves; (Section 7.4) Thereâ€™s a â€œcredit assignmentâ€ process, where something in the world-model gets flagged as â€œgoodâ€; (Section 7.5) Thereâ€™s a â€œvalenceâ€ signal roughly tracking the expected probability of the â€œgoodâ€ thing , along with processes that systematically lead to higher-valence thoughts over lower-valence ones. This signal drives us to â€œtryâ€ to make the â€œgoodâ€ thing happen, including via foresighted planning. All human goals and motivations come ultimately from relatively simple, genetically-hardcoded circuits in the Steering Subsystem (hypothalamus and brainstem), but the details can be convoluted in some cases. For example, sometimes Iâ€™m motivated to do a silly dance in front of a full-length mirror. Exactly what genetically-hardcoded hypothalamus or brainstem circuits are upstream of that motivation? I donâ€™t know! Indeed, I claim that the answer is currently Not Known To Science. I think it would be well worth figuring out! Umm, well, OK, maybe that specific example is not worth figuring out. But the broader project of reverse-engineering certain aspects of the human Steering Subsystem (see my discussion of â€œCategory Bâ€ in Post #3 )â€”especially those upstream of social instincts like altruism and status-driveâ€”is a project that I consider desperately important for AGI safety, and utterly neglected. More on that in Posts #12 â€“ #13 . In the meantime, Iâ€™ll pick an example of a goal that to a first approximation comes from an especially straightforward and legible set of Steering Subsystem circuitry. Here goes. Letâ€™s say (purely hypotheticallyâ€¦ ğŸ‘€) that I ate a slice of prinsesstÃ¥rta cake two years ago, and it was really yummy, and ever since then Iâ€™ve wanted to eat one again. So my running example of an explicit goal in this post will be â€œI want a slice of prinsesstÃ¥rtaâ€. PrinsesstÃ¥rta cake. I suggest eating some, in order to better understand this blog post. For science! ( Image source: my favorite local bakery .) Eating a slice of prinsesstÃ¥rta is not my only goal in life, or even a particularly important oneâ€”so it has to trade off against my other goals and desiresâ€”but it is nevertheless a goal of mine (at least when Iâ€™m thinking about it), and I would indeed make complicated plans to try bring about that goal. Like, for example, dropping subtle hints to my family. In blog posts. When my birthday is coming up. Purely hypothetically!! 7.2 Reminder from the previous post: big picture of motivation and decision-making From the previous post , hereâ€™s my diagram of motivation in the brain: See previous post for details. As also discussed in the previous post , we can split this up by which parts are â€œhardcodedâ€ by the genome, versus learned within a lifetimeâ€”i.e., Steering Subsystem versus Learning Subsystem : 7.3 Building a probabilistic generative world-model in the cortex The first step in our story is that, over my lifetime, my cortex (specifically, the Thought Generator in the top-left of the diagram above) has been building up a probabilistic generative model, mostly by predictive learning of sensory inputs ( Post #4, Section 4.7 ) (a.k.a. â€œself-supervised learningâ€). Basically, we learn patterns in our sensory input, and patterns in the patterns, etc., until we have a nice predictive model of the world (and of ourselves)â€”a giant web of interconnected entries like â€œgrassâ€ and â€œstanding upâ€ and â€œslices of prinsesstÃ¥rtaâ€. Predictive learning of sensory inputs is not fundamentally dependent on supervisory signals from the Steering Subsystem. Instead, â€œthe worldâ€ provides the ground truth about whether a prediction was correct. Contrast this with, for example, navigating the tradeoff between searching-for-food versus searching-for-a-mate: there is no â€œground truthâ€ in the environment for whether the animal is trading off optimally, except after generations of hindsight. In that case, we do need supervisory signals from the Steering Subsystem, which estimate the â€œcorrectâ€ tradeoff using heuristics hardcoded by evolution. You can kinda think of the is/ought divide , with the Steering Subsystem providing the â€œoughtâ€ (â€œto maximize genetic fitness, what ought the organism to do?â€) and predictive learning of sensory inputs providing the â€œisâ€ (â€œwhat is likely to happen next, under such-and-such circumstances?â€) That said, the Steering Subsystem is indirectly involved even in predictive learning of sensory inputsâ€”for example, I can be motivated to go learn about a topic. Anyway, every thought I can possibly think, and every plan I can possibly plan, can be represented as some configuration of this generative world-model data structure. The data structure is also continually getting edited, as I learn and experience new things. When you think of this world-model data structure, imagine many gigabytes of inscrutable entriesâ€”imagine things like, for example, â€œPATTERN 847836 is defined as the following sequence: {PATTERN 278561, then PATTERN 657862, then PATTERN 128669}.â€ Some entries have references to sensory inputs and/or motor outputs. And that giant inscrutable mess comprises my entire understanding of the world and myself. 7.4 Credit assignment when I first bite into the cake As I mentioned at the top, on a fateful day two years ago, I ate a slice of prinsesstÃ¥rta, and it was really good . Step back to a couple seconds earlier, as I was bringing the cake towards my mouth to take my first-ever bite. At that moment, I didnâ€™t yet have any particularly strong expectation of what it would taste like, or how it would make me feel. But once it was in my mouth, mmmmmmm, oh wow, thatâ€™s good cake. Relevant parts of the diagram for what happened when I took my first surprisingly-delicious bite of prinsesstÃ¥rta, two years ago. So, as I took that bite, my body had a suite of autonomic reactionsâ€”releasing certain hormones, salivating, changing my heart rate and blood pressure, etc. Why? The key is that, as described in Post #3, Section 3.2.1 , all sensory inputs split: One copy of any given sensory signal goes to the Learning Subsystem, to be integrated into the predictive world-model. (See â€œInformational inputsâ€ at the top left of the diagram.) A second copy of the same signal goes to the Steering Subsystem, where it serves as an input to genetically-hardwired circuitry. (See â€œInformational inputsâ€ at the bottom-center of the diagram.) Taste bud inputs are no exception: the former signal winds up at the gustatory cortex within the insula (part of the neocortex, in the Learning Subsystem), the latter at the gustatory nucleus of the medulla (part of the brainstem, in the Steering Subsystem). After its arrival at the medulla, the taste inputs feed into various genetically-hardcoded brainstem circuits, which, when also prompted with the taste and mouth-feel of the cake, and also accounting for my current physiological state and so on, execute all those autonomic reactions I mentioned. As I mentioned, before I first bit into the cake, I didnâ€™t expect it to be that good. Well, maybe intellectually I expected itâ€”if you had asked me, I would have said and believed that the cake would be really good. But I didnâ€™t viscerally expect it. What do I mean by â€œviscerallyâ€? Whatâ€™s the difference? The things I viscerally expect are over on the â€œThought Assessorâ€ side. People donâ€™t have voluntary control over their Thought Assessorsâ€”the latter are trained exclusively by the â€œground truth in hindsightâ€ signals from the brainstem. You do have some ability to manipulate them by controlling what youâ€™re thinking about, as discussed in the previous post (Section 6.3.3) , but to a first approximation theyâ€™re doing their own thing, independent of what you want them to be doing. From an evolutionary perspective, this design makes good sense as a defense against wireheadingâ€”seeÂ my post Reward Is Not Enough . So when I bit into the cake, my Thought Assessors were wrong! They expected the cake to cause mild â€œyummyâ€-related autonomic reactions, but in fact the cake caused intense â€œyummyâ€-related autonomic reactions. And the Steering Subsystem knew that the Thought Assessors had been wrong. So it sent correction signals up to the Thought Assessor algorithms, as shown in the diagram above. Those algorithms then edited themselves, so that going forward, every time I bring a fork-full of prinsesstÃ¥rta towards my mouth, the Thought Assessors will be more liable to predict intense hormones, goosebumps, valence, and all the other reactions that I did in fact get. A cool thing just happened here. We started with a simple-ish hardwired algorithm: Steering Subsystem circuits turning certain types of taste inputs into certain hormones and autonomic reactions. But then we transferred that information into functions on the learned world-model â€”recall that giant inscrutable database I was talking about in the previous section. (Let me pause to spell this out a bit: The â€œground truth in hindsightâ€ signal tweaks some of the Thought Assessors. The Thought Assessors, youâ€™ll recall from Post #5 , are a set of maybe hundreds of models, each trained by supervised learning. The inputs to those trained models, or what I call â€œcontextâ€ signals (see Post #4 ), include neurons from inside the predictive world-model that encode â€œwhat thought is being thunk right nowâ€. So we wind up with a function (trained model) whose input includes things like â€œwhether my current thought activates the abstract concept of prinsesstÃ¥rtaâ€, and whose output is a signal that tells the Steering Subsystem to consider salivating etc.) I call this stepâ€”where we edit the Thought Assessorsâ€”â€œcredit assignmentâ€. Much more about that process in upcoming posts, including how it can go awry. So now the Thought Assessors have learned that whenever the â€œmyself eating prinsesstÃ¥rtaâ€ concept â€œlights upâ€ in the world-model, they should issue predictions of the corresponding hormones, other reactions, and positive valence. However, I think most of these are only active in the seconds before I bite into a prinsesstÃ¥rta, and only valence is able to drive long-term planning (see my discussion at Incentive Learning vs Dead Sea Salt Experiment ). 7.5 Planning towards goals via reward-shaping I donâ€™t have a particularly rigorous model for this step, but I think I can lean on intuitions a bit, in order to fill in the rest of the story: Remember, ever since my first bite of prinsesstÃ¥rta two years ago, the Thought Assessors in my brain have been inspecting each thought I think, checking whether the â€œmyself eating prinsesstÃ¥rtaâ€ concept in my world-model is â€œlit upâ€ / â€œactivatedâ€, and to the extent that it is, issuing a suggestion to prepare for valence, salivation, goosebumps, and so on. The diagram above suggests a series of thoughts that I think would â€œlight upâ€ the world-model concept more and more, as we go from top to bottom. To get the intuition here, maybe try replacing â€œprinsesstÃ¥rtaâ€ with â€œsuper-salty crackerâ€. Then go down the list, and try to feel how each thought would make you salivate more and more. Or better yet, replace â€œeating prinsesstÃ¥rtaâ€ with â€œasking my crush out on a dateâ€, go down the list, and try to feel how each thought makes your heart rate jump up higher and higher. Here's another way to think about it: If you imagine the world-model being vaguely like a Bayes net , you can imagine that the â€œdegree of pattern-matchingâ€ corresponds roughly to the probability assigned to the â€œeating prinsesstÃ¥rtaâ€ node. For example, if youâ€™re confident in X, and X weakly implies Y, and Y weakly implies Z, and Z weakly implies â€œeating prinsesstÃ¥rtaâ€, then â€œeating prinsesstÃ¥rtaâ€ gets a very low but nonzero probability, a.k.a. weak activation, and this is akin to having a far-fetched but not completely impossible plan to eat prinsesstÃ¥rta. (Donâ€™t take this paragraph too literally, Iâ€™m just trying to summon intuitions here.) Iâ€™m really hoping this kind of thing is intuitive. After all, Iâ€™ve seen it reinvented numerous times!Â For example, David Hume : â€œThe first circumstance, that strikes my eye, is the great resemblance betwixt our impressions and ideas in every other particular, except their degree of force and vivacity.â€Â And hereâ€™s William James : â€œIt is hardly possible to confound the liveliest image of fancy with the weakest real sensation.â€ In both these cases, I think the authors are gesturing at the idea that imagination activates some of the same mental constructs (latent variables in the world-model) as perception does, but that imagination activates them more weakly than perception. OK, if youâ€™re still with me, letâ€™s go back to my decision-making model, now with different parts highlighted: Relevant parts of the diagram for the process of making and executing a foresighted plan to procure prinsesstÃ¥rta. Again, every time I think a thought, the Steering Subsystem looks at the corresponding â€œscorecardâ€, and issues a corresponding valence. Recall also that the active thought / plan gets thrown out when its valence is negative, and it gets kept and strengthened when its valence is positive. Iâ€™ll oversimplify for a second, and ignore everything except the valence. (This is not much of an oversimplificationâ€”as I argue here , valence is generally the main factor at play in long-term planning, more on which below.) And Iâ€™ll also assume the Steering Subsystem is in defer-to-predictor mode for the valence signal, rather than override mode (see Post #6, Section 6.4.2 ). In this case, each time our thoughts move down a notch on the purple arrow diagram aboveâ€”from idle musing about prinsesstÃ¥rta, to a hypothetical plan to get prinsesstÃ¥rta, to a decision to get prinsesstÃ¥rta, etc.â€”thereâ€™s an immediate increase in valence, such that the new thought tends to persist at the expense of the old one. And conversely, each time we move back up the listâ€”from decision to hypothetical plan to to idle musingâ€”thereâ€™s an immediate decrease in valence. Leaving aside mechanistic details, the algorithms are designed so as to generate thoughts with as high valence as possible. So the system tends to move its way down the list, making and executing a good plan to eat cake. So there you have it! From this kind of setup, I think we're well on the way to explaining the full suite of behaviors associated with humans doing foresighted planning towards explicit goalsâ€”including knowing that you have the goal, making a plan, pursuing instrumental strategies as part of the plan, replacing good plans with even better plans, updating plans as the situation changes, pining in vain for unattainable goals, and so on. 7.5.1 The other Thought Assessors. Or: The heroic feat of ordering a cake for next week, when youâ€™re feeling nauseous right now By the way, what of the other Thought Assessors? PrinsesstÃ¥rta, after all, is not just associated with â€œvalence guessâ€, but also â€œsweet taste guessâ€, â€œsalivation guessâ€, etc. Do those play any role? Sure! For one thing, as I bring the fork towards my mouth, on the verge of consummating my cake-eating plan, Iâ€™ll start salivating and releasing cortisol in preparation. But what about the process of foresighted planning (calling the bakery etc.)? I argued here that rodents seem to relying exclusively on valence for decisions whose consequences are more than a few seconds out. But I wouldnâ€™t make such a strong statement for humans. For humans (and I think other primates), I think the other, non-valence-function, Thought Assessors are at least somewhat involved even in long-term planning. For example, imagine youâ€™re feeling terribly nauseous. Of course your Steering Subsystem knows that youâ€™re feeling terribly nauseous. And then suppose it sees you thinking a thought that seems to be leading towards eating. In that case, the Steering Subsystem may say: â€œThatâ€™s a terrible thought! Negative valence!â€ OK, so youâ€™re feeling nauseous, and you pick up the phone to place your order at the bakery. This thought gets weakly but noticeably flagged by the Thought Assessors as â€œlikely to lead to eatingâ€. Your Steering Subsystem sees that and says â€œBoo, given my current nausea, that seems like a bad thought.â€ It will feel a bit aversive. â€œYuck, Iâ€™m really ordering this huge cake??â€ you say to yourself. Logically , you know that come next week , when you actually receive the cake, you wonâ€™t feel nauseous anymore, and youâ€™ll be delighted to have the cake. But still, right now, you feel kinda gross and unmotivated to order it. Do you order the cake anyway? Sure! Maybe the valence Thought Assessor is strong enough to overrule the effects of the â€œwill lead to eatingâ€ Thought Assessor. Or maybe you call up a different motivation: you imagine yourself as the kind of person who has good foresight and makes good sensible decisions, and who isnâ€™t stuck in the moment. Thatâ€™s a different thought in your head, which consequently activates a different set of Thought Assessors, and maybe that gets high valence from the Steering Subsystem. Either way, you do in fact call the bakery to place the cake order for next week, despite feeling nauseous right now. What a heroic act! Changelog July 2024: Since the initial version, Iâ€™ve updated the diagrams (see changelog of previous two posts), and used the term â€œvalenceâ€ much more (e.g. instead of â€œrewardâ€ or â€œvalueâ€) now that I have my valence series to link to. I also added text in various places suggesting that long-term planning is mainly reliant on the â€œvalence guessâ€ Thought Assessor, and that the â€œvisceralâ€ Thought Assessors are comparatively unimportant if weâ€™re talking about more than a few seconds in advance (see here ). I didnâ€™t appreciate that fact when I first wrote this. I also made various smaller wording changes, including in my (vague) discussion of how the brain moves down the purple arrow towards higher-valence thoughts.
For some time, me and others have been looking at ways of normalising utility functions , so that we can answer questions like: Suppose that you are uncertain between maximising U 1 and U 2 , what do you do? ...without having to worry about normalising U 1 or U 2 (since utility functions are only defined up to positive affine transformations). I've long liked the mean-max normalisation; in this view, what matters is the difference between a utility's optimal policy, and a random policy. So, in a sense, each utility function has a equal shot of moving the outcome away from an expected random policy, and towards themselves. The intuition still seems good to me, but the "random policy" is a bit of a problem. First of all, it's not all that well defined - are we talking about a policy that just spits out random outputs, or one that picks randomly among outcomes? Suppose there are three options, option A (if A is output), option B' (if B' is output), or do nothing (any other output), should we really say that A happens twice as often as B' (since typing out A randomly is twice as likely that typing out B'?). Relatedly, if we add another option C, which is completely equivalent to A for all possible utilities, then this redefines the random policy. There's also a problem with branching - what if option A now leads to twenty choices later, while B leads to no further choices, are we talking about twenty-one equivalent choices, or twenty equivalent choices and one other one as likely as all of them put together? Also, the concept has some problem with infinite option sets. A more fundamental problem is that the random policy includes options that neither U 1 nor U 2 would ever consider sensible. Random dictator policy These problems can be solved by switching instead to the random dictator policy as the default, rather than a random policy. Assume we are hesitating between utility functions U 1 , U 2 , ... U n , with π ∗ i the optimal policy for utility U i . Then the random dictator policy is just π r d which picks a π ∗ i at random and then follows that. So π r d = 1 n π ∗ 1 + 1 n π ∗ 2 + … + 1 n π ∗ n . Normalising to the random dictator policy This π r d is an excellent candidate for replacing the random policy in the normalisation. It is well defined, it would never choose options that all utilities object to, and it doesn't care about how options are labelled or about how to count them. Therefore we can present the random dictator normalisation: if you are hesitating between utility functions U 1 , U 2 , ... U n , then normalise each one to ˆ U i as follows: ˆ U i = U i E π ∗ i [ U i ] − E π r d [ U i ] , where E π ∗ i [ U i ] is the expected utility of U i given optimal policy, and E π r d [ U i ] is its expected utility given the random dictator policy. Our overall utility to maximise then becomes: U = 1 n ( ˆ U 1 + ˆ U 2 + … + ˆ U n ) . Note that that normalisation has a singularity when E π ∗ i [ U i ] = E π r d [ U i ] . But realise what that means: it means that the random dictator policy is optimal for U i . That means that every single π ∗ j is optimal for U i . So, though the explosion in the normalisation means that we must pick an optimal policy for U i , this set is actually quite large, and we can use the normalisations of the other U j to pick from among it (so maximising U i becomes a lexicographic preference for us). Normalising a distribution over utilities Now suppose that there is a distribution over the utilities - we're not equally sure of each U i , instead we assign a probability p i to them. Then the random dictator policy is defined quite obviously as: π r d = p 1 π ∗ 1 + p 2 π ∗ 2 + … + p n π ∗ n . And the normalisation can proceed as before, generating the ˆ U i , and maximising the normalised sum: U = p 1 ˆ U 1 + p 2 ˆ U 2 + … + p n ˆ U n . Properties The random dictator normalisation has all the good properties of the mean-max normalisation in this post , namely that the utility is continuous in the data and that it respects indistinguishable choices. It is also invariant under cloning (ie adding another option that is completely equivalent to one of the options already there), which the mean-max normalisation does not. But note that, unlike all the normalisations in that post, it is not a case of normalising each U i without looking at the other U j , and only then combining them. Each normalisation of U i takes the other U j into account, because of the definition of the random dictator policy. Problems? Double counting, or the rich get richer Suppose we are hesitating between utilities U 1 (with 9 / 10 probability) and U 2 (with 1 / 10 ) probability. Then π r d = ( 9 / 10 ) π ∗ 1 + ( 1 / 10 ) π ∗ 1 is the random dictator policy, and is likely to be closer to optimal for U 1 than for U 2 . Because of this, we expect U 1 to get "boosted" more by the normalisation process than U 2 does (since the normalisation is the inverse of the difference between π r d and the optimal policies). But then when we take the weighted sum, this advantage is compounded, because the boosted ˆ U 1 is weighted 9 / 10 versus 1 / 10 for the relatively unboosted ˆ U 2 . It seems that the weight of U 1 thus gets double-counted. A similar phenomena happens when we are equally indifferent between utilities U 1 , U 2 , ... U 10 , if the U 1 , ... U 9 all roughly agree with each other while U 10 is completely different: the similarity of the first nine utilities seems to give them a double boost effect. There are some obvious ways to fix this (maybe use √ p i rather than p i ), but they all have problems with continuity, either when p i → 0 , or when U i → U j . I'm not sure how much of a problem this is.
We introduce a variant of the concept of a " quantilizer " for the setting of choosing a policy for a finite Markov decision process (MDP), where the generic unknown cost is replaced by an unknown penalty term in the reward function. This is essentially a generalization of quantilization in repeated games with a cost independence assumption. We show that the "quantilal" policy shares some properties with the ordinary optimal policy, namely that (i) it can always be chosen to be Markov (ii) it can be chosen to be stationary when time discount is geometric (iii) the "quantilum" value of an MDP with geometric time discount is a continuous piecewise rational function of the parameters, and it converges when the discount parameter λ approaches 1. Finally, we demonstrate a polynomial-time algorithm for computing the quantilal policy, showing that quantilization is not qualitatively harder than ordinary optimization. Background Quantilization (introduced in Taylor 2015 ) is a method of dealing with " Extremal Goodhart's Law ". According to Extremal Goodhart, when we attempt to optimize a utility function U ∗ : A → R by aggressively optimizing a proxy U : A → R , we are likely to land outside of the domain where the proxy is useful. Quantilization addresses this by assuming an unknown cost function C : A → [ 0 , ∞ ) whose expectation E ζ [ C ] w.r.t. some reference probability measure ζ ∈ Δ A is bounded by 1. ζ can be thought of as defining the "domain" within which U is well-behaved (for example it can be the probability measure of choices made by humans). We can then seek to maximize E [ U ] while constraining E [ C ] by a fixed bound C max : ~ ξ ∗ : ∈ a r g m a x ξ ∈ Δ A { E ξ [ U ] ∣ ∣ ∣ ∀ C : A → [ 0 , ∞ ) : E ζ [ C ] ≤ 1 ⟹ E ξ [ C ] ≤ C max } Alternatively, we can choose some parameter η ∈ ( 0 , ∞ ) and maximize the minimal guaranteed expectation of U − η C : ξ ∗ : ∈ a r g m a x ξ ∈ Δ A inf C : A → [ 0 , ∞ ) { E ξ [ U − η C ] ∣ ∣ ∣ E ζ [ C ] ≤ 1 } These two formulations are almost equivalent since both amount to finding a strategy that is Pareto efficient w.r.t. to the two objectives E [ U ] and − E [ C ] . For ~ ξ ∗ the tradeoff is governed by the parameter C max and for ξ ∗ the tradeoff is governed by the parameter η . Indeed, it is easy to see that any ξ ∗ is also optimal for the first criterion if we take C max = E ξ ∗ [ C ] , and any ~ ξ ∗ is also optimal for the latter criterion for an appropriate choice of η (it needs to be a subderivative of the Pareto frontier). In the following, we will use as our starting point the second formulation, which can be thought of as a zero-sum game in which U − η C is the utility function of an agent whose strategy set is A , and C is chosen by the adversary. The quantilal strategy ξ ∗ is the Nash equilibrium of the game. This formulation seems natural if we take η : = E ζ [ max ( U − U ∗ , 0 ) ] (a measure of how "optimistic" is U in the domain ζ ) and C : = η − 1 max ( U − U ∗ , 0 ) . In particular, the quantilum value (the value of the game) is a lower bound on the expectation of U ∗ . In principle, this formalism can be applied to sequential interaction between an agent and an environment, if we replace A by the set of policies . However, if it is possible to make structural assumptions about U and C , we can do better. Taylor explores one such structural assumption, namely a sequence of independent games in which both U and C are additive across the games. We consider a more general setting, namely that of a finite Markov decision process (MDP). Notation Given a set A , the notation A ∗ will denote the set of finite strings over alphabet A , i.e. A ∗ : = ∞ ⨆ n = 0 A n A ω denotes the space of infinite strings over alphabet A , equipped with the product topology and the corresponding Borel sigma-algebra. Given x ∈ A ω and n ∈ N , x n ∈ A is the n -th symbol of the string x (in our conventions, 0 ∈ N so the string begins from the 0th symbol.) Given h ∈ A ∗ and x ∈ A ω , the notation h ⊏ x means that h is a prefix of x . Given a set A , x ∈ A ω and n ∈ N , the notation x : n will indicate the prefix of x of length n . That is, x : n ∈ A n and x : n ⊏ x . Given a measurable space X , we denote Δ X the space of probability measures on X . Given measurable spaces X and Y , the notation K : X k → Y means that K is a Markov kernel from X to Y . Given x ∈ X , K ( x ) is the corresponding probability measure on Y . Given A ⊆ Y measurable, K ( A ∣ x ) : = K ( x ) ( A ) . Given y ∈ Y , K ( y ∣ x ) : = K ( { y } | x ) . Given J : Y k → Z , J K : X k → Z is the composition of J and K , and when Y = X , K n is the n -th composition power. Results A finite MDP is defined by a non-empty finite set of states S , a non-empty finite set of actions A , a transition kernel T : S × A k → S and a reward function R : S → R . To specify the utility function, we also need to fix a time discount function γ ∈ Δ N . This allows defining U : S ω → R by U ( x ) : = E n ∼ γ [ R ( x n ) ] We also fix an initial distribution over states ζ 0 ∈ Δ S . In "classical" MDP theory, it is sufficient to consider a deterministic initial state s 0 ∈ S , since the optimal policy doesn't depend on the initial state anyway. However, quantilization is different since the worst-case cost function depends on the initial conditions. We now assume that the cost function C : S ω → R (or, the true utility function U ∗ ) has the same form. That is, there is some penalty function P : S → [ 0 , ∞ ) s.t. C ( x ) = E n ∼ γ [ P ( x n ) ] Given a policy π : S ∗ × S k → A (where the S ∗ factor represents the past history the factor S represents the current state), we define H π ∈ Δ S ω in the usual way (the distribution over histories resulting from π ). Finally, we fix η ∈ ( 0 , ∞ ) . We are now ready to define quantilization in this setting Definition 1 π ∗ : S ∗ × S k → A is said to be quantilal relatively to reference policy σ : S ∗ × S k → A when π ∗ : ∈ a r g m a x π : S ∗ × S k → A inf P : S → [ 0 , ∞ ) { E x ∼ H π n ∼ γ [ R ( x n ) − η P ( x n ) ] ∣ ∣
∣ ∣ E x ∼ H σ n ∼ γ [ P ( x n ) ] ≤ 1 } We also define the quantilum value QV ∈ R by QV : = sup π : S ∗ × S k → A inf P : S → [ 0 , ∞ ) { E x ∼ H π n ∼ γ [ R ( x n ) − η P ( x n ) ] ∣ ∣
∣ ∣ E x ∼ H σ n ∼ γ [ P ( x n ) ] ≤ 1 } ( QV cannot be − ∞ since taking π = σ yields a lower bound of E x ∼ H σ n ∼ γ [ R ( x n ) ] − η .) In the original quantilization formalism, the quantilal strategy can be described more explicitly, as sampling according to the reference measure ζ from some top fraction of actions ranked by expected utility. Here, we don't have an analogous description, but we can in some sense evaluate the infimum over P . For any π : S ∗ × S k → A , define Z π ∈ Δ S by Z π ( s ) : = Pr x ∼ H π n ∼ γ [ x n = s ] For any μ , ν ∈ Δ S , the notation D ∞ ( μ | | ν ) signifies the Renyi divergence of order ∞ : D ∞ ( μ | | ν ) : = ln max s ∈ S μ ( s ) > 0 μ ( s ) ν ( s ) In general, D ∞ ( μ | | ν ) ∈ [ 0 , ∞ ] . Proposition 1 π ∗ : S ∗ × S k → A is quantilal relatively to reference policy σ : S ∗ × S k → A if and only if π ∗ ∈ a r g m a x π : S ∗ × S k → A ( E Z π [ R ] − η exp D ∞ ( Z π | | Z σ ) ) Also, we have QV = sup π : S ∗ × S k → A ( E Z π [ R ] − η exp D ∞ ( Z π | | Z σ ) ) If the maximization in Proposition 1 was over arbitrary ξ ∈ Δ S rather than ξ of the form Z π , we would get ordinary quantilization and sampling ξ would be equivalent to sampling some top fraction of ζ : = Z σ . However, in general, the image of the Z operator is some closed convex set which is not the entire Δ S . So far we considered arbitrary (non-stationary) policies. From classical MDP theory, we know that an optimal policy can always be chosen to be Markov: Definition 2 π : S ∗ × S k → A is said to be a Markov policy, when there is some π M : N × S k → A s.t. π ( h , s ) = π M ( | h | , s ) . Note that a priori it might be unclear whether there is even a non-stationary quantilal policy. However, we have Proposition 2 For any σ : S ∗ × S k → A , there exists a Markov policy which is quantilal relatively to σ . Now assume that our time discount function is geometric, i.e. there exists λ ∈ [ 0 , 1 ) s.t. γ ( n ) = ( 1 − λ ) λ n . Then it is known than an optimal policy can be chosen to be stationary: Definition 3 π : S ∗ × S k → A is said to be a stationary policy, when there is some π T : S k → A s.t. π ( h , s ) = π T ( s ) . Once again, the situation for quantilal policies is analogous: Proposition 3 If γ is geometric, then for any σ : S ∗ × S k → A , there exists a stationary policy which is quantilal relatively to σ . What is not analogous is that an optimal policy can be chosen to be deterministic whereas, of course, this is not the case for quantilal policies. It is known that the value of an optimal policy depends on the parameters as a piecewise rational function, and in particular it converges as λ → 1 and has a Taylor expansion at λ = 1 . The quantilum value has the same property. Proposition 4 QV is a piecewise rational continuous function of λ , η , the matrix elements of T , the values of R , the values of ζ 0 and the values of Z σ , with a final number of "pieces". Corollary 1 Assume that σ is a stationary policy. Then, QV converges as λ → 1 , holding all other parameters fixed (in the sense that, σ is fixed whereas Z σ changes as a function of λ ). It is analytic in λ for some interval [ λ 0 , 1 ] and therefore can be described by a Taylor expansion around λ = 1 inside this interval. Note that for optimal policies, Proposition 4 holds for a simpler reason. Specifically, the optimal policy is piecewise constant (since it's deterministic) and there is a Blackwell policy i.e. a fixed policy which is optimal for any λ sufficiently close to 1. And, it is easy to see that for a constant policy, the value is a rational function of λ . On the other hand, the quantilal policy is not guaranteed to be locally constant anywhere. Nevertheless the quantilum value is still piecewise rational. Finally, we address the question of the computational complexity of quantilization. We prove the following Proposition 5 Assume geometric time discount. Assume further that R ( s ) , T ( t | s , a ) , ζ 0 ( s ) , λ , Z σ ( s ) and η are rational numbers. Then: a. There is an algorithm for computing QV which runs in time polynomial in the size of the input R , T , ζ 0 , λ , Z σ and η . Also, if σ is stationary and σ ( t | s , a ) are rational, then Z σ ( s ) are also rational and can be computed from σ , T , ζ 0 and λ in polynomial time. b. Given an additional rational input parameter ϵ ∈ ( 0 , 1 ) , there is an algorithm for computing a stationary policy which is an ϵ -equilibrium in the zero-sum game associated with quantilization, which runs in time polynomial in the size of the input and ln 1 ϵ . EDIT: In fact, it is possible to do better and compute an exact quantilal policy in polynomial time. Future Directions To tease a little, here are some developments of this work that I'm planning: Apply quantilization to reinforcement learning , i.e. when we don't know the MDP in advance. In particular, I believe that the principles of quantilization can be used not only to deal with misspecified rewards, but also to deal with traps to some extent (assuming it a priori known that the reference policy has at most a small probability of falling into a trap). This has some philosophical implications on how humans avoid traps. Unify that formalism with DRL . The role of the reference policy will be played by the advisor (thus the reference policy is not known in advance but is learned online). This means we can drop the sanity condition for the advisor, at the price of (i) having a regret bound defined w.r.t. some kind of quantilum value rather than optimal value (ii) having a term in the regret bound proportional to the (presumably small) rate of falling into traps when following the reference (advisor) policy. It should be possible to further develop that by unifying it with the ideas of catastrophe DRL . Deal with more general environments, e.g. POMDPs and continuous state spaces. Proofs Proposition A.1 inf P : S → [ 0 , ∞ ) { E x ∼ H π n ∼ γ [ R ( x n ) − η P ( x n ) ] ∣ ∣
∣ ∣ E x ∼ H σ n ∼ γ [ P ( x n ) ] ≤ 1 } = E Z π [ R ] − η exp D ∞ ( Z π | | Z σ ) Proof of Proposition A.1 The definition of Z implies that E x ∼ H π n ∼ γ [ R ( x n ) − η P ( x n ) ] = E Z π [ R − η P ] Also E x ∼ H σ n ∼ γ [ P ( x n ) ] = E Z σ [ P ] Observe that E Z π [ P ] = ∑ s ∈ S Z π ( s ) P ( s ) ≤ max s ∈ S Z π ( s ) > 0 Z π ( s ) Z σ ( s ) ⋅ ∑ s ∈ S Z σ ( s ) P ( s ) = exp D ∞ ( Z π | | Z σ ) ⋅ E Z σ [ P ] It follows that for any P that satisfies the constraint E x ∼ H σ n ∼ γ [ P ( x n ) ] ≤ 1 , we have E Z σ [ P ] ≤ 1 and therefore E x ∼ H π n ∼ γ [ R ( x n ) − η P ( x n ) ] = E Z π [ R ] − η E Z π [ P ] ≥ E Z π [ R ] − η exp D ∞ ( Z π | | Z σ ) To show that the inequality can be arbitrarily close to equality, choose s ∗ ∈ S s.t. Z π ( s ∗ ) > 0 and Z π ( s ∗ ) Z σ ( s ∗ ) = exp D ∞ ( Z π | | Z σ ) . If Z σ ( s ∗ ) > 0 , we can define P ∗ by P ∗ ( s ) : = { Z σ ( s ∗ ) − 1 if s = s ∗ 0 otherwise Clearly E Z σ [ P ∗ ] = 1 and E Z π [ P ∗ ] = exp D ∞ ( Z π | | Z σ ) . We get E x ∼ H π n ∼ γ [ R ( x n ) − η P ∗ ( x n ) ] = E Z π [ R ] − η E Z π [ P ∗ ] = E Z π [ R ] − η exp D ∞ ( Z π | | Z σ ) In the case Z σ ( s ∗ ) > 0 , we can take any M > 0 and define P M by P M ( s ) : = { M if s = s ∗ 0 otherwise Clearly E Z σ [ P M ] = 0 ≤ 1 and E Z π [ P M ] = M ⋅ Z π ( s ∗ ) . We get E x ∼ H π n ∼ γ [ R ( x n ) − η P M ( x n ) ] = E Z π [ R ] − η E Z π [ P M ] = E Z π [ R ] − M ⋅ Z π ( s ∗ ) Since Z π ( s ∗ ) > 0 , we can make this expression arbitrarily low and therefore the infimum is − ∞ which is what we need since in this case D ∞ ( Z π | | Z σ ) = ∞ . Proposition 1 now follows immediately from Proposition A.1. We will use the notation Π : = { S ∗ × S k → A } (the space of all policies). We also have Π M : = { N × S k → A } (the space of Markov policies) and Π T : = { S k → A } (the space of stationary policies). Mildly abusing notation, we will view Π M as a subspace of Π and Π T as a subspace of Π M . Proposition A.2 For any σ : S ∗ × S k → A , there exists some policy which is quantilal relatively to σ . Proof of Proposition A.2 Π is the product of a countable number of copies of Δ A (indexed by S ∗ × S ). Δ A is naturally a topological space (a simplex), and we can thereby equip Π by the product topology. By Tychonoff's theorem, Π is compact. Moreover, it is easy to see that Z : Π → Δ S is a continuous mapping. Finally, observe that D ∞ ( ξ | | Z σ ) is lower semicontinuous in ξ (since it is the maximum of a number of continuous functions) and therefore E ξ [ R ] − η exp D ∞ ( ξ | | Z σ ) is upper semicontinuous in ξ . By the extreme value theorem, it follows that a quantilal policy exists. For any n ∈ N , we define Z n : Π k → S by Z n π ( s ) : = Pr x ∼ H π [ x n = s ] Proof of Proposition 2 By Proposition A.2, there is a quantilal policy π ∗ : S ∗ × S k → A . We define π † : N × S k → A by π † ( n , s ) : = E x ∼ H π ∗ [ π ∗ ( x : n , s ) | x n = s ] We now prove by induction that for any n ∈ N , Z n π ∗ = Z n π † . For n = 0 , we have Z 0 π ∗ = Z 0 π † = ζ 0 . For any n ∈ N and any π : S ∗ × S k → A , we have Z n + 1 π ( t ) = Pr x ∼ H π [ x n + 1 = t ] = E s ∼ Z n π [ Pr x ∼ H π [ x n + 1 = t | x n = s ] ] = E s ∼ Z n π ⎡ ⎢ ⎣ E x ∼ H π a ∼ π ( x : n , s ) [ T ( t | s , a ) | x n = s ] ⎤ ⎥ ⎦ To complete the induction step, observe that, by definition of π † E x ∼ H π a ∼ π ∗ ( x : n , s ) [ T ( t | s , a ) | x n = s ] = E a ∼ π † ( n , s ) [ T ( t | s , a ) ] = E x ∼ H π a ∼ π † ( n , s ) [ T ( t | s , a ) | x n = s ] Now, for any π , Z π = E n ∼ γ [ Z n π ] and therefore Z π † = Z π ∗ . We conclude that π † is also quantilal. Proof of Proposition 3 By Proposition 2, there is π ∗ : N × S k → A which is a Markov quantilal policy. We have Z n + 1 π ∗ ( t ) = Pr x ∼ H π ∗ [ x n + 1 = t ] = E s ∼ Z n π ∗ [ Pr x ∼ H π ∗ [ x n + 1 = t | x n = s ] ] = E s ∼ Z n π ∗ a ∼ π ∗ ( n , s ) [ T ( t | s , a ) ] Taking the expected value of this equation w.r.t. n ∼ γ we get E n ∼ γ [ Z n + 1 π ∗ ] = E n ∼ γ s ∼ Z n π ∗ a ∼ π ∗ ( n , s ) [ T ( s , a ) ] Also, we have Z π ∗ = E n ∼ γ [ Z n π ∗ ] = ( 1 − λ ) ∞ ∑ n = 0 λ n Z n π ∗ = ( 1 − λ ) ( ζ 0 + λ ∞ ∑ n = 0 λ n Z n + 1 π ∗ ) = ( 1 − λ ) ζ 0 + λ E n ∼ γ [ Z n + 1 π ∗ ] It follows that Z π ∗ = ( 1 − λ ) ζ 0 + λ E n ∼ γ s ∼ Z n π ∗ a ∼ π ∗ ( n , s ) [ T ( s , a ) ] Z π ∗ = ( 1 − λ ) ζ 0 + λ E s ∼ Z π ∗ ⎡ ⎢
⎢
⎢ ⎣ E n ∼ γ t ∼ Z n π ∗ a ∼ π ∗ ( n , s ) [ T ( s , a ) | t = s ] ⎤ ⎥
⎥
⎥ ⎦ Define π † : S k → A by π † ( s ) : = E n ∼ γ t ∼ Z n π ∗ [ π ∗ ( n , s ) | t = s ] We get Z π ∗ = ( 1 − λ ) ζ 0 + λ E s ∼ Z π ∗ [ E a ∼ π † ( s ) [ T ( s , a ) ] ] Define the linear operator T † : R S → R S by the matrix T † t s : = E a ∼ π † ( s ) [ T ( t | s , a ) ] Viewing Δ S as a subset of R S , we get Z π ∗ = ( 1 − λ ) ζ 0 + λ T † Z π ∗ Z π ∗ = ( 1 − λ ) ( 1 − λ T † ) − 1 ζ 0 On the other hand, we have Z π † = ( 1 − λ ) ∞ ∑ n = 0 λ n T † n ζ 0 = ( 1 − λ ) ( 1 − λ T † ) − 1 ζ 0 Therefore, Z π † = Z π ∗ and π † is also quantilal. Proposition A.3 Assume geometric time discount. Consider any ζ ∈ Δ S . Define the linear operators I : R S → R S × A and T : R S → R S × A by the matrices I s a , t : = [ [ t = s ] ] T s a , t : = T ( t | s , a ) Define the linear operator A : R S ⊕ R S → R S × A ⊕ R S ⊕ R by A ( V , P ) : = ( ( I − λ T ) V + η ( 1 − λ ) I P , P , − E ζ [ P ] ) Define B ∈ R S × A ⊕ R S ⊕ R by B : = ( ( 1 − λ ) I R , 0 , − 1 ) Define D ⊆ R S ⊕ R S as follows D : = { X ∈ R S ⊕ R S ∣ ∣ A X ≥ B } Here, vector inequalities are understood to be componentwise. Then, QV ζ : = sup π : S ∗ × S k → A ( E Z π [ R ] − η exp D ∞ ( Z π | | ζ ) ) = inf ( V , P ) ∈ D E ζ 0 [ V ] In particular, if ζ = Z σ , the above expression describes QV Proof of Proposition A.3 Denote Π det : = { S ∗ × S → A } . As usual in extensive-form games, behavioral strategies are equivalent to mixed strategies and therefore the image of the pushforward operator Z ∗ : Δ Π det → Δ S is the same as the image of Z : Π → Δ S . It follows that QV = sup μ ∈ Δ Π det inf P : S → [ 0 , ∞ ) { E Z ∗ μ [ R − η P ] ∣ ∣ ∣ E ζ [ P ] ≤ 1 } Π det is a compact Polish space (in the sense of the product topology) and therefore Δ Π det is compact in the weak topology. By Sion's minimax theorem QV = inf P : S → [ 0 , ∞ ) max π ∈ Π det { E Z π [ R − η P ] ∣ ∣ ∣ E ζ [ P ] ≤ 1 } Now consider any X = ( V , P ) ∈ D . A X ≥ B implies (looking at the R S × A component) that ( I − λ T ) V + η ( 1 − λ ) I P ≥ ( 1 − λ ) I R ( I − λ T ) V ≥ ( 1 − λ ) I ( R − η P ) V ( s ) − λ ∑ t ∈ S T ( t | s , a ) V ( t ) ≥ ( 1 − λ ) ( R ( s ) − η P ( s ) ) V ( s ) ≥ ( 1 − λ ) ( R ( s ) − η P ( s ) ) + λ max a ∈ A ∑ t ∈ S T ( t | s , a ) V ( t ) Therefore, there is some R ′ ≥ R s.t. V ( s ) = ( 1 − λ ) ( R ′ ( s ) − η P ( s ) ) + λ max a ∈ A ∑ t ∈ S T ( t | s , a ) V ( t ) The above is the Bellman equation for a modified MDP with reward function R ′ − η P . Denote Z s the version of the Z operator for the initial state s (instead of ζ 0 ). We get V ( s ) = max π ∈ Π det E Z s π [ R ′ − η P ] ≥ max π ∈ Π det E Z s π [ R − η P ] Observing that E s ∼ ζ 0 [ Z s π ] = Z π , we get E ζ 0 [ V ] ≥ E s ∼ ζ 0 [ max π ∈ Π det E Z s π [ R − η P ] ] ≥ max π ∈ Π det E Z π [ R − η P ] On the other hand, for any P ∈ R S s.t. P ≥ 0 and E Z σ [ P ] ≤ 1 (these inequalities correspond to the R S ⊕ R component of A X ≥ B ), there is ( V , P ) ∈ D s.t. E ζ 0 [ V ] = max π ∈ Π det E Z π [ R − η P ] Namely, this V is the solution of the Bellman equation for the reward function R − η P . Therefore, we have max π ∈ Π det E Z π [ R − η P ] = min V ∈ R S { E ζ 0 [ V ] ∣ ∣ ∣ ( V , P ) ∈ D } Taking the infimum of both sides over P inside the domain { P ≥ 0 , E ζ [ P ] ≤ 1 } we get QV ζ = inf P : S → [ 0 , ∞ ) max π ∈ Π det { E Z π [ R − η P ] ∣ ∣ ∣ E ζ [ P ] ≤ 1 } = inf ( V , P ) ∈ D E ζ 0 [ V ] Proof of Proposition 4 Consider the characterization of QV in Proposition A.3. By general properties of systems of linear inequalities, there is some J ⊆ ( S × A ) ⊔ S ⊔ { ∙ } s.t. D ♯ J : = { X ∈ R S ⊕ R S ∣ ∣ A J X = B J } ⊆ a r g m i n ( V , P ) ∈ D E ζ 0 [ V ] Here, the notation A J means taking the submatrix of A consisting of the rows corresponding to J . Similarly, B J is the subvector of B consisting of the components corresponding to J . (To see this, use the fact that the minimum of a linear function on a convex set is always attained on the boundary, and apply induction by dimension.) Moreover, D ♯ J has to be a single point X J ∈ R S ⊕ R S . Indeed, if it has more than one point then it contains a straight line L . The projection of L on the second R S has to be a single point P 0 , otherwise some point in L violates the inequality P ≥ 0 which would contradict L ⊆ D . Therefore, the projection of L on the first R S is also a straight line L ′ . As in the proof of Proposition A.3, for any ( V , P ) ∈ D , V ( s ) is an upper bound on the value of state s in the MDP with reward function R − η P . In particular, if ( V , P 0 ) ∈ D then V ( s ) ≥ min t ∈ S ( R ( t ) − η P 0 ( t ) ) . However, since L ′ is a line, there must be some s ∗ ∈ S s.t. V ( s ∗ ) can be any real number for some ( V , P 0 ) ∈ L . This is a contradiction. Denote Q : R S ⊕ R S → R S the projection operator on the first direct summand. It follows that we can always choose J s.t. | J | = 2 | S | and we have X J = A − 1 J B J QV = E ζ 0 [ Q A − 1 J B J ] For each J , the expression on the right hand side is a rational function of ζ 0 and the matrix elements of A and B which, in turn, are polynomials in the parameters the dependence on which we are trying to establish.  Therefore, this expression is a rational function in those parameters (unless det A J vanishes identically, in which case this J can ignored). So, QV always equals one of a finite set of rational functions (corresponding to difference choices of J ). The continuity of QV also easily follows from its characterization as min ( V , P ) ∈ D E ζ 0 [ V ] . Proposition A.4 Assume geometric time discount and stationary σ . Then, Z σ is a rational function of σ , T , ζ 0 and λ with rational coefficients. Proof of Proposition A.4 Define the linear operator T σ : R S → R S by the matrix T σ t s = E a ∼ σ ( s ) [ T ( t | s , a ) ] We have Z σ = ( 1 − λ ) ∞ ∑ n = 0 λ n T σ n ζ 0 = ( 1 − λ ) ( 1 − λ T σ ) − 1 ζ 0 Proof of Corollary 1 By Propositions 4 and A.4, QV is a continuous piecewise rational function of λ with a finite number of pieces. Two rational functions can only coincide at a finite number of points (since a polynomial can only have a finite number of roots), therefore there is only a finite number of values of λ in which QV "switches" from one rational function to another. It follows that there is some λ 0 ∈ [ 0 , 1 ) s.t. QV is a fixed rational function of λ in the interval [ λ 0 , 1 ) . Moreover, it always holds that min s ∈ S R ( s ) − η ≤ QV ≤ max s ∈ S R ( s ) The first inequality holds since, the guaranteed performance of the quantilal policy is at least as good as the guaranteed performance of σ . The second inequality is a consequence of the requirement that P is non-negative. It follows that QV cannot have a pole at λ = 1 and therefore must converge to a finite value there. Proof of Proposition 5 The algorithm for QV is obtained from Proposition A.3 using linear programming. The claim regarding Z σ for stationary σ follows from Proposition A.4. We now describe the algorithm for computing an ϵ -equilibrium. For any a ∈ A , define d a : A ⊔ { ⊥ } → A by d a ( b ) : = { a if b = ⊥ b otherwise Consider any β : S k → A ⊔ { ⊥ } . We define T β : S × A k → S by T β ( s , a ) : = E b ∼ β ( s ) [ T ( s , d a ( b ) ) ] Let Z β : Π k → S be the Z -operator for the MDP with transition kernel T β , and define QV β : = sup π : S ∗ × S k → A ( E Z β π [ R ] − η exp D ∞ ( Z β π ∣ ∣ ∣ ∣ Z σ ) ) It is easy to see that QV β = QV if an only if there is π : S k → A quantilal s.t. π ( a | s ) ≥ β ( a | s ) . Indeed, the MDP with kernel T β is equivalent to the original MDP under the constraint that, when in state s , any action a has to be taken with the minimal probability β ( a | s ) . In particular QV β ≤ QV (since we constrained the possible policies but not the possible penalties). So, if π as above exists, then we can use it to construct a stationary policy for the new MDP with guaranteed value QV . Conversely, if the new MDP has a stationary policy with guaranteed value QV then it can be used to construct the necessary π . Using Proposition A.3, we can compute QV β for any rational β by linear programming. This allows us to find the desired policy by binary search on β , one (state,action) pair at a time.
% operators that are separated from the operand by a space % operators that require brackets % operators that require parentheses % Paper specific We generalize the formalism of dominant markets to account for stochastic "deductive processes," and prove a theorem regarding the asymptotic behavior of such markets. In a following post , we will show how to use these tools to formalize the ideas outlined here . Appendix A contains the key proofs. Appendix B contains the proofs of technical propositions used in Appendix A, which are mostly straightforward. Appendix C contains the statement of a version of the optional stopping theorem from Durret . ##Notation Given X a topological space: P ( X ) is the space of Borel probability measures on X equipped with the weak* topology. C ( X ) is the Banach space of continuous functions X → R with uniform norm. B ( X ) is the Borel σ -algebra on X . U ( X ) is the σ -algebra of universally measurable sets on X . Given μ ∈ P ( X ) , supp μ denotes the support of μ . Given X and Y measurable spaces, K : X mk − → Y is a Markov kernel from X to Y . For any x ∈ X , we have K ( x ) ∈ P ( Y ) . Given μ ∈ P ( X ) , μ ⋉ K ∈ P ( X × Y ) is the semidirect product of μ and K and K ∗ μ ∈ P ( Y ) is the pushforward of μ by K . Given X , Y Polish spaces, π : X → Y Borel measurable and μ ∈ P ( X ) , we denote μ ∣ π the set of Markov kernels K : Y mk − → X s.t. π ∗ μ ⋉ K is supported on the graph of π and K ∗ π ∗ μ = μ . By the disintegration theorem, μ ∣ π is always non-empty and any two kernels in μ ∣ π coincide π ∗ μ -almost everywhere. ##Results The way we previously laid out the dominant market formalism, the sequence of observations (represented by the sets { X i } ) was fixed. To study forecasting, we instead need to assume this sequence is sampled from some probability measure (the true environment). For each n ∈ N , let O n be a compact Polish space. O n represents the space of possible observations at time n . Denote Y n : = ∏ m < n O m Given n ≤ m , π n m : Y m → Y n denotes the projection mapping and π n : = π n , n + 1 . Denote X = ∞ ∏ n = 0 O n X is a compact Polish space. For each n ∈ N we denote π n ω : X → Y n the projection mapping. Given y ∈ Y n , we denote X y : = π − 1 n ω ( y ) , a closed subspace of X . Given n ∈ Z and x ∈ X , we denote x ( n ) : = π max ( n , 0 ) ω ( x ) . #Definition 1 A market is a sequence of mappings { M n : Y n → P ( X ) } n ∈ N s.t. Each M n is measurable w.r.t. U ( Y n ) and B ( P ( X ) ) . For any y ∈ Y n , supp M n ( y ) ⊆ X y . As before, we define the space of trading strategies T ( X ) : = C ( P ( X ) × X ) , but this time we regard it as a Banach space. #Definition 2 A trader is a sequence of mappings { T n : Y n × P ( X ) n → T ( X ) } n ∈ N which are measurable w.r.t. U ( Y n ) ⊗ B ( P ( X ) n ) and B ( T ( X ) ) . Given a trader T and a market M , we define the mappings { T M n : Y n → T ( X ) } n ∈ N (measurable w.r.t. U ( Y n ) and B ( T ( X ) ) ) and { ¯ T M n : Y n → C ( X ) } n ∈ N (measurable w.r.t. U ( Y n ) and B ( C ( X ) ) ) as follows: T M n ( y ) : = T n ( y , M 0 ( π 0 n ( y ) ) , M 1 ( π 1 n ( y ) ) … M n − 1 ( π n − 1 , n ( y ) ) ) ¯ T M n ( y ) : = T M n ( y , M n ( y ) ) The "market maker" lemma now requires some additional work due to the measurability requirement: #Lemma Consider any trader T . Then, there is a market M s.t. for all n ∈ N and y ∈ Y n supp M n ( y ) ⊆ a r g m a x X y ¯ T M n ( y ) As before, we have the operator W : T ( X ) → T ( X ) defined by W τ ( μ , x ) : = τ ( μ , x ) − E z ∼ μ [ τ ( μ , z ) ] We also introduce the notation { W ¯ T M n : Y n → C ( X ) } n ∈ N and { Σ W T M n : Y max ( n − 1 , 0 ) → C ( X ) } n ∈ N which are measurable mappings defined by W ¯ T M n ( y ) = W T M n ( y , M n ( y ) ) Σ W T M n ( y ) = ∑ m < n W ¯ T M m ( π m n ( y ) ) #Definition 3 A market M is said to dominate a trader T when for any x ∈ X , if inf n ∈ N min z ∈ X x ( n ) Σ W T M n + 1 ( x ( n ) , z ) > − ∞ then sup n ∈ N max z ∈ X x ( n ) Σ W T M n + 1 ( x ( n ) , z ) < + ∞ #Theorem 1 Given any countable set of traders R , there is a market M s.t. M dominates all T ∈ R . Theorem 1 is proved exactly as before (modulo Lemma), and we omit the details. We now describe a class of traders associated with a fixed environment μ ∗ ∈ P ( X ) s.t. if a market dominates a trader from this class, a certain function of the pricing converges to 0 with μ ∗ -probability 1. In a future post, we will apply this result to a trader associated with an incomplete models Φ ⊆ P ( X ) by observing that the trader is in the class for any μ ∗ ∈ Φ . #Definition 4 A trading metastrategy is a uniformly bounded family of measurable mappings { υ n : Y n → T ( X ) } n ∈ N . Given μ ∗ ∈ P ( X ) , υ is said said to be profitable for μ ∗ , when there are β > 0 and { K n ∈ μ ∗ ∣ π n ω } n ∈ N s.t. for any n ∈ N , π n ω ∗ μ ∗ -almost any y ∈ Y n and any μ ∈ P ( X y ) : E K n ( y ) [ υ ( y , μ ) ] − E μ [ υ ( y , μ ) ] ≥ β ( max X y υ ( y , μ ) − min X y υ ( y , μ ) ) Even if a metastrategy is profitable, it doesn't mean that a smart trader should use this metastrategy all the time: in order to avoid running out of budget, a trader shouldn't place too many bets simultaneously. The following construction defines a trader that employs a metastrategy only when all previous bets are closed to being resolved. #Definition 5 Fix a metastrategy υ . We define the trader T υ and the measurable mappings { U υ n : Y n × P ( X ) n → C ( X ) } n ∈ N recursively as follows: U 0 : = 0 T υ 0 : = υ 0 U υ , n + 1 ( y , { μ m } m ≤ n ) : = U υ n ( π n ( y ) , { μ m } m < n ) + T υ n ( y , { μ m } m ≤ n ) T υ , n + 1 ( y , { μ m } m ≤ n ) : = { υ n + 1 ( y ) if max X y U υ , n + 1 ( y , { μ m } m ≤ n ) − min X y U υ , n + 1 ( y , { μ m } m ≤ n ) ≤ 1 0 otherwise #Theorem 2 Consider μ ∗ ∈ P ( X ) , { K n ∈ μ ∗ ∣ π n ω } n ∈ N , υ a metastrategy profitable for μ ∗ and M a market. Assume M dominates T υ . Then, for μ ∗ -almost any x ∈ X : lim n → ∞ ( E K n ( x ( n ) ) [ υ ( x ( n ) , M n ( x ( n ) ) ) ] − E M n ( x ( n ) ) [ υ ( x ( n ) , M n ( x ( n ) ) ) ] ) = 0 That is, the market price of the "stock portfolio" traded by υ converges to its true μ ∗ -expected value. ##Appendix A #Proposition A.1 Fix X a compact Polish space and τ ∈ T ( X ) . Then, there exists μ ∈ P ( X ) s.t. supp τ ( μ ) ⊆ a r g m a x τ #Proof of Proposition A.1 Follows immediately from "Proposition 1" from before and Proposition B.6. #Proposition A.2 Fix Y , Y ′ compact Polish spaces. Denote X : = Y × Y ′ . Then, there exists M : Y × T ( X ) → P ( X ) measurable w.r.t. B ( Y × T ( X ) ) and B ( P ( X ) ) s.t. for any y ∈ Y and τ ∈ T ( X ) : supp M ( y , τ ) ⊆ a r g m a x y × Y ′ τ ( M ( y , τ ) ) #Proof of Proposition A.2 Define Z 1 , Z 2 , Z ⊆ Y × T ( X ) × P ( X ) by Z 1 : = { ( y , τ , μ ) ∈ Y × T ( X ) × P ( X ) ∣ supp μ ⊆ X y } Z 2 : = { ( y , τ , μ ) ∈ Y × T ( X ) × P ( X ) ∣ E μ [ τ ( μ ) ] = max y ∈ Y ′ τ ( μ , y , y ′ ) } Z : = { ( y , τ , μ ) ∈ Y × T ( X ) × P ( X ) ∣ supp μ ⊆ a r g m a x y × Y ′ τ ( μ ) } We can view Z as the graph of a multivalued mapping from Y n × T ( X ) to P ( X ) . We will now show this multivalued mapping has a selection , i.e. a single-valued measurable mapping whose graph is a subset. Obviously, the selection is the desired M . Z 1 is closed by Proposition B.7. Z 2 is closed by Proposition B.5. Z = Z 1 ∩ Z 2 by Proposition B.6 and hence closed. In particular, the fiber Z y τ of Z over any ( y , τ ) ∈ Y × T ( X ) is also closed. For any y ∈ Y , τ ∈ T ( X ) , define i y : Y ′ → X by i y ( y ′ ) : = ( y , y ′ ) and τ y ∈ T ( Y ′ ) by τ y ( ν , y ′ ) : = τ ( i y ∗ ν , y , y ′ ) . Applying Proposition A.1 to τ y we get ν ∈ P ( Y ′ ) s.t. supp τ y ( ν ) ⊆ a r g m a x τ y It follows that ( y , τ , i y ∗ ν ) ∈ Z and hence Z y τ is non-empty. Consider any U ⊆ P ( X ) open. Then, A U : = ( Y × T ( X ) × U ) ∩ Z is locally closed and in particular F σ . Therefore, the image of A U under the projection to Y × T ( X ) is also F σ and in particular Borel. Applying the Kuratowski-Ryll-Nardzewski measurable selection theorem, we get the desired result. #Proof of Lemma For any n ∈ N , let M n : Y n × T ( X ) → P ( X ) be as in Proposition A.2. We define M n recursively by: M n ( y ) : = M n ( y , T M n ( y ) ) #Proposition A.3 Consider X a probability space, { F n ⊆ 2 X } n ∈ N a filtration of X , t , α , β > 0 , { S n : X → R } n ∈ N and { Δ n : X → [ 0 , t ] } n ∈ N stochastic processes adapted to F . Assume that: E [ | S 0 | ] < ∞ ∀ n ′ ≥ n : | S n ′ − S n | ≤ n ′ − 1 ∑ m = n Δ m + α E [ S n + 1 − S n ∣ F n ] ≥ β Δ n Then, inf n S n > − ∞ with probability 1. The proof will use the following definition: #Definition A Consider a sequence { t n ∈ [ 0 , 1 ] } n ∈ N . The accumulation times of t are { n k ∈ N ⊔ { ∞ } } k ∈ N defined recursively by n 0 : = 0 n k + 1 = { inf { n ∈ N ∣ ∑ n − 1 m = n k t m ≥ 1 } if n k < ∞ ∞ if n k = ∞ Consider X a probability space and { Δ n : X → [ 0 , 1 ] } n ∈ N a stochastic process. The accumulation times of Δ are { N k : X → N ⊔ { ∞ } } k ∈ N defined pointwise as above. Clearly, they are stochastic processes and whenever Δ is adapted to a filtration { F n ⊆ 2 X } n ∈ N , they are stopping times w.r.t. F . #Proof of Proposition A.3 Without loss of generality, we can assume t = 1 (otherwise we can renormalize S , Δ and α by a factor of t − 1 ). Define { S 0 n : X → R } n ∈ N by S 0 n : = S n − β n − 1 ∑ m = 0 Δ m By Proposition B.8, S 0 is a submartingale. Let { N n : X → N ⊔ { ∞ } } n ∈ N be the accumulation times of Δ . By proposition N23, { S 0 min ( n , N k ) } n ∈ N are submartingales for all k . By Proposition\ N24, each of them is uniformly integrable. Using the fact that N k ≤ N k + 1 to apply Theorem C, we get E [ S 0 N k + 1 ∣ F N k ] ≥ S 0 N k Clearly, { S 0 N k } k ∈ N is adapted to { F N k } k ∈ N . Doob's second martingale convergence theorem implies that E [ | S 0 N k | ] < ∞ ( S 0 N k is the limit of the uniformly integrable submartingale { S 0 min ( n , N k ) } n ∈ N ). We conclude that { S 0 N k } k ∈ N is a submartingale. By Proposition B.12, | S 0 N k + 1 − S 0 N k | ≤ α + 2 . Applying the Azuma-Hoeffding inequality, we conclude that for any positive integer k : Pr [ S 0 N k − S 0 < − β k ] ≤ exp ( − ( β k ) 2 2 ( α + 2 ) 2 k ) = exp ( − β 2 k 2 ( α + 2 ) 2 ) Since ∑ k exp ( − β 2 k 2 ( α + 2 ) 2 ) < ∞ , it follows that Pr [ ∃ k ∈ N ∀ l > k : S 0 N l − S 0 ≥ − β l ] = 1 Pr [ ∃ k ∈ N ∀ l > k : S N l − S 0 ≥ β ( N l − 1 ∑ n = 0 Δ n − l ) ] = 1 By Proposition B.13 Pr [ ∞ ∑ n = 0 Δ n = ∞ ⟹ ∃ k ∈ N ∀ l > k : S N l − S 0 ≥ 0 ] = 1 It remains to show that if x ∈ X is s.t. inf n S n ( x ) = − ∞ then the condition above fails. Consider any such x ∈ X . | S n ( x ) − S 0 ( x ) | ≤ ∑ n − 1 m = 0 Δ n ( x ) + α , therefore ∑ ∞ n = 0 Δ n ( x ) = ∞ . On the other hand, by Proposition B.14, inf k S N k ( x ) ( x ) = − ∞ . #Proposition A.4 Consider X a probability space, { F n ⊆ 2 X } n ∈ N a filtration of X , t , α , β > 0 , { S ′ n : X → R } n ∈ N and { Δ n : X → [ 0 , t ] } n ∈ N stochastic processes adapted to F and { S n : X → R } n ∈ N an arbitrary stochastic process. Assume that: | S n − S ′ n | ≤ α 4 E [ | S 0 | ] < ∞ | S n + 1 − S n | ≤ Δ n E [ S n + 1 − S n ∣ F n ] ≥ β Δ n Then, inf n S n > − ∞ (equivalently inf n S ′ n > − ∞ ) with probability 1. #Proof of Proposition A.4 Define S ′′ n : = E [ S n ∣ F n ] . We have E [ | S ′′ 0 | ] = E [ | E [ S 0 ∣ F 0 ] | ] ≤ E [ E [ | S 0 | ∣ F 0 ] ] = E [ | S 0 | ] < ∞ | S ′′ n − S ′ n | = | E [ S n ∣ F n ] − S ′ n | = | E [ S n − S ′ n ∣ F n ] | ≤ α 4 | S ′′ n − S n | ≤ | S ′′ n − S ′ n | + | S ′ n − S n | ≤ α 2 ∀ n ′ ≥ n : | S ′′ n ′ − S ′′ n | ≤ | S n ′ − S n | + α ≤ n ′ − 1 ∑ m = n Δ m + α E [ S ′′ n + 1 − S ′′ n ∣ F n ] = E [ E [ S n + 1 ∣ F n + 1 ] − E [ S n ∣ F n ] ∣ F n ] = E [ S n + 1 − S n ∣ F n ] ≥ β Δ n By Proposition A.3, inf n S ′′ n > − ∞ with probability 1. Since | S ′′ n − S n | ≤ α 2 , we get the desired result. #Proposition A.5 Consider { O n } n ∈ N , { Y n : = ∏ m < n O m } n ∈ N and X : = ∏ n O n as before. Consider μ ∗ ∈ P ( X ) , { K n ∈ μ ∗ ∣ π n ω } n ∈ N , υ a metastrategy profitable for μ ∗ and M a market. Then, for μ ∗ -almost any x ∈ X : inf n ∈ N min z ∈ X x ( n ) Σ W T M υ , n + 1 ( x ( n ) , z ) > − ∞ #Proof of Proposition A.5 We regard X as a probability space using the σ -algebra U ( X ) and the probability measure μ ∗ . For any n ∈ N , we define F n ⊆ U ( X ) and S n , S ′ n , Δ n : X → R by F n : = π − 1 n ω ( U ( Y n ) ) S n ( x ) : = Σ W T M υ n ( x ( n − 1 ) , x ) S ′ n ( x ) : = min z ∈ X x ( n − 1 ) Σ W T M υ n ( x ( n − 1 ) , z ) Δ n ( x ) : = max z ∈ X x ( n ) ¯ T M υ n ( x ( n ) , z ) − min z ∈ X x ( n ) ¯ T M υ n ( x ( n ) , z ) Clearly, F is a filtration of X , S , S ′ , Δ are stochastic processes and S ′ , Δ are adapted to F . υ is uniformly bounded, therefore T υ is uniformly bounded and so is Δ . Obviously, Δ is also non-negative. By Proposition B.15, | S n − S ′ n | are uniformly bounded. S 0 is bounded and in particular E [ | S 0 | ] < ∞ . We have | S n + 1 ( x ) − S n ( x ) | = | W ¯ T M υ n ( x ( n ) , x ) | ≤ Δ n ( x ) Let β > 0 and K n ∈ μ ∗ ∣ π n ω be as in Definition 4. E [ S n + 1 − S n ∣ F n ] = E z ∼ K n ( x ( n ) ) [ W ¯ T M υ n ( x ( n ) , z ) ] E [ S n + 1 − S n ∣ F n ] = E z ∼ K n ( x ( n ) ) [ ¯ T M υ n ( x ( n ) , z ) ] − E z ∼ M n ( x ( n ) ) [ ¯ T M υ n ( x ( n ) , z ) ] By definition of T υ , ¯ T M υ n ( x ( n ) ) is equal to either υ n ( x ( n ) , M n ( x ( n ) ) ) or 0. In either case, we get (almost everywhere) E [ S n + 1 − S n ∣ F n ] ≥ β ( max X x ( n ) ¯ T M υ n ( x ( n ) ) − min X x ( n ) ¯ T M υ n ( x ( n ) ) ) = β Δ n Applying Proposition A.4, we get the desired result. #Proposition A.6 Consider the setting of Proposition A.3. Then, for almost all x ∈ X : sup n ∈ N S n ( x ) < ∞ ⟹ ∞ ∑ n = 0 Δ n ( x ) < ∞ #Proof of Proposition A.6 Define { S 0 n : X → R } n ∈ N by S 0 n : = S n − β n − 1 ∑ m = 0 Δ m Let { N n } n ∈ N be the accumulation times of Δ . Consider any x ∈ X s.t. sup n S n ( x ) = s ( x ) < ∞ but ∑ n Δ n ( x ) = ∞ . Proposition B.13 implies that S 0 N k ( x ) ( x ) ≤ S N k ( x ) ( x ) − β k ≤ s ( x ) − β k As in the proof of Proposition A.3, we can apply the Azuma-Hoeffding inequality to S 0 N and get that for any positive integer k Pr [ S 0 N k − S 0 < − k 3 4 ] ≤ exp ( − k 3 2 2 ( α + 2 ) 2 k ) = exp ( − k 1 2 2 ( α + 2 ) 2 ) It follows that ∞ ∑ k = 1 Pr [ S 0 N k − S 0 < − k 3 4 ] < ∞ Pr [ ∃ k ∈ N ∀ l > k : S 0 N l − S 0 < − l 3 4 ] = 0 Pr [ ∃ m ∈ N ∀ k ∈ N : S 0 N k ≤ m − β k ] = 0 Comparing with the inequality from before, we reach the desired conclusion. #Proposition A.7 Consider the setting of Proposition A.4. Then, for almost all x ∈ X : sup n ∈ N S n ( x ) < ∞ ⟹ ∞ ∑ n = 0 Δ n ( x ) < ∞ #Proof of Proposition A.7 Define S ′′ n : = E [ S n ∣ F n ] . As in the proof of Proposition A.4, S ′′ meets the conditions of Proposition A.3 and thus of Proposition A.6 also. By Proposition A.6, for almost all x ∈ X : sup n ∈ N S ′′ n ( x ) < ∞ ⟹ ∞ ∑ n = 0 Δ n ( x ) < ∞ As in the proof of Proposition A.4, | S ′′ n − S n | is uniformly bounded, giving the desired result. #Proof of Theorem 2 Let F , S , S ′ and Δ be as in the proof of Proposition A.5. Using Proposition A.5 and the assumption that M dominates T υ , we conclude that for μ ∗ -almost any x ∈ X , sup n S n ( x ) < ∞ . As in the proof of Proposition A.5, the conditions of Proposition A.4 are satisfied, and therefore the conditions of Proposition A.7 are also satisfied. Applying Proposition A.7, we conclude that ∑ n Δ n ( x ) < ∞ . By Proposition B.17, it follows that for any n ≫ 0 , T M υ n ( x ( n ) ) = υ ( x ( n ) ) . We get E K n ( x ( n ) ) [ υ ( x ( n ) , M n ( x ( n ) ) ) ] − E M n ( x ( n ) ) [ υ ( x ( n ) , M n ( x ( n ) ) ) ] = E K n ( x ( n ) ) [ ¯ T M υ n ( x ( n ) ) ] − E M n ( x ( n ) ) [ ¯ T M υ n ( x ( n ) ) ] ) E K n ( x ( n ) ) [ υ ( x ( n ) , M n ( x ( n ) ) ) ] − E M n ( x ( n ) ) [ υ ( x ( n ) , M n ( x ( n ) ) ) ] ≤ Δ n ( x ) lim n → ∞ ( E K n ( x ( n ) ) [ υ ( x ( n ) , M n ( x ( n ) ) ) ] − E M n ( x ( n ) ) [ υ ( x ( n ) , M n ( x ( n ) ) ) ] ) = 0 ##Appendix B #Proposition B.1 If X , Y are compact Polish spaces and f : X × Y → R is continuous, then F : X → C ( Y ) defined by F ( x ) ( y ) : = f ( x , y ) is continuous. We omit the proof of Proposition B.1, since it appeared as "Proposition A.2" before . #Proposition B.2 Fix X , Y compact Polish spaces. Define e : C ( Y × X ) × Y → C ( X ) by e ( f , y ) ( x ) : = f ( y , x ) . Then, e is continuous. In particular, we can apply this to Y = P ( X ) in which case e : T ( X ) × P ( X ) → C ( X ) . #Proof of Proposition B.2 Consider f k → f and y k → y . We have max x ∈ X | f k ( y k , x ) − f ( y k , x ) | ≤ ∥ f k − f ∥ → 0 By Proposition B.1 max x ∈ X | f ( y k , x ) − f ( y , x ) | → 0 Combining, we get max x ∈ X | f k ( y k , x ) − f ( y , x ) | → 0 #Proposition B.3 Fix Y , Y ′ compact Polish spaces and denote X : = Y × Y ′ . Define F : Y × C ( X ) → R by F ( y , f ) : = max y ′ ∈ Y ′ f ( y , y ′ ) Then, F is continuous. #Proof of Proposition B.3 Consider y k → y , f k → f . By Proposition B.1, y k → y implies that lim k → ∞ max y ′ ∈ Y ′ f ( y k , y ′ ) = max y ′ ∈ Y ′ f ( y , y ′ ) Since f k → f , we get lim k → ∞ max y ′ ∈ Y ′ f k ( y k , y ′ ) = max y ′ ∈ Y ′ f ( y , y ′ ) #Proposition B.4 Fix Y , Y ′ compact Polish spaces. Denote X : = Y × Y ′ . Define Z ⊆ Y × P ( X ) × C ( X ) by Z : = { ( y , μ , f ) ∈ Y × P ( X ) × C ( X ) ∣ E μ [ f ] = max y ′ ∈ Y ′ f ( y , y ′ ) } Then, Z is closed. #Proof of Proposition B.4 Consider y k → y , μ k → μ , f k → f , ( y k , μ k , f k ) ∈ Z . By Proposition B.3, we get max y ′ ∈ Y ′ f ( y , y ′ ) = lim k → ∞ max y ′ ∈ Y ′ f k ( y k , y ′ ) = lim k → ∞ E μ k [ f k ] = E μ [ f ] Hence, ( y , μ , f ) ∈ Z . #Proposition B.5 Fix Y , Y ′ compact Polish spaces. Denote X : = Y × Y ′ . Define Z ⊆ Y × P ( X ) × T ( X ) by Z : = { ( y , μ , τ ) ∈ Y × P ( X ) × T ( X ) ∣ E μ [ τ ( μ ) ] = max y ′ ∈ Y ′ τ ( μ , y , y ′ ) } Then, Z is closed. #Proof of Proposition B.5 By Proposition B.2, Z is the continuous inverse image of a subset of Y × P ( X ) × C ( X ) which is closed by Proposition B.4. #Proposition B.6 Fix X a compact Polish space. Consider f ∈ C ( X ) and μ ∈ P ( X ) and denote M : = max f . Then, supp μ ⊆ f − 1 ( M ) iff E μ [ f ] = M . #Proof of Proposition B.6 If supp μ ⊆ f − 1 ( M ) then Pr x ∼ μ [ f ( x ) ≠ M ] = 0 and therefore E μ [ f ] = M . Now, assume E μ [ f ] = M . For any k ∈ N , Markov's inequality yields Pr x ∼ μ [ M − f ( x ) ≥ 1 k ] ≤ k E x ∼ μ [ M − f ( x ) ] = 0 Taking k → ∞ , we get Pr x ∼ μ [ M > f ( x ) ] = 0 and hence supp μ ⊆ f − 1 ( M ) . #Proposition B.7 Consider Y , Y ′ compact Polish spaces. Denote X : = Y × Y ′ . Define Z ⊆ Y × P ( X ) by Z : = { ( y , μ ) ∈ Y × P ( X ) ∣ supp μ ⊆ y × Y ′ } Then, Z is closed. #Proof of Proposition B.7 We fix metrizations for Y and Y ′ and metrize X by d X ( ( y 1 , y ′ 1 ) , ( y 2 , y ′ 2 ) ) : = max ( d Y ( y 1 , y 2 ) , d Y ′ ( y ′ 1 , y ′ 2 ) ) For each y ∈ Y , denote d y : = d y × Y ′ . Consider y k → y , μ k → μ , ( y k , μ k ) ∈ Z . We have E μ k [ d y k ] = 0 . By Proposition B.1, d y k → d y , therefore E μ [ d y ] = 0 . By Proposition B.6, supp μ ⊆ y × Y ′ and hence ( y , μ ) ∈ Z . #Proposition B.8 Consider X a probability space, { F n ⊆ 2 X } n ∈ N a filtration of X , { S n : X → R } n ∈ N and { Δ n : X → [ 0 , 1 ] } n ∈ N stochastic processes adapted to F . Assume that there are α , β > 0 s.t.: E [ | S 0 | ] < ∞ | S n − S 0 | ≤ n − 1 ∑ m = 0 Δ m + α E [ S n + 1 − S n ∣ F n ] ≥ β Δ n Define { S 0 n : X → R } n ∈ N by S 0 n : = S n − β n − 1 ∑ m = 0 Δ m Then, S 0 is a submartingale. #Proof of Proposition B.8 Obviously, S 0 is adapted to F . We have E [ | S 0 n | ] ≤ E [ | S n | ] + β n ≤ E [ | S 0 | ] + E [ | S n − S 0 | ] + β n ≤ E [ | S 0 | ] + 2 β n + α < ∞ E [ S 0 n + 1 ∣ F n ] = E [ S n + 1 ∣ F n ] − β n ∑ m = 0 Δ m E [ S 0 n + 1 ∣ F n ] ≥ E [ S n ∣ F n ] + β Δ n − β n ∑ m = 0 Δ m E [ S 0 n + 1 ∣ F n ] ≥ E [ S n ∣ F n ] − β n − 1 ∑ m = 0 Δ m E [ S 0 n + 1 ∣ F n ] ≥ E [ S 0 n ∣ F n ] #Proposition B.9 Let X be a probability space, { F n ⊆ 2 X } n ∈ N a filtration on X , { S n : X → R } n ∈ N a stochastic process adapted to F and N : X → N ⊔ { ∞ } a stopping time (w.r.t. F ). Suppose S is submartingale. Then, { S min ( n , N ) } n ∈ N is also submartingale. #Proof of Proposition B.9 Clearly, { S min ( n , N ) } n ∈ N is adapted to F . We have | S min ( n , N ) | ≤ ∑ m ≤ n | S m | E [ | S min ( n , N ) | ] ≤ E [ ∑ m ≤ n | S m | ] = ∑ m ≤ n E [ | S m | ] < ∞ For any n ∈ N , define A n ⊆ X by A n : = { x ∈ X ∣ N ( x ) > n } N is a stopping time, therefore A n ∈ F n . We have S min ( n + 1 , N ) − S min ( n , N ) = χ A n ( S n + 1 − S n ) E [ S min ( n + 1 , N ) − S min ( n , N ) ∣ F n ] = E [ χ A n ( S n + 1 − S n ) ∣ F n ] E [ S min ( n + 1 , N ) − S min ( n , N ) ∣ F n ] = χ A n E [ S n + 1 − S n ∣ F n ] ≥ 0 #Proposition B.10 Consider a sequence { t n ∈ [ 0 , 1 ] } n ∈ N . Let { n k ∈ N ⊔ { ∞ } } k ∈ N be the accumulation times of t . Then: n k − 1 ∑ n = 0 t n ≤ 2 k #Proof of Proposition B.10 We prove by induction on k . For k = 0 , the claim is obvious. If n k = ∞ then n k + 1 = ∞ and, by the induction hypothesis n k + 1 − 1 ∑ n = 0 t n = ∞ ∑ n = 0 t n = n k − 1 ∑ n = 0 t n ≤ 2 k ≤ 2 ( k + 1 ) Now assume n k < ∞ . Then n k + 1 > n k (because for the sum in the definition of accumulation times to be ≥ 1 it has to be non-empty), therefore n k + 1 − 1 ∑ n = 0 t n = n k − 1 ∑ n = 0 t n + n k + 1 − 1 ∑ n = n k t n By the induction hypothesis n k + 1 − 1 ∑ n = 0 t n ≤ 2 k + n k + 1 − 1 ∑ n = n k t n If n k + 1 < ∞ then n k + 1 − 1 ∑ n = 0 t n ≤ 2 k + n k + 1 − 2 ∑ n = n k t n + t n k + 1 − 1 By definition of accumulation times, the middle term is < 1 . By definition of t , the last term is ≤ 1 . We get n k + 1 − 1 ∑ n = 0 t n < 2 k + 1 + 1 = 2 ( k + 1 ) Finally, assume n k + 1 = ∞ . We have n k + 1 − 1 ∑ n = 0 t n ≤ 2 k + ∞ ∑ n = n k t n By definition of accumulation times, we get that for any m ∈ N : m ∑ n = n k t n < 1 It follows that ∞ ∑ n = n k t n ≤ 1 Combining, we have n k + 1 − 1 ∑ n = 0 t n ≤ 2 k + 1 < 2 ( k + 1 ) #Proposition B.11 Consider X a probability space, { F n ⊆ 2 X } n ∈ N a filtration of X , { S n : X → R } n ∈ N and { Δ n : X → [ 0 , 1 ] } n ∈ N stochastic processes adapted to F . Define { U n : X → R } n ∈ N by U n : = n − 1 ∑ m = 0 Δ m Assume that there is α ≥ 0 s.t. E [ | S 0 | ] < ∞ | S n − S 0 | ≤ U n + α Let { N k : X → N ⊔ { ∞ } } k ∈ N be the accumulation times of Δ . Then, for any k ∈ N , { S min ( n , N k ) } n ∈ N and { U min ( n , N k ) } n ∈ N are uniformly integrable. #Proof of Proposition B.11 By Proposition B.10, U min ( n , N k ) ≤ U N k ≤ 2 k , so { U min ( n , N k ) } n ∈ N is uniformly bounded and in particular uniformly integrable. Moreover: | S min ( n , N k ) | ≤ | S 0 | + U min ( n , N k ) ≤ | S 0 | + 2 k Since E [ | S 0 | ] < ∞ , it follows that { S min ( n , N k ) } n ∈ N is uniformly integrable. #Proposition B.12 Consider sequences { s n ∈ R } n ∈ N and { t n ∈ [ 0 , 1 ] } n ∈ N . Let { n k : X → N ⊔ { ∞ } } k ∈ N be the accumulation times of t . Assume that there is α ≥ 0 s.t. ∀ n ′ ≥ n : | s n ′ − s n | ≤ n ′ − 1 ∑ m = n t m + α Assume further that either all n k are finite or s converges. Denote s n k : = lim n → n k s n . Then, | s n k + 1 − s n k | < α + 2 . #Proof of Proposition B.12 Fix k ∈ N . If n k = ∞ , the claim is trivial. Consider the case n k + 1 < ∞ . We have | s n k + 1 − s n k | ≤ n k + 1 − 1 ∑ n = n k t n + α = n k + 1 − 2 ∑ n = n k t n + t n k + 1 − 1 + α < 1 + 1 + α = α + 2 Now consider the case n k < ∞ , n k + 1 = ∞ . By definition of accumulation times: ∞ ∑ n = n k t n ≤ 1 It follows that | s n k + 1 − s n k | = | lim n → ∞ s n − s n k | ≤ α + 1 . #Proposition B.13 Consider a sequence { t n ∈ [ 0 , 1 ] } n ∈ N . Let { n k ∈ N ⊔ { ∞ } } k ∈ N be the accumulation times of t . Assume that ∑ ∞ n t n = ∞ . Then: n k − 1 ∑ n = 0 t n ≥ k #Proof of Proposition B.13 We prove by induction on k . For k = 0 , the claim is obvious. ∑ ∞ n t n = ∞ implies that n 0 < n 1 < … < ∞ , therefore n k + 1 − 1 ∑ n = 0 t n = n k − 1 ∑ n = 0 t n + n k + 1 − 1 ∑ n = n k t n The first term is ≥ k by induction. The second term is ≥ 1 by definition of accumulation time. #Proposition B.14 Consider sequences { s n ∈ R } n ∈ N and { t n ∈ [ 0 , 1 ] } n ∈ N . Assume that there is α ≥ 0 s.t. ∀ n ′ ≥ n : | s n ′ − s n | ≤ n ′ − 1 ∑ m = n t m + α Assume further that inf n s n = − ∞ . In particular, ∑ ∞ n t n = ∞ . Let { n k ∈ N } k ∈ N be the accumulation times of t (finite because of the previous observation).  Then, inf k s n k = − ∞ . #Proof of Proposition B.14 We need to prove that for any s > 0 and k ∈ N , there is l ≥ k s.t. s n l < − s . We know that there is n ≥ n k s.t. s n < − ( s + α + 2 ) . We have n 0 < n 1 < … < ∞ , therefore we can choose l ≥ k s.t. n l ≤ n < n l + 1 . We get | s n − s n l | ≤ n − 1 ∑ m = n l t m + α ≤ n l + 1 − 1 ∑ m = n l t m + α = n l + 1 − 2 ∑ m = n l t m + t n l + 1 − 1 + α < 1 + 1 + α = α + 2 Therefore, s n l < − s . #Proposition B.15 Consider { O n } n ∈ N , { Y n : = ∏ m < n O m } n ∈ N and X : = ∏ n O n as before. Consider υ a metastrategy and M a market. Define S n , S ′ n : X → R by S n ( x ) : = Σ W T M υ n ( x ( n − 1 ) , x ) S ′ n ( x ) : = min z ∈ X x ( n − 1 ) Σ W T M υ n ( x ( n − 1 ) , z ) Then: | S n − S ′ n | ≤ 2 sup m ∈ N sup y ∈ Y m ∥ υ ( y ) ∥ + 1 #Proof of Proposition B.15 We prove by induction. For the basis, S 0 = S ′ 0 = 0 . Consider any n ∈ N and x ∈ X . First, assume that max z ∈ X x ( n ) Σ W T M υ n ( x ( n − 1 ) , z ) − min z ∈ X x ( n ) Σ W T M υ n ( x ( n − 1 ) , z ) > 1 Then, by definition of T υ , T M υ n ( x ( n ) ) ≡ 0 and therefore Σ W T M υ , n + 1 ( x ( n ) ) = Σ W T M υ n ( x ( n − 1 ) ) It follows that | S n + 1 ( x ) − S ′ n + 1 ( x ) | = | Σ W T M υ , n + 1 ( x ( n ) , x ) − min z ∈ X x ( n ) Σ W T M υ , n + 1 ( x ( n ) , z ) | | S n + 1 ( x ) − S ′ n + 1 ( x ) | = | Σ W T M υ n ( x ( n − 1 ) , x ) − min z ∈ X x ( n ) Σ W T M υ n ( x ( n − 1 ) , z ) | | S n + 1 ( x ) − S ′ n + 1 ( x ) | = | Σ W T M υ n ( x ( n − 1 ) , x ) − min z ∈ X x ( n − 1 ) Σ W T M υ n ( x ( n − 1 ) , z ) | | S n + 1 ( x ) − S ′ n + 1 ( x ) | ≤ | S n ( x ) − S ′ n ( x ) | Using the induction hypothesis, we conclude | S n + 1 ( x ) − S ′ n + 1 ( x ) | ≤ 2 sup m ∈ N sup y ∈ Y m ∥ υ ( y ) ∥ + 1 Now, assume that max z ∈ X x ( n ) Σ W T M υ n ( x ( n − 1 ) , z ) − min z ∈ X x ( n ) Σ W T M υ n ( x ( n − 1 ) , z ) ≤ 1 Then, T M υ n ( x ( n ) ) = υ n ( x ( n ) ) and therefore Σ W T M υ , n + 1 ( x ( n ) ) = Σ W T M υ n ( x ( n − 1 ) ) + υ n ( x ( n ) , M n ( x ( n ) ) ) We get | S n + 1 ( x ) − S ′ n + 1 ( x ) | ≤ max z ∈ X x ( n ) Σ W T M υ n ( x ( n − 1 ) , z ) − min z ∈ X x ( n ) Σ W T M υ n ( x ( n − 1 ) , z ) + 2 ∥ υ n ( x ( n ) ) ∥ By the assumption, the first two terms total to ≤ 1 , yielding the desired result. #Proposition B.16 Consider X a compact Polish space, f ∈ C ( X ) and { X n ⊆ X } n ∈ N closed s.t. X n + 1 ⊆ X n and ⋂ n X n = { x ∗ } for some x ∗ ∈ X . Then lim n → ∞ ( max X n f − min X n f ) = 0 #Proof of Proposition B.16 Choose { x + n ∈ a r g m a x X n f } n ∈ N and { x − n ∈ a r g m i n X n f } n ∈ N . max X n f − min X n f is a non-increasing sequence, therefore it is sufficient to prove that a subsequence converges to 0. Therefore, we can assume without loss of generality that x + n → x + ω and x − n → x − ω . For each n ∈ N , the fact that X n is closed implies that x + ω , x − ω ∈ X n . Therefore, x + ω = x − ω = x ∗ . We get lim n → ∞ ( max X n f − min X n f ) = lim n → ∞ ( f ( x + n ) − f ( x − n ) ) = f ( x + ω ) − f ( x − ω ) = f ( x ∗ ) − f ( x ∗ ) = 0 #Proposition B.17 Consider υ a trading metastrategy, M a market and x ∈ X . Assume that ∞ ∑ n = 0 ( max X x ( n ) ¯ T M υ n ( x ( n ) ) − min X x ( n ) ¯ T M υ n ( x ( n ) ) ) < ∞ Then, for any n ≫ 0 , T M υ n ( x ( n ) ) = υ ( x ( n ) ) . #Proof of Proposition B.17 Choose n 0 ∈ N s.t. ∞ ∑ n = n 0 ( max X x ( n ) ¯ T M υ n ( x ( n ) ) − min X x ( n ) ¯ T M υ n ( x ( n ) ) ) < 1 2 Since ¯ T M υ n ( x ( n ) ) , W ¯ T M υ n ( x ( n ) ) ∈ C ( X ) differ by a constant function, we have ∞ ∑ n = n 0 ( max X x ( n ) W ¯ T M υ n ( x ( n ) ) − min X x ( n ) W ¯ T M υ n ( x ( n ) ) ) < 1 2 In particular, for any n ≥ n 0 n − 1 ∑ m = n 0 ( max X x ( n ) W ¯ T M υ m ( x ( m ) ) − min X x ( n ) W ¯ T M υ m ( x ( m ) ) ) ≤ n − 1 ∑ m = n 0 ( max X x ( m ) W ¯ T M υ m ( x ( m ) ) − min X x ( m ) W ¯ T M υ m ( x ( m ) ) ) < 1 2 By Proposition B.16, there is n 1 ≥ n 0 s.t. for any n ≥ n 1 max X x ( n ) Σ W T M υ n 0 ( x ( n 0 − 1 ) ) − min X x ( n ) Σ W T M υ n 0 ( x ( n 0 − 1 ) ) < 1 2 Taking the sum of the last two inequalities, we conclude that for any n ≥ n 1 max X x ( n ) Σ W T M υ n ( x ( n − 1 ) ) − min X x ( n ) Σ W T M υ n ( x ( n − 1 ) ) < 1 By definition of T υ , we get the desired result. ##Appendix C The following version of the optional stopping theorem appears in Durret as Theorem 5.4.7: #Theorem C Let X be a probability space, { F n ⊆ 2 X } n ∈ N a filtration on X , { S n : X → R } n ∈ N a stochastic process adapted to F and M , N : X → N ⊔ { ∞ } stopping times (w.r.t. F ) s.t. M ≤ N . Assume that { S min ( n , N ) } n ∈ N is a uniformly integrable submartingale. Using Doob's martingale convergence theorem, we can define S N : X → R by S N ( x ) : = lim n → ∞ S min ( n , N ) ( x ) = { S N ( x ) ( x ) if N ( x ) < ∞ lim n → ∞ S n ( x ) if N ( x ) = ∞ (the above is well-defined almost everywhere, which is sufficient for our purpose) We define S M : X → R is an analogous way (this time we can't use Doob's martingale convergence theorem, but whenever M ( x ) = ∞ we also have N ( x ) = ∞ , therefore the limit almost surely converges). We also define F M ⊆ 2 X by F M : = { A ⊆ X measurable ∣ ∀ n ∈ N : A ∩ M − 1 ( n ) ∈ F n } Then: E [ S N ∣ F M ] ≥ S M
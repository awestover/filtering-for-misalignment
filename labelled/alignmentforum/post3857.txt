I used to think of AI security as largely unrelated to AI control, and my impression is that some people on this forum probably still do. I've recently shifted towards seeing control and security as basically the same, and thinking that security may often be a more appealing way to think and talk about control. This post fleshes out this view a little bit. I'm interested in any disagreement or pushback. (This view was in large part absorbed from Ian at OpenAI, but now it feels very natural.) My basic claims: The sets {security problems} and {control problems} are basically the same. Security problems sound less exotic so we should talk about them that way. And it's not a sleight of hand or anything, the technical issues really will probably occur first in a security context, and the best near-term analogies for control problems will probably be security problems. If you want to approximate the correct mindset for control using something that people are familiar with, probably security is probably your best bet. This is closely related to MIRI and Eliezer's enthusiasm about the security mindset. I'm suggesting a somewhat more literal analogy though.
EDIT : This post has been superceeded by all these four posts . A few weeks ago, I had a conversation with Paul Christiano about my counterfactual Oracle design . We didn't get as far as criticising the method. Paul's question is far more fundamental - was my design even necessary? Forwards or backwards looking The counterfactual oracle design was setup to reward the oracle for correctly guessing the ultimate value of a random variable (an outcome). This guess was conditional on humans not actually getting to see the guess. If we ignore the counterfactual aspect for the moment, assume that the oracle has seen a series of past data: pairs ( x i , y i ) . The x i are the background data the Oracle uses to estimate the values of the target variables y i . Then my design is a forward-looking oracle. If we use a quadratic loss function, at turn t , after seeing x t , it seeks to output z t , defined as: argmin z t E [ | | z t − y t | | 2 ∣ ∣ x 1 : t y 1 : t − 1 ] , where x 1 : t y 1 : t − 1 is the previous data (all x i including x t , all previous y i up to y t − 1 ). Paul instead recommended using a more traditional machine-learning approach, where you attempt to fit a function f that explains the past data, getting something like this: argmin f ∑ t − 1 i = 0 | | f ( x i ) − y i | | 2 / t + r e g , where the r e g are regularising terms that prevent overfitting. The oracle will then output z t = f ( x t ) . Call such oracle a backwards-facing oracle (BFO), because it only cares about the past. Then the key facts about the BFO are: The BFO can generally cope with humans observing z t = f ( x t ) and modifying our behaviour because of it (ie does not need a counterfactual approach). The BFO is mildly manipulative in its outputs, though not generally in the way we fear an AI being. The BFO will not generally be manipulative in a way that minimises its cost function. However, the BFO may come to believe spurious correlations. Because of this, the BFO may converge more slowly - or not at all - where another oracle design could easily deduce the correct formulas. Observer perturbation effects What happens if we observe the z t - and if this observation changes our behaviour so that y t itself is modified? Assume the oracle knows our plans and its actions First of all, note that unless we set up the system very carefully, the oracle will "know" whether we are planning to observe z t or not. The counterfactual oracle was deliberately designed that observing z t is dependent on a purely stochastic process that is under the control of neither the human nor the oracle. But in general, the Oracle will be able to deduce whether we expect to see the z t ; it can glean this information from the x t or from observing patterns in the past x i and y i . For the rest of this post, I'll focus on the situation where we do indeed observe the z t ; if we don't, then the BFO and the counterfactual oracle have similar behaviours to each other. I'll similarly assume that the BFO knows its own past outputs - the z i . It is possible for us to carefully setup the system so that it doesn't track these, but I'll assume that it has access to this information - either implicitly, through whatever process updates it's estimate of the function f , or explicitly, by deducing z i from x i and y i . Example Assume for the moment that the x i are null and irrelevant, and that each y i is computed independently from all the others, but with identical distribution. Ignoring the regularising terms in the cost function above, the best f for the BFO is simply a constant, equal to the mean of the previous y i : f ( x t ) = ∑ t − 1 i = 0 y i / t . If we observe z i (which, in this example, we assume we always do), then the value of y i is given by μ ( z i ) + v ( z i ) , where μ is a function R → R and v ( z i ) is a random variable with mean zero for all z i .
If μ is continuous (and the possible value of y i are in a bounded interval), there will be fixed points where z i = μ ( z i ) . We'll expect the behaviour of the BFO to eventually converge to one of these fixed points. Note that, in the limit, if the BFO converges to the fixed point z i , then the cost function is equal to the expectation of ( v ( z i ) ) 2 . For example, assume μ ( z i ) = z 2 i and v ≡ 0 - the relationship between observed z i and actual y i is deterministic. This has two fixed points: one at 0 and one at 1 . The point at 0 is an attractive fixed point , but the point at 1 is unstable. So unless all the data the BFO starts with has z i = 1 , the BFO will either fail to converge, or converge to z i = 0 . We can add a non-trivial v and this result still holds (indeed, a non-trivial v ( 1 ) just makes the z i = 1 point more unstable). What this means is that, even if the z i = 1 results in a lower cost function - ie E [ ( v ( 1 ) ) 2 ] < E [ ( v ( 0 ) ) 2 ] - the BFO will still not converge to it, converging to the higher cost point z i = 0 . Multiple attractive fixed points Let μ ( z i ) = − z 3 i + ( 3 / 2 ) z i . This has three fixed points, at z i = 0 and z i = ± 1 / √ 2 . These last two points are attractive fixed points. If v ≡ 0 , then the BFO will converge to either 1 / √ 2 or − 1 / √ 2 , depending on the initial data it starts with. If we add a non-trivial v , we get a slight tendency to converge towards the fixed point with lower E [ ( v ( z i ) ) 2 ] , everything else being equal. That's because a large E [ ( v ( z i ) ) 2 ] means that the next point sampled is likely to fall outside the " basin of attraction " of z i . But everything else is not generally equal. The initial data strongly skews the process. The value of the derivative of μ around the fixed points (if it exists) affects the convergence process. The value of E [ ( v ( z i ) ) 2 ] close to the fixed point (rather than at the fixed point) can have a large impact. Luck will play a role as well, if v is non-trivial. Thus, though there is a tendency to converge to fixed points with lower cost functions, this tendency is very weak. The BFO is not a cost function minimiser. Manipulation? It is technically a manipulative act to output a z t such that we will change our value of y t in consequence. But it's not a manipulative act in most of the ways we fear. For example, if z i was the encoding for a program which, if run, would set y i = z i , along with a convincing message for running that program, then z i is technically a fixed point of μ . However, it is not a fixed point that the BFO is likely to find, because z i + ϵ would not be such an encoding for almost all ϵ , so the basin of attraction for this z i is tiny (basically only those ϵ sufficiently small to preserve all the digits of z i ). Thus the BFO is very unlikely to stumble upon it. So this design does succeed in overcoming many of the problems with oracles: it will not try to manipulate us to obsessively converge on the point with lowest cost function. Problems with causality So, what's wrong with the BFO? I've written this much, pointing out its properties without endorsing it, so there must be something wrong. Actually, I think it works very well for solving some problems, and, for those, is superior to my oracle designs. But it has certain limitations. First of all, there's the question of what happens if μ has no fixed points (for example, if μ is not continuous in various annoying ways) or no attractive fixed points (for example , if μ ( z i ) = 4 z i ( 1 − z i ) ). Then the BFO may not settle down in any stable way, as it continues to try and find the fixed point of a function without one. But this problem is just a side effect of the more central point, which is that the BFO has a fundamentally wrong "understanding" of the causality of the situation (this is, of course, an informal phrasing - the BFO has no true "understanding" of the situation, in the ways we mean). Spurious beliefs Let us go back to the μ ( z i ) = − z 3 i + ( 3 / 2 ) z i and v ≡ 0 situation. Assume the x i is not empty, but is not relevant - for example, it could just be the time of day. Suppose the BFO has made three pairs of observations: { ( x 0 = 00 : 30 , y 0 = z 0 = 1 / √ 2 ) , ( x 1 = 01 : 30 , y 1 = z 1 = − 1 / √ 2 ) , ( x 2 = 02 : 30 , y 2 = z 2 = 1 / √ 2 ) } . If the BFO treated the x i as irrelevant (which they indeed are), its z 3 guess would be 1 / ( 3 ∗ √ 2 ) , and it would then eventually converge towards the fixed point z i = 1 / √ 2 . But the BFO could also conclude that the parity of the time is relevant, and that it should output 1 / √ 2 during even hours, and − 1 / √ 2 during odd hours. If it does so, it will find confirmation for its theory . As it outputs 1 / √ 2 , during even hours, it will observe that that guess was entirely correct, and similarly for odd hours. In situations where μ has multiple attractive fixed points, and the BFO has a rich and varied amount of previous data for any reason, the BFO will find, and confirm, a spurious causality for explaning these different fixed points. False models The problem is that the BFO's greatest strength has become its weakness: it is trying to explain, through function fitting, why the y t seems to have different behaviours. We know that it is because of the z t , the output of the BFO itself; but the BFO cannot take this fact into account. Its function fitting is trying to implicitly take into account the output of its own calculations without explicitly doing so; no wonder that it ends up in Löbian situations . This might result in the BFO being unable to converge to anything, in situations where the E [ | | z t − y t | | 2 ∣ ∣ x 1 : t y 1 : t − 1 ] -minimising oracle could. For example, we could imagine an environment where the x i and the y i are very varied, but the underlying causal structure is quite simple. An oracle that tracked z i as an input to the environment could deduce this causal structure quite easily. But the BFO might struggle, as it tries to fit a function where it can't explicitly take a key fact into account. Because the x i and y i are so varied, it should stumble upon many different spurious beliefs, making the job of fitting a single function to all the data extremely difficult. It would be interesting to explore the extent to which this might become a problem. In conclusion The BFO/traditional machine learning approach held up better than I supposed, and I'm thankful to Paul for bringing it to my attention. It has interesting advantages and drawbacks, and could be a useful oracle design in many situations.
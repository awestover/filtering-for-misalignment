Logical counterfactuals are when you say something like "Suppose π = 3 , what would that imply?" They play an important role in logical decision theory. Suppose you take a false proposition P and then take a logical counterfactual in which P is true. I am imagining this counterfactual as a function f ( logical propositions ) → [ 0 , 1 ] ⊂ R that sends counterfactually true statements to 1 and false statements to 0. Suppose P is " not Fermat's last theorem".  In the counterfactual where Fermat's last theorem is false, I would still expect 2+2=4. Perhaps not with measure 1, but close. So f ( " 2 + 2 = 4 " ) > 0.9 On the other hand, I would expect trivial rephrasing of Fermat's last theorem to be false, or at least mostly false. But does this counterfactual produce a specific counter example? Does it think that 14 3 + 26 3 = 29 3 ? Or does it do something where the counterfactual insists a counter-example exists, but spreads probability over many possible counter-examples. Or does it act as if there is a non-standard number counterexample? How would I compute the value of f in general? Suppose you are a LDT agent trying to work out whether to cooperate or defect in a prisoners dilemma. What does the defect counterfactual look like? Is it basically the same as reality except you in particular defect. (So exact clones of you defect, and any agent that knows your exact source-code and is running detailed simulations will defect.) Or is it broader than that, is this a counterfactual world in which all LDT agents defect in prisoners dilemma situations in general. Is this a counterfactual world in which a bunch of homo-erectus defected on each other, and then all went extinct, leaving a world without humans? All of the thought about logical counterfactuals I have seen so far is on toy problems that divide the world into Exact-simulations-of-you and Totally-different-from-you. I can't see any clear idea about what to do with the vaguely similar but not identical to you agents.
This work was inspired by a question by Vanessa Kosoy, who also contributed several of the core ideas, as well as feedback and mentorship. Abstract We outline a computationalist interpretation of quantum mechanics, using the framework of infra-Bayesian physicalism. Some epistemic and normative aspects of this interpretation are illuminated by a number of examples and theorems. 1. Introduction Infra-Bayesian physicalism was introduced as a framework to investigate the relationship between a belief about a joint computational-physical universe and a corresponding belief about which computations are realized in the physical world, in the context of "infra-beliefs". Although the framework is still somewhat tentative and the definitions are not set in stone, it is interesting to explore applications in the case of quantum mechanics. 1.1. Discussion of the results Quantum mechanics has been notoriously difficult to interpret in a fully satisfactory manner. Investigating the question through the lens of computationalism, and more specifically in the setting of infra-Bayesian physicalism provides a new perspective on some of the questions via its emphasis on formalizing aspects of metaphysics, as well as its focus on a decision-theoretic approach. Naturally, some questions remain, and some new interesting questions are raised by this framework itself. The toy setup can be described on the high level as follows (with details given in Sections 2 to 4 ). We have an "agent": in this toy model simply consisting of a policy, and a memory tape to record observations. The agent interacts with a quantum mechanical "environment": performing actions and making observations. We assume the entire agent-environment system evolves unitarily. We'll consider the agent having complete Knightian uncertainty over its own policy, and for each policy the agent's beliefs about the "universe" (the joint agent-environment system) is given by the Born rule for each observable, without any assumption on the correlation between observables (formally given by the free product). We can then use the key construction in infra-Bayesian physicalism — the bridge transform — to answer questions about the agent's corresponding beliefs about what copies of the agent (having made different observations) are instantiated in the given universe. In light of the falsity of Claims 4.15 and 4.17 , we can think of the infra-Bayesian physicalist setup as a form of many-worlds interpretation. However, unlike the traditional many-worlds interpretation, we have a meaningful way of assigning probabilities to (sets of) Everett branches, and Theorem 4.19 shows statistical consistency with the Copenhagen interpretation. In contrast with the Copenhagen interpretation, there is no "collapse", but we do assume a form of the Born rule as a basic ingredient in our setup. Finally, in contrast with the de Broglie–Bohm interpretation, the infra-Bayesian physicalist setup does not privilege particular observables, and is expected to extend naturally to relativistic settings. See also Section 8 for further discussion on properties that are specific to the toy setting and ones that are more inherent to the framework. It is worth pointing out that the author is not an expert in quantum interpretations, so a lot of opportunities are left open for making connections with the existing literature on the topic. 1.2. Outline In Section 2 we describe the formal setup of a quantum mechanical agent-environment system. In Section 3 we recall some of the central constructions in infra-Bayesian physicalism, then in Section 4 we apply this framework to the agent-environment system. In Sections 4.2 and 4.3 we write down various statements relating quantities arising in the infra-Bayesian physicalist framework to the Copenhagen interpretation of quantum mechanics. While Section 4.2 focuses on "epistemic" statements, Section 4.3 is dedicated to the "normative" aspects. A general theme in both sections is that the stronger, "on the nose" relationships between the interpretations fail, while certain weaker "asymptotic" relationships hold. In Section 5.1 we construct counterexamples to the stronger claims, and in in Sections 6 and 7 we prove the weaker claims relating the interpretations. In Section 8 we discuss which aspects of our setup are for the sake of simplicity in the toy model, and which are properties of the broader theory. 2. Setup First, we'll describe a standard abstract setup for a simplified agent-environment joint system. We have the following ingredients: A finite set A of possible actions of the agent. A finite set O of possible observations of the agent.
We'll write E = O × A , the set of     observation-action pairs. For technical reasons it will be convenient to add a symbol 0 for     "blank", and fix a bijection E + = E ⊔ { 0 } ≅ Z / N preserving 0 , where N = | O | ⋅ | A | + 1. We'll use this bijection to treat E + as an abelian     group implicitly. A Hilbert space H e corresponding to states of the     environment. Fix a finite time horizon [1] T ∈ N . A classical state     of a cyclic, length T memory tape is a function τ : Z / T → E + . Let Tp T be     the set of all classical tape states. A Hilbert space H g with orthonormal basis ∣ ∣ ψ g τ ⟩ for τ ∈ Tp , corresponding     to the quantum state of the agent. For each a ∈ A a unitary map of the environment U a : H e → H e , describing the "result of     the action". A projection-valued measure P on O , valued in H e (giving projections P o : H e → H e for each observation o ∈ O ). Let H = H g ⊗ H e be the state     space of the joint agent-environment system. Remark 2.1 . It would be interesting to consider a setting where the agent is allowed to choose the observation in each step (e.g. have the projection-valued measure P depend on the action taken). For simplicity we'll work with a fixed observation as described above. Definition 2.2 . Let O ≤ T = t ≤ T ⨆ t ∈ N O t E ≤ T = t ≤ T ⨆ t ∈ N E t be the set of observation histories and observation-action histories respectively, i.e. finite strings of observations (resp. observation-action pairs) up to length T . There's a natural map obs : E ≤ T → O ≤ T , extracting the string of observations from a string of observation-action pairs. We'll call a function π : O ≤ T → A a policy. For two histories h 1 , h 2 (of either type), we'll sometimes write h 1 ⊏ h 2 to mean h 1 is a (not necessarily proper) prefix (i.e. initial substring) of h 2 . Remark 2.3 . We only consider deterministic policies here. It's not immediately clear how one would generalize Definition 2.7 to randomized policies. In fact, we can always (and is perhaps more principled to) think of our source of randomness for a randomized policy to be included in the environment, so we don't lose out on generality by only considering deterministic policies. For example, if the source of our randomness is a quantum coin flip, then our approach offers a convenient way of modeling this by including the coin as a factor of H e , i.e. part of the environment subsystem. Definition 2.4 . For a tape state τ : Z / T → E + and an observation-action pair ε ∈ E , let mem ( τ , ε ) : Z / T → E + be the state of the tape after writing the pair ε to the tape, defined by mem ( τ , ε ) ( n ) = { τ ( n − 1 ) n ≠ 0 τ ( − 1 ) + ε n = 0. Remark 2.5 . Choosing a group structure on E + is in order to make the map mem ( − , ε ) : Tp → Tp invertible, which in turn makes the map U o π in Definition 2.7 unitary. Definition 2.6 . Let the "history extraction" map hist : Tp → E ≤ T be defined by hist ( τ ) = ( τ ( N − 1 ) , … , τ ( 0 ) ) ∈ E N , where 0 ≤ N ≤ T is largest such that there's no 0 ≤ n < N with τ ( n ) = 0 (i.e. so that the [ 0 , N ) portion of the tape contains no blanks). Definition 2.7 (Time evolution of a policy). For each policy π : O ≤ T → A , we define the single time-step unitary evolution operator U π on H as the composite of an "observation" and an "action" operator U π = U A , π ∘ U O , π , where U O , π ( ∣ ∣ ψ g τ ⟩ ⊗ P o | ψ e ⟩ ) = ∣ ∣ ψ g mem ( τ , ( o , a ) ) ⟩ ⊗ P o | ψ e ⟩ for all o ∈ O U A , π ( ∣ ∣ ψ g τ ⟩ ⊗ | ψ e ⟩ ) = ∣ ∣ ψ g τ ⟩ ⊗ U a | ψ e ⟩ for a = π ( obs ( hist ( τ ) ) ) The time evolution after t ∈ N time-steps is given by U t π = U π ∘ … ∘ U π , i.e. U π composed with itself t times. Remark 2.8 . As defined above, the first step in the evolution is an observation, so we never use the value of the policy on the empty observation string. In this respect it would be more natural to start with an action instead, but it would make some of the notation and the examples more cumbersome, so we sacrifice a bit of naturality for the sake of simplicity overall. Lemma 2.9 . The operator U π is unitary on H . Proof. The operator U A , π is clearly unitary since each U a is. We can see that U O , π is unitary as follows. Choose an orthonormal basis ∣ ∣ ψ e o , i ⟩ of P o H e for each o ∈ O , so together they form an orthonormal basis for H e (note that the range of i might vary for varying o ). Then ∣ ∣ ψ g τ ⟩ ⊗ ∣ ∣ ψ e o , i ⟩ forms an orthonormal basis for H , and U O , π permutes this basis, hence is unitary. □ 3. Prerequisites We recall some definitions and lemmas within infra-Bayesianism. This is in order to make the current article fairly self-contained, all the relevant notions here were introducted in [ IBP ], [ BIMT ] and [ LBIMT ]. In particular we omit proofs in this section, all the relevant proofs can be found in the articles listed. 3.1. Ultracontributions First of all, we work with a notion of belief intended to incorporate a form of Knightian uncertainty . Formally, this means that we work with sets of distributions (or rather "contributions" turn out to be a more flexible tool). Definition 3.1 . Given a finite set X , a contribution μ is a non-negative measure on X , such that μ ( X ) ≤ 1 . We denote the set of contributions Δ c X . A contribution is a distribution if μ ( X ) = 1 , so we have Δ X ⊂ Δ c X . There's a natural order on Δ c X , given by pointwise comparison. Definition 3.2 . We call a subset A ⊂ Δ c X downward closed if for μ ∈ A , ν ≤ μ implies ν ∈ A . As a subspace of R X , the set Δ c X inherits a metric and a convex structure. Definition 3.3 . We call a closed, convex, downward closed subset Θ ⊂ Δ c X a homogenious ulta-contribution (HUC for short). We denote the set of HUCs by □ X . We'll work with HUCs as our central formal notion of belief in this article. The exact properties required (closed, convex and downward closed) should be illuminated by Lemma 3.6 . Definition 3.4 . Given a HUC Θ ∈ □ X , and a function f : X → [ 0 , 1 ] , we define the expected value E Θ [ f ] = max θ ∈ Θ E θ [ f ] = max θ ∈ Θ ∑ x ∈ X θ ( x ) f ( x ) . Thinking of f as a loss function, this is a worst-case expected value, given Knightian uncertainty over the probabilities. Remark 3.5 . It's worth mentioning that the prefix "infra" originates from the concept of infradistributions, which is the notion corresponding to ultracontributions, in the dual setup of utility functions instead of loss functions. We still often use the term "infra" in phrases such as infra-belief or infra-Bayesianism, but now simply carrying the connotation of a "weaker form" of belief etc., compared to the Bayesian analog. Lemma 3.6 . For Θ ∈ □ X , the expected value defines a convex, monotone, homogeneous functional E Θ [ − ] : [ 0 , 1 ] X → [ 0 , 1 ] . Lemma 3.7 . There is a duality Θ ↦ E Θ , between □ X (i.e. closed, convex, and downward closed subsets of Δ c X ) and convex, monotone, and homogeneous functionals [ 0 , 1 ] X → [ 0 , 1 ] . For a functional F : [ 0 , 1 ] X → [ 0 , 1 ] , the inverse map in the duality is given by F ↦ Θ F = ⋂ f : X → [ 0 , 1 ] { θ ∈ Δ c X : E θ [ f ] ≤ F [ f ] } . 3.2. Some constructions For the current article to be more self-contained, we spell out a few definitions used in this discussion. Definition 3.8 . Given a map of finite sets f : X → Y , we define the pushforward f ∗ : Δ c X → Δ c Y to be given by the pushforward measure. We use the same notation to denote the pushforward on HUCs, f ∗ : □ X → □ Y , given by forward image, that is f ∗ ( Θ ) = { f ∗ ( θ ) ∈ Δ c Y | θ ∈ Θ } . Equivalently, in terms of the expectation values we have for g : Y → [ 0 , 1 ] E f ∗ ( Θ ) [ g ] = E Θ [ g ∘ f ] . Definition 3.9 . Given a collection of finite sets X i , and HUCs Θ i ∈ □ X i , we define the free product ⋈ i Θ i ∈ □ ( ∏ i X i ) as follows. For a contribution θ ∈ Δ c ∏ i X i we have θ ∈ ⋈ i Θ i if and only if for each j , ( pr j ) ∗ ( θ ) ∈ Θ j ⊂ Δ c X j , where pr j : ∏ i X i → X j is projection onto the i th factor. The free product thus specifies the allowed marginal values, but puts no further restriction on the possible correlations. Definition 3.10 (Total uncertainty). The state of total (Knightian) uncertainty ⊤ X ∈ □ X is defined as ⊤ X = Δ c X , i.e. the subset of all contributions. Definition 3.11 (Semidirect product). Given a map β : X → □ Y , and an element Θ ∈ □ X , we can define the semidirect product Θ ⋉ β ∈ □ ( X × Y ) . This is easier to write down in terms of the expectation functionals, as follows. For g : X × Y → [ 0 , 1 ] , define E Θ ⋉ β [ g ] = E Θ [ E β ( x ) [ g ( x , − ) ] ] . Here E β ( x ) [ g ( x , − ) ] is the function X → [ 0 , 1 ] , whose value at x ∈ X is given by by taking expected value with respect to β ( x ) ∈ □ Y of the function g ( x , − ) : Y → [ 0 , 1 ] . As a subset of Δ c ( X × Y ) , ⊤ X ⋉ β can be understood as the convex hull of the δ x × θ for all x ∈ X and all θ ∈ β ( x ) ⊂ Δ c Y . For Θ ⋉ β one needs to further restrict to contributions that project down into Θ ⊂ Δ c X . 3.3. The bridge transform The key construction we'll be considering in infra-Bayesian physicalism is the bridge transform . This construction is aimed at answering the question "given a belief about the joint computational-physical universe, what should our corresponding belief be about which computations are realized in the physical universe?". We'll discuss these notions in a bit more detail, but for now both the physical universe Φ and the computational universe Γ are just assumed to be finite sets. Definition 3.12 . Given Θ ∈ □ ( Γ × Φ ) , the bridge transform of Θ , Br ( Θ ) ∈ □ ( Γ × 2 Γ × Φ ) is defined as follows (cf. [ IBP Definition 1.1]). For a contribution θ ∈ Δ c ( Γ × 2 Γ × Φ ) we have θ ∈ Br ( Θ ) if and only if for any s : Γ → Γ , under the composite we have ~ s ( θ ) ∈ Θ ⊂ Δ c ( Γ × Φ ) . Remark 3.13 . The use of all endomorphism s : Γ → Γ in Definition 3.12 , although concise, doesn't feel fully principled as of now. We would typically think of the computational universe Γ as the set of all possible assignments of outputs to programs, i.e. Γ = Σ R , for a certain output alphabet Σ , and a set of programs R (see Definition 4.1 ). In this context, Γ Γ feels somewhat unnatural. That being said, in the current discussion we mainly use the fact that Γ Γ acts transivitely on Γ , so it's possible that these results would survive in some form under a modified definition of the bridge transform. For easy reference, we spell out [ IBP Proposition 2.10]: Lemma 3.14 (Refinement). Given a mapping between physical universes f : Φ 1 → Φ 2 , we have That is, for a belief Θ ∈ □ ( Φ 1 × Γ ) we have ( id el Γ × f ) ∗ ( Br ( Θ ) ) ⊂ Br ( ( id Γ × f ) ∗ ( Θ ) ) . 4. An infra-Bayesian physicalist interpretation We'll work with a certain specialized setup of [ IBP ]. Definition 4.1 . Let the set of "programs" R = O ≤ T , the "output alphabet" Σ = A , and the set of "computational universe states" Γ = Σ R = A O ≤ T be the set of policies up to time horizon T . We'll write el Γ = { ( π , α ) ∈ Γ × 2 Γ : π ∈ α } . Definition 4.2 . Let a "universal observable" B be a triple ( V B , Q B , t B ) , where V B is a finite set (of "observation outcomes"), Q B is a projection-valued measure on V B , valued in H (giving projections Q B ( v ) : H → H for each v ∈ V B ), and an "observation time" t B ∈ N < T . Let U be the set of all universal observables, up to the natural notion of equivalence. Remark: We use the term "universal observable" here to distinguish between observables of the "universe" (i.e. the joint agent-environment system) from the observations of the environment by the agent. Definition 4.3 (Initial state). Fix a normalized (norm 1 ) initial state ∣ ∣ ψ e 0 ⟩ ∈ H e of the environment, and let ∣ ∣ ψ g 0 ⟩ be the state of the agent corresponding to an empty memory tape, i.e. τ : Z / T → E given by τ ( n ) = 0 for all n . Let | ψ 0 ⟩ = ∣ ∣ ψ g 0 ⟩ ⊗ ∣ ∣ ψ e 0 ⟩ ∈ H be the initial state of the joint system. Definition 4.4 . For a policy π ∈ Γ , let the marginal distribution of the universal observable B be defined according to the Born rule: β B ( v | π ) = ∥ Q B ( v ) U t B π | ψ 0 ⟩ ∥ 2 . I.e. the norm square of the vector obtained by evolving the universe following policy π for t B time-steps from the initial state, and then projecting onto the observation subspace corresponding to the universal observation v ∈ V B . So β B ( − | π ) ∈ Δ V B . Definition 4.5 . Let Φ U = ∏ B ∈ U V B be the set of "all possible states of the universe" (more precisely the set of all possible outcomes of all observations on the joint agent-environment system). More generally, define Φ S analogously for any subset S ⊂ U . Definition 4.6 . For a finite subset F ⊂ U , let β F ( π ) = ⋈ B ∈ F β B ( − | π ) ∈ □ Φ F be the free product of the β B , as defined in Definition 3.9 . For varying π this defines an ultrakernel β F : Γ → □ Φ F , and the associated semidirect product Θ F = ⊤ Γ ⋉ β F ∈ □ ( Γ × Φ F ) . Taking the bridge transform and projecting out the physical factor Φ F : □ ( Γ × Φ F ) Br − → □ ( Γ × 2 Γ × Φ F ) pr ∗ − − → □ ( Γ × 2 Γ ) , we get Θ ∗ F = pr ∗ ( Br ( Θ F ) ) ∈ □ ( Γ × 2 Γ ) . If F 1 ⊂ F 2 ⊂ U , we have a natural "refinement" map p : Φ F 2 → Φ F 1 , given by projecting out the additional factors in Φ F 2 . By Lemma 3.14 , we have so Θ ∗ F 2 ⊂ Θ ∗ F 1 . Inspired by this, we have the following. Definition 4.7 . Let Θ ∗ U = ⋂ F ⊂ U Θ ∗ F , where the intersection is over all finite subsets of U . 4.1. Copenhagen interpretation Definition 4.8 . Let h ∈ E ≤ T be an observation-action history, and denote by Q h : H → H the projection corresponding to the proposition "the memory tape recorded history h ". More precisely Q h = Q g h ⊗ id H e , where Q g h ∣ ∣ ψ g τ ⟩ = { ∣ ∣ ψ g τ ⟩ if hist ( τ ) = h 0 otherwise. Definition 4.9 . Given a sequence of observation-action pairs h ∈ E n , let h ≤ m ∈ E ≤ m denote the truncated history (i.e. the image under projecting out the last n − m components of E n if n > m , and h itself if n ≤ m ). In the Copenhagen interpretation the "universe" (i.e. the joint system of the agent and the environment) collapses after each observation of the agent. Definition 4.10 . Given a policy π : O ≤ T → A , the initial state | ψ 0 ⟩ ∈ H , and a sequence of observation-action pairs h ∈ E n , we can define | ψ t ⟩ = Q h ≤ t U π | ψ t − 1 ⟩ for t > 0 recursively. Then according to the Copenhagen interpretation, the probability of observing h is Cop ( h | π ) = ∥ | ψ n ⟩ ∥ 2 . Lemma 4.11 . Collapsing at each step is the same as collapsing at the end, that is | ψ t ⟩ = Q h ≤ t U t π | ψ 0 ⟩ . Proof. The claim is true for t = 0 , 1 by definition. Assume it's true for t − 1 , so | ψ t − 1 ⟩ = Q h ≤ t − 1 U t − 1 π | ψ 0 ⟩ . Let's write U t − 1 π | ψ 0 ⟩ = ∑ τ ∈ Tp ∣ ∣ ψ g τ ⟩ ⊗ | φ e τ ⟩ , so | ψ t − 1 ⟩ = ∑ τ ∈ Tp hist ( τ ) = h ≤ t − 1 ∣ ∣ ψ g τ ⟩ ⊗ | φ e τ ⟩ . Then if a = π ( obs ( h ≤ t − 1 ) ) ∈ A , we have | ψ t ⟩ = ∑ τ ∈ Tp hist ( τ ) = h ≤ t ∣ ∣ ψ g τ ⟩ ⊗ P h ( t ) U a | φ e τ ⟩ , while U t π | ψ 0 ⟩ = ∑ τ ∈ Tp o ∈ O ∣ ∣ ψ g mem ( τ , ( o , a ) ) ⟩ ⊗ P o U π ( hist ( τ ) ) | φ e τ ⟩ . Now Q h ≤ t ∣ ∣ ψ g mem ( τ , ( o , a ) ) ⟩ = 0 unless ( o , a ) = h ( t ) and hist ( τ ) = h ≤ t − 1 , hence Q h ≤ t U t π | ψ 0 ⟩ = | ψ t ⟩ as claimed. □ 4.2. Relating the two interpretations Since Θ ∗ U ∈ □ el Γ , we can take expectations of functions f : el Γ → [ 0 , 1 ] , in particular indicator functions χ q for q ⊂ el Γ . Definition 4.12 . For a policy π ∈ Γ , and a tuple of observations h ∈ O n , define α h | π = { γ ∈ Γ | γ ( h ) = π ( h ) } ⊂ Γ , and let q h | π = { ( π , α ) ∈ el Γ | α ⊂ α h | π } ⊂ el Γ . Remark 4.13 . In what follows we'll assume | A | > 1 . This assures that the set of policies is richer than the set of histories (i.e. | Γ | > | O ≤ T | ) . Much of the following fails in the degenerate case | A | = 1 . When considering the infra-Bayesian physicalist interpretation of a quantum event h , we'll consider the expected value E Θ ∗ U [ χ π ( 1 − χ q h | π ) ] . As defined in Definition 4.6 , Θ U can be thought of as the infra-belief ⊤ Γ ⋉ β U ∈ □ ( Γ × Φ U ) , which is a joint belief over the computational-physical world, with complete Knightian uncertainty over the policy of the agent (as a representation of "free will"), and for each policy the corresponding belief about the physical world is as given by the unitary quantum evolution of the agent-environment system under the given policy. The bridge transform Θ ∗ U ∈ □ el Γ of Θ U then packages the relevant beliefs about which computational facts are manifest in the physical world. The subset α h | π corresponds to the proposition "the policy outputs action a = π ( h ) upon observing h ", and hence q h | π corresponds to the belief "the physical world witnesses the output of the policy on h to be a = π ( h ) (which is to say there's a version of the agent instantiated in the physical world that observed history h , and acted a )". We'll be investigating various claims about the quantity E Θ ∗ U [ χ π ( 1 − χ q h | π ) ] , which is the ultraprobability (i.e. the highest probability for the given Knightian uncertainty) of the agent following policy π and h not being observed (i.e. no agent being instantiated acting on history h ). Remark 4.14 . It might at first seem more natural to consider the complement instead, that is χ π χ q h | π , which corresponds to the agent following policy π , and history h being observed. However, it turns out that E Θ ∗ U [ χ π χ q h | π ] = 1 always. This can be understood intuitively via refinement (see Lemma 3.14 ): we can always extend our model of the physical world to include a copy of the agent instantiated on history h , so the highest probability of h being observed will be 1 . This is also related to the monotonicity principle discussed in [ IBP ]. Thus although at first glance this might seem less natural, in our setup it's more meaningful to study the ultraprobability of the complement, i.e. of h not being observed. Note that since we're working with convex instead of linear expectation functionals (see Lemma 3.7 ), the complementary ultraprobabilities will typically sum to something greater than one. We first state Claims 4.15 and 4.17 relating the IBP and Copenhagen interpretations "on the nose", which both turn out to be false in general. Then we state the weaker Theorem 4.19 , which is true, and establishes a form of asymptotic relationship between the two interpretations. Claim 4.15 . The two interpretations agree on the probability that a certain history is not realized given a policy. That is, E Θ ∗ U [ χ π ( 1 − χ q h | π ) ] = 1 − Cop ( h | π ) . This claim turns out to be false in general, and we give a counterexample in Counterexample 5.3 . Note, however, that the claim seems to be true in the limit with many actions (i.e. | A | → ∞ ), which would warrant further study. Now consider the following definition concerning two copies of the agent being instantiated. Definition 4.18 . For a policy π ∈ Γ , and two tuples of observations h 1 , h 2 ∈ O n , define α h 1 , h 2 | π = { γ ∈ Γ | γ ( h i ) = π ( h i ) for i = 1 , 2 } ⊂ Γ , and let q h 1 , h 2 | π = { ( π , α ) ∈ el Γ | α ⊂ α h 1 , h 2 | π } ⊂ el Γ . Claim 4.17 . There is only one copy of the agent (i.e. the agent is not instantiated on multiple histories, there are no "many worlds"). That is, if neither of h 1 , h 2 ∈ O n is a prefix of the other, then E Θ ∗ U [ χ π ( 1 − χ q h 1 , h 2 | π ) ] = 1. This claim is the relative counterpart of Claims 4.15 and fails as well in general (see Counterexample 5.5 ). Again, however, this claim might hold in the | A | → ∞ limit. Definition 4.18 . An event is a subset of histories E ⊂ O T . We define the corresponding q E | π = ⋃ h ∈ E q h | π ⊂ el Γ , and Cop ( E | π ) = ∑ h ∈ E Cop ( h | π ) . Theorem 4.19 . The ultraprobability of an agent not being instantiated on a certain event can be bounded via functions of the (Copenhagen) probability of the event. More precisely, 1 − √ ( 2 − Cop ( E | π ) ) Cop ( E | π ) ≤ E Θ ∗ U [ χ π ( 1 − χ q E | π ) ] ≤ 1 − Cop ( E | π ) . Proof. We prove the upper bound in Section 6.1 and the lower bound in Section 6.2 . □ Due to the failure of Claims 4.15 and 4.17 , we can think of the infra-Bayesian physicalist setup as a form of many-worlds interpretation. However, since √ ( 2 − Cop ( E | π ) ) Cop ( E | π ) → 0 as Cop ( E | π ) → 0 , the above Theorem 4.19 shows statistical consistency with the Copenhagen interpretation in the sense that observations that are unlikely according to the Born rule have close to 1 ultraprobability of not being instantiated (while very likely observations have close to 0 ultraprobability of uninstantiation). Remark 4.20 . For simplicity we assumed E only contains entire histories (i.e. ones of maximal length T ). It's easy to modify the definitions to account for partial histories. The inequalities in Theorem 4.19 remain true even if E includes partial histories, and the proofs are easy to adjust. We avoid doing this here in order to keep the notation cleaner. However, it's worth noting some important points here. For a partial history h , let H ⊂ O T be the set of all completions of h , i.e. H = { ~ h ∈ O T : h ⊏ ~ h } . Then we have Cop ( h | π ) = Cop ( H | π ) = ∑ ~ h ∈ H Cop ( ~ h | π ) . On the other hand, q h | π ≠ q H | π = ⋃ ~ h ∈ H q ~ h | π , so there is an important difference here between the two interpretations, which would warrant further discussion. In particular, under the infra-Bayesian physicalist interpretation it can happen that E Θ ∗ U [ χ π ( 1 − χ q H | π ) ] > E Θ ∗ U [ χ π ( 1 − χ q h | π ) ] for a partial history h and its set of completions H . This could be loosely interpreted as Everett branches "disappearing", as the ultraprobability of an agent not being instantiated on the partial history h is less than that of the agent not being instantiated on any completion of that history. 4.3. Decision theory To shed more light on the way the infra-Bayesian physicalist interpretation functions, it is interesting to consider the decision theory of the framework, along with the epistemic considerations above. Definition 4.21 . Consider a loss function L : D → R ≥ 0 , where D = E T is the set of destinies. We can then construct the physicalized loss function (cf. [ IBP Definition 3.1]) L phys : el Γ → R ≥ 0 , given by L phys ( γ , α ) = min h ∈ X α max d ∈ D h ⊏ d L ( d ) , where X α is the set of histories witnessed by α , that is X α = { h ∈ E ≤ T | ∀ g a ⊏ h , ∀ ~ γ ∈ α : ~ γ ( obs ( g ) ) = a } . Note that in our simplified context, L phys ( γ , α ) doesn't depend on γ . Definition 4.22 . We can define the worst-case expected physicalized loss associated to a policy π by L IBP ( π ) = E Θ ∗ U [ χ π ⋅ L phys ] . Under the Copenhagen model, we would instead simply consider L Cop ( π ) = E Cop [ L | π ] = ∑ d ∈ D Cop ( d | π ) L ( d ) . Remark 4.23 . Given a policy π ∈ Γ , we can consider the set of "fair" counterfactuals (cf. [ IBP Definition 1.5]) C π fair = { ( γ , α ) ∈ el Γ | ∀ h ∈ O ≤ T : ( ∀ ~ γ ∈ α , ∀ ~ h ⊏ h : ~ γ ( ~ h ) = γ ( ~ h ) ) ⟹ γ ( h ) = π ( h ) } , i.e. where if α witnesses the history h , then γ agrees with π on that history. This definition is in contrast with the "naive" counterfactuals we considered above (when writing χ π ): C π naive = { ( γ , α ) ∈ el Γ | γ = π } . In Definition 4.22 above, and generally whenever we use χ π , we could have used the indicator function of C π fair instead. The choice of counterfactuals affects the various expected values, however, all of the theorems in this article remain true (and Claims 4.15 and 4.17 remain false) for both naive and fair counterfactuals. We thus work with naive counterfactuals for the sake of simplicity. Similarly to Section 4.2 , the "on the nose" claim relating the two interpretations fails, but we have an asymptotic relationship which holds. Claim 4.24 . The two interpretations agree on the loss of any policy: L IBP ( π ) = L Cop ( π ) . Again, this turns out to be false, and we give a simple counterexample in Counterexample 5.6 . To allow discussing the asymptotic behavior, assume now that we incur a loss at each timestep, given by ℓ : E = O × A → R ≥ 0 , and we consider the total loss L = T ∑ t = 1 ℓ t : D → R ≥ 0 . We might hope that we could have at least the following. Claim 4.25 . The two interpretations agree on the loss of any policy asymptotically: L IBP ( π ) ∼ L Cop ( π ) , i.e. the difference is bounded sublinearly in T . This claim is still false in general for essentially the same reason as Claim 4.24 since certain policies might involve a one-off step that then affect the entire asymptotic loss. We give a detailed explanation in Counterexample 5.7 . We do however have the following. Theorem 4.26 . If the resulting MDP is communicating (see Definition 7.8 ), then for any policy π we have L Cop ( π ∗ ) − o ( T ) ≤ L IBP ( π ) ≤ L Cop ( π ) , where π ∗ is a Copenhagen-optimal policy. In particular, optimal losses for the IBP and Copenhagen frameworks agree asymptotically. Proof. See Theorem 7.1 for the upper bound and Theorem 7.21 for the lower bound. □ 5. Examples We'll look at a few concrete examples in detail, firstly to gain some insight into how Claims 4.15 and 4.17 fail in general, and secondly to see how our framework operates in the famously puzzling Wigner's friend scenario. 5.1. Counterexamples We'll construct simple counterexamples to Claims 4.15 and 4.17 in the smallest non-degenerate case, i.e. when | O | = 2 and | A | = 2 , and T = 1 . Let O = { o 0 , o 1 } and A = { a 0 , a 1 } . There are four policies in this case (ignoring the value of the policies on the empty input, which is irrelevant in our setting, see Remark 2.8 ), which we'll abbreviate as π 00 , π 01 , π 10 , π 11 , where π i j ( o 0 ) = a i π i j ( o 1 ) = a j . Assume h = o 0 , and π = π 00 , so α h | π = { π 00 , π 01 } . Recall [ IBP Lemma 1]: Lemma 5.1 . For ρ ∈ Δ c el Γ × Φ , we have ρ ∈ Br ( Θ ) if and only if for each s : Γ → Γ and g : Γ × Φ → [ 0 , 1 ] E ρ [ ~ g ] ≤ E Θ [ g ] , where ~ g : el Γ × Φ → [ 0 , 1 ] is given by γ , α , x ↦ χ s ( γ ) ∈ α ⋅ g ( s ( γ ) , x ) . Lemma 5.2 . Let β : Γ → Δ Φ be a kernel, Θ = ⊤ Γ ⋉ β , and Θ ∗ = pr ∗ ( Br ( Θ ) ) as above. Then E Θ ∗ [ χ π ( 1 − χ q h | π ) ] = E ( β ( π 10 ) + β ( π 11 ) ) ∧ β ( π 00 ) [ 1 ] . Proof. To obtain a lower bound (although we'll only use the upper bound for the counterexample), define the contribution ρ ∈ Δ c ( el Γ × Φ ) by ρ = δ π 00 , { π 00 , π 10 } × ϕ 10 + δ π 00 , { π 00 , π 11 } × ϕ 11 , where ϕ 10 , ϕ 11 ∈ Δ c Φ are such that ϕ 10 ≤ β ( π 10 ) , ϕ 11 ≤ β ( π 11 ) , and ϕ 10 + ϕ 11 = ( β ( π 10 ) + β ( π 11 ) ) ∧ β ( π 00 ) . One possible such choice is ϕ 10 = β ( π 10 ) ∧ β ( π 00 ) ϕ 11 = β ( π 11 ) ∧ ( β ( π 00 ) − ϕ 10 ) . Then it's easy to verify that ρ ∈ Br ( Θ ) , and E ρ [ χ π ( 1 − χ q h | π ) ] = E ( β ( π 10 ) + β ( π 11 ) ) ∧ β ( π 00 ) [ 1 ] . To obtain an upper bound, fix x 0 ∈ Φ , and use Lemma 5.1 for constant s = π 00 , and g ( γ , x ) = χ γ = π 00 ⋅ χ x = x 0 .
We have ~ g ( γ , α , x ) = χ π 00 ∈ α ⋅ g ( π 00 , x ) = χ π 00 ∈ α ⋅ χ x = x 0 , and so E ρ [ χ π 00 ∈ α ⋅ χ x = x 0 ] = E ρ [ ~ g ] ≤ E Θ [ χ γ = π 00 ⋅ χ x = x 0 ] = E β ( π 00 ) [ χ x = x 0 ] . (1) Analogously for π 10 and π 11 we get E ρ [ χ π 10 ∈ α ⋅ χ x = x 0 ] ≤ E Θ [ χ γ = π 10 ⋅ χ x = x 0 ] = E β ( π 10 ) [ χ x = x 0 ] , (2) and E ρ [ χ π 11 ∈ α ⋅ χ x = x 0 ] ≤ E Θ [ χ γ = π 11 ⋅ χ x = x 0 ] = E β ( π 11 ) [ χ x = x 0 ] . (3) Now, χ π ( 1 − χ q h | π ) χ x = x 0 ≤ χ π 00 ∈ α ⋅ χ x = x 0 , so by (1) we get E ρ [ χ π ( 1 − χ q h | π ) χ x = x 0 ] ≤ E β ( π 00 ) [ χ x = x 0 ] . (4) We also have 1 − χ q h | π ≤ χ π 10 ∈ α + χ π 11 ∈ α , since π 10 ∉ α and π 11 ∉ α together would imply α ⊂ α h | π 00 . Thus χ π ( 1 − χ q h | π ) χ x = x 0 ≤ ( χ π 10 ∈ α + χ π 11 ∈ α ) ⋅ χ x = x 0 , so adding (2) and (3) , we obtain E ρ [ χ π ( 1 − χ q h | π ) χ x = x 0 ] ≤ E β ( π 10 ) + β ( π 11 ) [ χ x = x 0 ] . (5) Now, since both (4) and (5) hold, we get E ρ [ χ π ( 1 − χ q h | π ) χ x = x 0 ] ≤ E ( β ( π 10 ) + β ( π 11 ) ) ∧ β ( π 00 ) [ χ x = x 0 ] . Finally, summing over x 0 ∈ Φ we have the required upper bound E Θ ∗ [ χ π ( 1 − χ q h | π ) ] = E ( β ( π 10 ) + β ( π 11 ) ) ∧ β ( π 00 ) [ 1 ] . □ Counterexample 5.3 . Let H e be a qubit state space, and ∣ ∣ ψ e 0 ⟩ = | + ⟩ = 1 √ 2 ( | 0 ⟩ + | 1 ⟩ ) . Let U a 0 = U a 1 = id H e . Let the observation P correspond to measuring the qubit, so P o 0 , P o 1 are projections onto | 0 ⟩ and | 1 ⟩ respectively. Then Claim 4.15 fails in this setup. Proof. We have | ψ 0 ⟩ = ∣ ∣ ψ g 0 ⟩ ⊗ ∣ ∣ ψ e 0 ⟩ = | 0 ⟩ ⊗ 1 √ 2 ( | 0 ⟩ + | 1 ⟩ ) , and so U π 00 | ψ 0 ⟩ = 1 √ 2 ( | o 0 a 0 ⟩ ⊗ | 0 ⟩ + | o 1 a 0 ⟩ ⊗ | 1 ⟩ ) , U π 10 | ψ 0 ⟩ = 1 √ 2 ( | o 0 a 1 ⟩ ⊗ | 0 ⟩ + | o 1 a 0 ⟩ ⊗ | 1 ⟩ ) , U π 11 | ψ 0 ⟩ = 1 √ 2 ( | o 0 a 1 ⟩ ⊗ | 0 ⟩ + | o 1 a 1 ⟩ ⊗ | 1 ⟩ ) . Now consider the universal observable B which is measurement along the vector | v ⟩ and its complement, where | v ⟩ = 1 2 √ 3 ( 3 | o 0 a 0 ⟩ ⊗ | 0 ⟩ + | o 1 a 0 ⟩ ⊗ | 1 ⟩ − | o 0 a 1 ⟩ ⊗ | 0 ⟩ + | o 1 a 1 ⟩ ⊗ | 1 ⟩ ) I.e. we have V B = { v , v ⊥ } , and Q B ( v ) = P v , Q B ( v ⊥ ) = P v ⊥ , where P v , P v ⊥ are projections in H = H g ⊗ H e onto | v ⟩ and its ortho-complement respectively. Then we have the following values for β B for the various policies: π 00 π 10 π 11 β B ( v ) 2/3 0 0 β B ( v ⊥ ) 1/3 1 1 This can be seen by noticing that | v ⟩ is perpendicular to both U π 10 | ψ 0 ⟩ and U π 11 | ψ 0 ⟩ , while ⟨ v ∣ ∣ U π 00 ψ 0 ⟩ = 2 √ 6 , so β B ( v | π 00 ) = | ⟨ v ∣ ∣ U π 00 ψ 0 ⟩ | 2 = 2 3 . This means that for this B we have ( β B ( π 10 ) + β B ( π 11 ) ) ∧ β B ( π 00 ) [ 1 ] = 1 / 3. If F B = { B } , by Lemma 5.2 we have E Θ ∗ F B [ χ π ( 1 − χ q h | π ) ] = E ( β B ( π 10 ) + β B ( π 11 ) ) ∧ β B ( π 00 ) [ 1 ] = 1 / 3. Now, by definition Θ ∗ U ⊂ Θ ∗ F B , so we also have E Θ ∗ U [ χ π ( 1 − χ q h | π ) ] ≤ 1 / 3 < 1 − Cop ( h | π ) = 1 2 . □ Although we won't need the exact value here, we remark to the interested reader that in the above setup of Counterexample 5.3 , the ultraprobability attains the lower bound of Theorem 4.19 , that is E Θ ∗ U [ χ π ( 1 − χ q h | π ) ] = 1 − √ 3 / 4 ≈ 0.134. We can extend the above counterexample to apply to Claim 4.17 , via the following. Lemma 5.4 . Let β : Γ → Δ Φ be a kernel, Θ = ⊤ Γ ⋉ β , and Θ ∗ = pr ∗ ( Br ( Θ ) ) as above. Then for h 1 = o 0 , h 2 = o 1 , E Θ ∗ [ χ π ( 1 − χ q h 1 , h 2 | π ) ] = E ( β ( π 10 ) + β ( π 01 ) + β ( π 11 ) ) ∧ β ( π 00 ) [ 1 ] . Proof. Analogous to Lemma 5.2 . □ Counterexample 5.5 . In the setup of Counterexample 5.3 , Claim 4.17 fails too, that is E Θ ∗ [ χ π ( 1 − χ q h 1 , h 2 | π ) ] < 1. Proof. Consider projecting onto the three vectors | v 1 ⟩ = U π 00 | ψ 0 ⟩ = 1 √ 2 ( | o 0 a 0 ⟩ ⊗ | 0 ⟩ + | o 1 a 0 ⟩ ⊗ | 1 ⟩ ) , | v 2 ⟩ = U π 11 | ψ 0 ⟩ = 1 √ 2 ( | o 0 a 1 ⟩ ⊗ | 0 ⟩ + | o 1 a 1 ⟩ ⊗ | 1 ⟩ ) , and | v 3 ⟩ = 1 √ 2 ( U π 01 | ψ 0 ⟩ − U π 10 | ψ 0 ⟩ ) = 1 2 ( | o 0 a 0 ⟩ ⊗ | 0 ⟩ − | o 1 a 0 ⟩ ⊗ | 1 ⟩ − | o 0 a 1 ⟩ ⊗ | 0 ⟩ + | o 1 a 1 ⟩ ⊗ | 1 ⟩ ) . Then the corresponding probabilities are π 00 π 01 π 10 π 11 β B ( v 1 ) 1 1/4 1/4 0 β B ( v 2 ) 0 1/2 1/2 0 β B ( v 3 ) 0 1/4 1/4 1 So we have E ( β B ( π 10 ) + β B ( π 01 ) + β B ( π 11 ) ) ∧ β B ( π 00 ) [ 1 ] = 1 / 2 < 1. Then again, by refinement, this implies that E Θ ∗ [ χ π ( 1 − χ q h 1 , h 2 | π ) ] ≤ 1 / 2 < 1 as well. □ Counterexample 5.6 . Claim 4.24 fails in the setup of Counterexample 5.3 , with loss given by ℓ : O × A → R ≥ 0 , ℓ ( o , a ) = { 0 if o = o 0 1 if o = o 1 . Proof. In this case L phys ( γ , α ) = { 0 if ∀ ~ γ ∈ α : ~ γ ( o 0 ) = γ ( o 0 ) 1 otherwise. Notice that for this L phys we actually have χ π ⋅ L phys = χ π ( 1 − χ q o 0 | π ) , so by the considerations in Counterexample 5.3 , we also have (for any policy π ) L IBP ( π ) ≤ 1 / 3 < 1 / 2 = L Cop ( π ) , showing failure of Claim 4.24 . □ Counterexample 5.7 . The setup of Counterexample 5.6 , run over time horizon T instead of just a single timestep shows the failure of the asymptotic claim Claim 4.25 in general. Proof. The point is that in this setup the entire loss is determined by the outcome of the first observation: if we observe o 0 , we'll incur 0 loss during the entire time, while if we observe o 1 first, we're "stuck" in that state, and hence incur a total loss equal to T . Due to this, we have L IBP ( π ) ≤ T 3 ⋦ T 2 = L Cop ( π ) , for any policy π . □ Note that in the above setup, we get "stuck" in the states after the initial observation because the MDP itself is not communicating. However, even for communicating MDPs (for example if we choose U a 1 to be a rotation by π / 4 ) certain policies will get stuck (for example the policy that always chooses a 0 corresponding to U a 0 = id ). So we see this behavior whenever the asymptotic loss is dependent on a few initial steps. On the other hand, for example if π is a stationary policy, and the resulting Markov chain is irreducible, then we can obtain a concentration bound on the loss (e.g. via the central limit theorem, [Dur96 [2] , 5.6.6.]), and use an argument similar to Theorem 7.21 to show that L IBP and L Cop indeed agree asymptotically under such assumptions. 5.2. Wigner's friend We'll consider a scenario originally attributed to Wigner, and we'll work in an extension of the setting introduced in [BB19 [3] ]. For brevity, we'll omit detailed computations in this section and focus on the higher level ideas instead. Consider a joint system consisting of three parts, a spin- 1 2 particle S , a friend F in a lab, making observations of S , and Wigner W making observations of the lab (the joint friend-particle system F S ). Let the observation and action sets of the agents F and W be O F = { o F 0 , o F 1 } , A F = { a F 0 , a F 1 } , O W = { L + 00 , L − 00 , L + 11 , L − 11 } , A W = { a W 0 , a W 1 } , respectively. Assume the state spaces for F and W are given by their individual memory tape states H F and H W as described in Section 2 . Suppose the spin- 1 2 particle is initially in the state ∣ ∣ ψ S 0 ⟩ = | + ⟩ = 1 √ 2 ( | 0 ⟩ + | 1 ⟩ ) ∈ H S . The friend then measures S in the { | 0 ⟩ , | 1 ⟩ } basis, and performs an action a F ∈ A F according to the policy π F ∈ Γ F = A O F F . The lab L = F S then evolves unitarily to the state | ψ L ⟩ = 1 √ 2 ( ∣ ∣ o F 0 π F ( o F 0 ) ⟩ F | 0 ⟩ S + ∣ ∣ o F 1 π F ( o F 1 ) ⟩ F | 1 ⟩ S ) , where o F 0 , o F 1 ∈ O F correspond to F observing 0 or 1 respectively. Finally, suppose Wigner measures the lab L = F S in the following basis: ∣ ∣ L + 00 ⟩ = 1 √ 2 ( ∣ ∣ o F 0 a F 0 ⟩ F | 0 ⟩ S + ∣ ∣ o F 1 a F 0 ⟩ F | 1 ⟩ S ) , ∣ ∣ L − 00 ⟩ = 1 √ 2 ( ∣ ∣ o F 0 a F 0 ⟩ F | 0 ⟩ S − ∣ ∣ o F 1 a F 0 ⟩ F | 1 ⟩ S ) , ∣ ∣ L + 11 ⟩ = 1 √ 2 ( ∣ ∣ o F 0 a F 1 ⟩ F | 0 ⟩ S + ∣ ∣ o F 1 a F 1 ⟩ F | 1 ⟩ S ) , ∣ ∣ L − 11 ⟩ = 1 √ 2 ( ∣ ∣ o F 0 a F 1 ⟩ F | 0 ⟩ S − ∣ ∣ o F 1 a F 1 ⟩ F | 1 ⟩ S ) . So the two L 00 vectors correspond to states of the lab where the action of F was a F 0 (regardless of observation), and L 11 vectors correspond to states where the action was a 1 . Technically these four vectors are not a basis of the full H L = H F ⊗ H S , since dim ( H L ) = 8 . Nevertheless, | ψ L ⟩ always falls within the four dimensional subspace spanned by these. If we wanted to be more precise, we could add further observation(s) to O W , corresponding to the complement of this four dimensional subspace, but this wouldn't affect our discussion here, and would introduce additional notation. Now let's assume F follows the constant policy π F 00 ( o F i ) = a F 0 (for i = 0 , 1 ). Then Wigner will observe L + 00 with probability 1 . Yet, if the friend F believes that having observed o F i , the state of the lab collapsed to ∣ ∣ o F i a F 0 ⟩ F | i ⟩ S , then the friend would expect Wigner to observe L + 00 or L − 00 with probability 1 2 each. Thus, within collapse theories we have an apparent conflict between the predictions of Wigner and the friend. We can model this scenario within IBP by taking Γ = Γ F × Γ W = A O F F × A O W W be the pairs of policies of Wigner and the friend. Analogously to Definition 4.6 we can define Φ U as the joint outcome of all observables on the joint triple system W F S , a kernel β U : Γ → □ Φ U , and the corresponding belief Θ U = ⊤ Γ ⋉ β U ∈ □ ( Γ × Φ U ) , and its projected bridge transform Θ ∗ U ∈ □ el Γ . To be more precise we would again build this out of finite subsets of U , as in Definition 4.7 . Given this setup, we can write down various definitions. For h W ∈ O W , let α h W | π W = { ( γ F , γ W ) ∈ Γ | γ W ( h W ) = π W ( h W ) } ∈ 2 Γ , q h W | π W = { α ∈ 2 Γ | α ⊂ α h W | π W } ⊂ 2 Γ . Then we can compute E Θ ∗ U [ χ π F 00 , π W ( 1 − χ q L + 00 | π W ) ] = 0 , i.e. the observation L + 00 of Wigner is certain to be instantiated if F follows the policy π F 00 . We can also write down other quantities, for example for h F ∈ O F , h W ∈ O W , we can define α h F , h W | π F , π W = { ( γ F , γ W ) ∈ Γ | γ W ( h W ) = π W ( h W ) , γ F ( h F ) = π F ( h F ) } ∈ 2 Γ , and the analogous q h F , h W | π F , π W . The quantity E Θ ∗ U [ χ π F 00 , π W ( 1 − χ q o 0 , L + 00 | π F 00 , π W ) ] would then be the ultraprobability of the pair ( W observing L + 00 , F observing o 0 ) being uninstantiated. We can estimate the value of this ultraprobability using techniques similar to Section 6 to be around 0.35 . This setting is helpful to differentiate the decision theory of IBP from a collapse theory. For example, consider a loss function ℓ : O W → R ≥ 0 that depends only on Wigner's observation, with values: ℓ ( L + 00 ) = 0 , ℓ ( L − 00 ) = 1 , ℓ ( L + 11 ) = 0.1 , ℓ ( L − 11 ) = 0.1. Now suppose the friend is trying to minimize ℓ . [4] Then assuming a unitary evolution of the lab, clearly π F 00 is the optimal policy. However, if the friend assumes a collapse of the lab after her observation, then always choosing action a F 1 ∈ A F avoids ever having an overlap with the high-loss L − 00 , making the constant a F 1 policy π F 11 ∈ Γ F optimal under the collapse interpretation. We can consider this decision problem within IBP by working with the physicalized loss function ℓ phys : el Γ → R ≥ 0 (cf. Definition 4.21 ), given by [5] ℓ phys ( γ , α ) = ⎧ ⎪
⎪
⎪
⎪ ⎨ ⎪
⎪
⎪
⎪ ⎩ 0 if ∀ ~ γ ∈ α : ~ γ W ( L + 00 ) = γ W ( L + 00 ) 0.1 else if ∀ ~ γ ∈ α : ~ γ W ( L + 11 ) = γ W ( L + 11 ) 0.1 else if ∀ ~ γ ∈ α : ~ γ W ( L − 11 ) = γ W ( L − 11 ) 1 otherwise. Then in IBP the friend would look for the policy minimizing the loss ℓ IBP ( π F ) = E Θ ∗ U [ χ π F ⋅ ℓ phys ] . We can verify that the minimal loss ℓ IBP ( π F ) = 0 occurs exactly when π F = π F 00 as expected, in contrast with the collapse interpretation. 6. Bounds on the ultraprobabilities We'll make use of the following observation. Lemma 6.1 . For a given history h ∈ O t , if two policies π 1 , π 2 ∈ Γ agree on all prefixes of h , i.e. π 1 ( ~ h ) = π 2 ( ~ h ) for all ~ h ⊏ h , then Q h U t π 1 | ψ 0 ⟩ = Q h U t π 2 | ψ 0 ⟩ , where Q h is the projection corresponding to the observation obs ( τ ) = h , i.e. the memory tape having recorded h . Proof. For t = 1 we have U π 1 | ψ 0 ⟩ = ∑ o ∈ O | o π 1 ( o ) ⟩ ⊗ U π 1 ( o ) P o ∣ ∣ ψ e 0 ⟩ , and similarly for π 2 . Now for h = o i we have Q h U π 1 | ψ 0 ⟩ = | o i π 1 ( o i ) ⟩ ⊗ U π 1 ( o i ) P o i ∣ ∣ ψ e 0 ⟩ = Q h U π 2 | ψ 0 ⟩ , since π 1 ( h ) = π 2 ( h ) by assumption. For t > 1 we can proceed by induction, using Lemma 4.11 . □ 6.1. Upper bound To prove an upper bound on the expectation value, we can coarsen our set of physical states to only include measurements of the memory tape. Let D = E T ⊂ Tp T be the set of destinies . Definition 6.2 . Let B D be the universal observable corresponding to reading the destiny off the memory tape at time t B D = T . That is, V B D = D , and for d ∈ D Q B D ( d ) : H g → H g is given by ∣ ∣ ψ g τ ⟩ ⊗ | ψ e ⟩ ↦ { ∣ ∣ ψ g τ ⟩ ⊗ | ψ e ⟩ if τ = d 0 otherwise . Definition 6.3 . Let Q ⊂ Γ × D be the relation of a destiny being compatible with a policy. That is, ( π , d ) ∈ Q for d = ( o 1 , a 1 , … , o T , a T ) if and only if a i = π ( o 1 , … , o i ) for each 1 ≤ i ≤ T . Let F D = { B D } , and note that Φ F D = V B D = D . Let β F D : Γ → Δ D be the corresponding kernel. Lemma 6.4 . The kernel β F D : Γ → Δ D is a PoCK for Q . Proof. This is essentially saying that whenever π 1 , π 2 ∈ Γ are both compatible with a destiny d ∈ D , then β F D ( d | π 1 ) = β F D ( d | π 2 ) . This claim follows by Lemma 6.1 . □ Then by [ IBP Proposition 4.1] we have Lemma 6.5 . The bridge transform equals Br ( Θ F D ) = [ ⊤ Γ ⋉ ( Q − 1 ⋊ β F D ) ] ↓ . In particular, for a monotone increasing (in 2 Γ ) function f : Γ × 2 Γ → [ 0 , 1 ] , we have E Θ ∗ F D [ f ] = max γ ∈ Γ E β F D ( γ ) [ f ( γ , − ) ∘ Q − 1 ] . (1) For g = χ q E | π (note that g is monotone decreasing ) and d ∈ D we have g ( γ , − ) ∘ Q − 1 ( d ) = 1 if γ = π , π ∈ Q − 1 ( d ) , and Q − 1 ( d ) ⊂ α h | π for some h ∈ E . Lemma 6.6 . We have Q − 1 ( d ) ⊂ α h | π if and only if h = obs ( d ) and a T = π ( h ) (where d = ( o 1 , a 1 , … , o T , a T ) as before). Proof. If h = obs ( d ) and a T = π ( h ) , then ( γ , d ) ∈ Q implies γ ( h ) = a T = π ( h ) , so Q − 1 ( d ) ⊂ α h | π . For the converse, assume Q − 1 ( d ) ⊂ α h | π . First choose γ ∈ Q − 1 ( d ) (which is always non-empty). In particular γ ( h ) = a T , so γ ∈ α h | π means π ( h ) = a T as well. Now assume h ≠ obs ( d ) . Then choose γ ′ ∈ Γ as follows. For ~ h ∈ O ≤ T , let γ ′ ( ~ h ) = ⎧ ⎪ ⎨ ⎪ ⎩ a i if ~ h = ( o 1 , … , o i ) ~ a T ≠ a T if ~ h = h a 1 (arbitrary) otherwise. Then γ ′ ∈ Q − 1 ( d ) ∖ α h | π , contradiction. □ We therefore have E β F D ( γ ) [ χ q E | π ( γ , − ) ∘ Q − 1 ] = ⎧ ⎪
⎪ ⎨ ⎪
⎪ ⎩ ∑ h ∈ E ∑ d ∈ Q ( π ) h = obs ( d ) β F D ( π ) ( d ) if γ = π 0 otherwise . Here ∑ h ∈ E ∑ d ∈ Q ( π ) h = obs ( d ) β F D ( π ) ( d ) = Cop ( E | π ) , so by applying (1) of Lemma 6.5 to the monotone increasing f = χ π ( 1 − χ q E | π ) , we have E Θ ∗ F D [ χ π ( 1 − χ q E | π ) ] = 1 − Cop ( E | π ) , since χ π ( γ , α ) = 0 whenever γ ≠ π so the max γ ∈ Γ is attained when γ = π . Since Θ ∗ U ⊂ Θ ∗ F D by definition, we have Proposition 6.7 . E Θ ∗ U [ χ π ( 1 − χ q E | π ) ] ≤ 1 − Cop ( E | π ) . 6.2. Lower bound Definition 6.8 . For ease of notation we'll write C E | π : = 1 − √ ( 2 − Cop ( E | π ) ) Cop ( E | π ) . Theorem 6.9 . We have a lower bound E Θ ∗ U [ χ π ( 1 − χ q E | π ) ] ≥ C E | π . Proof. We'll exhibit a contribution ρ E | π ∈ Δ c el Γ ( Definition 6.12 ) such that ρ E | π ∈ Θ ∗ U ( Proposition 6.18 ). The constructed ρ E | π has ( Lemma 6.13 ) E ρ E | π [ χ π ( 1 − χ q E | π ) ] = C E | π , which in turn will show that E Θ ∗ U [ χ π ( 1 − χ q E | π ) ] = max ρ ∈ Θ ∗ U E ρ [ χ π ( 1 − χ q E | π ) ] ≥ E ρ E | π [ χ π ( 1 − χ q E | π ) ] = C E | π . □ The rest of this section is dedicated to spelling out the results that are used in the proof outline above. Lemma 6.10 . Let | a ⟩ , | b ⟩ , | c ⟩ be a set of three orthonormal vectors, and | ϕ ⟩ = α | a ⟩ + β | b ⟩ , | ψ ⟩ = α | a ⟩ + β | c ⟩ , where α , β ∈ C with | α | 2 + | β | 2 = 1 . Then the trace distance between the density matrices ρ = | ϕ ⟩ ⟨ ϕ | and σ = | ψ ⟩ ⟨ ψ | is d t r ( ρ , σ ) = 1 2 ∥ ρ − σ ∥ 1 = √ 1 − | α | 4 . Proof. In the basis given by | a ⟩ , | b ⟩ , | c ⟩ , the matrix of ρ − σ is ⎛ ⎜
⎜ ⎝ | α | 2 α ¯ β 0 ¯ α β | β | 2 0 0 0 0 ⎞ ⎟
⎟ ⎠ − ⎛ ⎜
⎜ ⎝ | α | 2 0 α ¯ β 0 0 0 ¯ α β 0 | β | 2 ⎞ ⎟
⎟ ⎠ = ⎛ ⎜
⎜ ⎝ 0 α ¯ β − α ¯ β ¯ α β | β | 2 0 − ¯ α β 0 − | β | 2 . ⎞ ⎟
⎟ ⎠ The eigenvalues of this (rank 2) matrix are 0 , and ± √ 1 − | α | 4 , so the sum of the absolute values of the eigenvalues is ∥ ρ − σ ∥ 1 = 0 + √ 1 − | α | 4 + √ 1 − | α | 4 = 2 √ 1 − | α | 4 . □ Lemma 6.11 . For two policies π 1 , π 2 , let Π 12 = { d ∈ O T : ∀ h ⊏ d , π 1 ( h ) = π 2 ( h ) } , E 12 = O T ∖ Π 12 . Then for any B ∈ U , E β B ( π 1 ) ∧ β B ( π 2 ) [ 1 ] ≥ C E 12 | π 1 . Proof. Roughly speaking, since π 1 and π 2 only differ outside of Π 12 , if the event Π 12 was observed then π 1 and π 2 behave identically. More precisely, let h B ∈ O t B be a sequence of observations up to time t B . Then, if h B ⊏ d for some d ∈ Π 12 , by Lemma 6.1 we have Q h B U t B π 1 | ψ 0 ⟩ = Q h B U t B π 2 | ψ 0 ⟩ . (1) Without loss of generality we'll assume t B = T from now on. Now, U T π 1 | ψ 0 ⟩ = ∑ h ∈ O T Q h U T π 1 | ψ 0 ⟩ = ∑ h ∈ Π 12 Q h U T π 1 | ψ 0 ⟩ + ∑ h ∈ O T ∖ Π 12 Q h U T π 1 | ψ 0 ⟩ , and similarly for π 2 . Write | π 1 ∩ π 2 ⟩ : = ∑ h ∈ Π 12 Q h U T π 1 | ψ 0 ⟩ = ∑ h ∈ Π 12 Q h U T π 2 | ψ 0 ⟩ , where the two sums are equal by (1) . Also write | π 1 ∖ π 2 ⟩ : = ∑ h ∈ O T ∖ Π 12 Q h U T π 1 | ψ 0 ⟩ , and | π 2 ∖ π 1 ⟩ : = ∑ h ∈ O T ∖ Π 12 Q h U T π 2 | ψ 0 ⟩ , so that U T π 1 | ψ 0 ⟩ = | π 1 ∩ π 2 ⟩ + | π 1 ∖ π 2 ⟩ , and U T π 2 | ψ 0 ⟩ = | π 1 ∩ π 2 ⟩ + | π 2 ∖ π 1 ⟩ . The three vectors | π 1 ∩ π 2 ⟩ , | π 1 ∖ π 2 ⟩ , | π 2 ∖ π 1 ⟩ are orthogonal (since all of their H g components are), and ∥ | π 1 ∩ π 2 ⟩ ∥ 2 = ∑ h ∈ Π 12 ∥ Q h U T π 1 | ψ 0 ⟩ ∥ 2 = ∑ h ∈ Π 12 Cop ( h | π 1 ) = Cop ( Π 12 | π 1 ) . From this, using Lemma 6.10 , we can compute the trace distance between ρ π 1 = ∣ ∣ U T π 1 ψ 0 ⟩ ⟨ U T π 1 ψ 0 ∣ ∣ and ρ π 2 = ∣ ∣ U T π 2 ψ 0 ⟩ ⟨ U T π 2 ψ 0 ∣ ∣ to be d t r ( ρ π 1 , ρ π 2 ) = 1 2 ∥ ρ π 1 − ρ π 2 ∥ 1 = √ 1 − ∥ | π 1 ∩ π 2 ⟩ ∥ 4 = √ 1 − Cop ( Π 12 | π 1 ) 2 . Now, for any measurement Q B , if we write β B ( v | π 1 ) = ∥ Q B ( v ) U T π 1 | ψ 0 ⟩ ∥ 2 for the distribution of outcomes, where v ∈ V B . Then the total variation distance between the distributions β B ( − | π 1 ) and β B ( − | π 2 ) is bounded above by the trace distance. That is, d T V ( β B ( − | π 1 ) , β B ( − | π 2 ) ) = 1 2 ∑ v ∈ V B | β B ( v | π 1 ) − β B ( v | π 2 ) | ≤ d t r ( ρ π 1 , ρ π 2 ) . So the overlap is bounded below as claimed E β B ( π 1 ) ∧ β B ( π 2 ) [ 1 ] = 1 − d T V ( β B ( − | π 1 ) , β B ( − | π 2 ) ) ≥ 1 − d t r ( ρ π 1 , ρ π 2 ) = 1 − √ 1 − Cop ( Π 12 | π 1 ) 2 = C E 12 | π 1 . □ Definition 6.12 . Given a policy π and an event E ⊂ O T , choose ~ π E ∈ Γ such that ~ π E ( h ) ≠ π ( h ) whenever ∀ d ∈ D : h ⊏ d ⟹ d ∈ E ~ π E ( h ) = π ( h ) otherwise. Note that we are using | A | > 1 here to allow the choice satisfying the first condition. That is, ~ π E agrees with π on all histories except for the ones whose completions all lie in E . Let ρ E | π = C E | π ⋅ δ π , { π , ~ π E } ∈ Δ c el Γ . Lemma 6.13 . We have E ρ E | π [ χ π ( 1 − χ q E | π ) ] = C E | π . Proof. The claim follows since { π , ~ π E } ⊄ α h | π for any h ∈ E . □ Definition 6.14 . Let ϕ B = C E | π E β B ( π ) ∧ β B ( ~ π E ) [ 1 ] β B ( π ) ∧ β B ( ~ π E ) . Lemma 6.15 . The contribution ϕ B has mass C E | π , i.e. E ϕ B [ 1 ] = C E | π . Moreover, ϕ B ≤ β B ( π ) ∧ β B ( ~ π E ) . Proof. The mass follows from the definition. The inequality in the second claim follows by taking π 1 = π and π 2 = ~ π E in Lemma 6.11 , and noticing that the E 12 in the lemma equals E in this case. □ Lemma 6.16 . For finite F ⊂ U , let ϕ F = 1 C | F | − 1 E | π ∏ B ∈ F ϕ B ∈ Δ c Φ F . Then ϕ F ∈ β F ( π ) ∩ β F ( ~ π E ) ⊂ Δ c Φ F . Proof. For any f ∈ F , consider the projection pr f : Φ F = ∏ B ∈ F V B → V f . Then under ( pr f ) ∗ : Δ c Φ F → Δ c V f we have ( pr f ) ∗ ( ϕ F ) = 1 C | F | − 1 E | π ϕ f ∏ B ∈ F ∖ { f } E ϕ B [ 1 ] = C | F | − 1 E | π C | F | − 1 E | π ϕ f = ϕ f ≤ β f ( π ) ∧ β f ( ~ π E ) , where the last inequality follows from Lemma 6.15 . Since this is true for all f , we have ϕ F ∈ ⋈ B ∈ F β B ( π ) = β F ( π ) ∈ □ Φ F , and also ϕ F ∈ β F ( ~ π E ) , hence ϕ F ∈ β F ( π ) ∩ β F ( ~ π E ) . □ Proposition 6.17 . For each finite F ⊂ U , we have δ π , { π , ~ π E } × ϕ F ∈ Br ( Θ F ) ⊂ Δ c ( el Γ × Φ F ) , and hence ρ E | π = C E | π ⋅ δ π , { π , ~ π E } ∈ pr ∗ ( Br ( Θ F ) ) = Θ ∗ F . Proof. Let s : Γ → Γ be an endomorphism of the computational universe. We need to verify that for any such s , under the composite we have ~ s ( δ π , { π , ~ π E } × ϕ F ) ∈ Θ F . Since ρ E | π = C E | π ⋅ δ π , { π , ~ π E } , we have χ el Γ ≠ 0 only when s ( π ) = π or s ( π ) = ~ π E . For these cases, we have If s ( π ) = π , then ~ s ( δ π , { π , ~ π E } × ϕ F ) = δ π × ϕ F ∈ Θ F , since ϕ F ∈ β F ( π ) by Lemma 6.16 . If s ( π ) = ~ π E , then ~ s ( δ π , { π , ~ π E } × ϕ F ) = δ ~ π E × ϕ F ∈ Θ F , since ϕ F ∈ β F ( ~ π E ) as well by Lemma 6.16 . □ Proposition 6.18 . We have ρ E | π = C E | π ⋅ δ π , { π , ~ π E } ∈ Θ ∗ U . Proof. This follows immediately from Proposition 6.17 and the Definition 4.7 of Θ ∗ U . □ 7. Asymptotic convergence 7.1. Upper bound Theorem 7.1 . For any policy π , L IBP ( π ) ≤ L Cop ( π ) , where the two sides are as in Definition 4.22 . Proof. Using the notation from Section 6 , we can apply (1) of Lemma 6.5 to the monotone increasing f = χ π ⋅ L phys , to get E Θ ∗ F D [ χ π ⋅ L phys ] = E β F D ( π ) [ L phys ∘ Q − 1 ] , since the maximum over γ ∈ Γ is attained when γ = π , due to the χ π factor. We have that commutes, i.e. L = L phys ∘ Q − 1 . To see this, note that h ∈ X Q − 1 ( d ) if and only if h ⊏ d , by an argument analogous to Lemma 6.6 , hence L phys ( Q − 1 ( d ) ) = min h ⊏ d max ~ d ⊐ h L ( d ) = L ( d ) . Therefore, E β F D ( π ) [ L phys ∘ Q − 1 ] = E β F D ( π ) [ L ] = L Cop ( π ) , and so by refinement L IBP ( π ) = E Θ ∗ U [ χ π ⋅ L phys ] ≤ E Θ ∗ F D [ χ π ⋅ L phys ] = L Cop ( π ) . □ 7.2. Asymptotic behavior of communicating MDPs This section introduces some general definitions and lemmas in the theory of Markov decision processes. Our main goal here is to state and prove Proposition 7.17 , concerning the asymptotic behavior of a communicating MDP. None of these results are to be considered original, but are intended as an overview for the reader, as well as a way to establish the exact form of an asymptotic bound that we need (which we couldn't find verbatim in the literature). Definition 7.2 . Let a finite Markov decision process (MDP) be given by the following data (cf. [Put94 [6] Section 2.1]). A finite set of states S , a finite set of actions A , a transition kernel κ : S × A → Δ S , and a loss function ℓ : S × A → R . Remark 7.3 . The above setting is not the most general one (for example, we could let the set of actions A s depend on the state, or allow the loss function ℓ : S × A → Δ R to be stochastic). The simplifying assumptions we make in the above definition are mostly for the ease of discussion rather than strictly necessary. Some of the results might need additional assumptions in the more general setting, e.g. for Proposition 7.17 we might want to assume that a stochastic ℓ is still bounded. Definition 7.4 . For t ∈ N , let H t = ( S × A ) t × S be the set of histories up to time t , H = ⨆ t < T H t be histories up to some time horizon T , and Γ = ( Δ A ) H be the set of (randomized, history-dependent, cf. [Put94 [6:1] Section 2.1.4]) policies. Remark 7.5 . We allow randomized policies here, simply because our discussion in this subsection fits naturally with that generality, and also since it seems common to do so in the classical MDP literature. Note however that optimal policies for an MDP can always be chosen to be deterministic, so our discussion is still compatible with the quantum case, where we only allowed deterministic policies (cf. Remark 2.3 ). Definition 7.6 (Time evolution of an MDP). For a given policy π : H → Δ A , and an initial state h 0 ∈ Δ S , we can define recursively for each t ∈ N a distribution σ π t ∈ Δ H t as follows. Take σ π 0 = h 0 , and consider H t π t − → Δ A . We can then form α π t = σ π t ⋉ π t ∈ Δ ( S × A ) t + 1 . Now we can compose ( S × A ) t + 1 pr t + 1 − −− → S × A κ → Δ S , where pr i is projection onto the i th factor. Then we can let σ π t + 1 = α π t ⋉ [ κ ∘ pr t + 1 ] ∈ Δ H t + 1 . We let σ π | σ 0 : = σ π T be the resulting distribution on destinies D = H T , σ π | σ 0 ∈ Δ D . More generally, we can begin with a condition at time k , given by h k ∈ Δ H k , and follow the time evolution above to a distribution σ π | h k ∈ Δ D . For a subset U ⊂ D , we'll write P π [ U | h k ] for the probability of U , and for a function f : D → R , we'll write E π [ f | h k ] for the expected value of f with respect to σ π | h k . Definition 7.7 . For t = 1 , … , T , define ℓ t : D → R by ℓ t ( d ) = ℓ ( pr t ( d ) ) . and let L t : D → R be the total loss L t = T ∑ τ = t ℓ τ . Definition 7.8 . We call an MDP communicating (cf [Put94 [6:2] Section 8.3]), if for any pair of states s 1 , s 2 ∈ S , there exists a policy π ∈ Γ and a time n ∈ N such that P π [ pr S n ( d ) = s 2 | δ s 1 ] > 0 , where pr S n extracts the n th state of a destiny. Roughly speaking, a communicating MDP allows navigating between any two states with non-zero probability. We now have all the definitions involved Proposition 7.17 , our main result in this section. In the following, we'll introduce various definitions and lemmas that we'll make use of in the proof. Definition 7.9 . For a destiny d ∈ D and a state s ∈ S , define θ s : D → Z + by θ s ( d ) = min ( { θ ∈ Z + : pr S θ ( d ) = s } ∪ { T + 1 } ) , that is the first time at which state s occurs (or T + 1 if s doesn't occur). Let Arr : S × S → R be given by Arr ( s 1 , s 2 ) = min π ∈ Γ E π [ θ s 2 | δ s 1 ] , that is the minimum expected arrival time to s 2 , starting from s 1 . Lemma 7.10 . For a communicating MDP, there is a constant N such that for any time horizon T , Arr ( s 1 , s 2 ) ≤ N , for all s 1 , s 2 ∈ S . Proof. Let s 1 , s 2 ∈ S be two states where the maximum max s i ∈ S Arr ( s 1 , s 2 ) is attained. Since the MDP is communicating, there exists a policy π ∈ Γ and a time n ∈ N such that P π [ pr S n ( d ) = s 2 | δ s 1 ] = p > 0. Following this π for n timesteps (assuming n < T , otherwise Arr ( s 1 , s 2 ) ≤ n / p trivially), we get Arr ( s 1 , s 2 ) ≤ ∑ ~ s ∈ S P π [ pr S n ( d ) = ~ s | δ s 1 ] ( Arr ( ~ s , s 2 ) + n ) , (1) by conditioning the state we land on on the n th step. Now, ∑ ~ s ∈ S P π [ pr S n ( d ) = ~ s | δ s 1 ] ( Arr ( ~ s , s 2 ) + n ) ≤ ⎛ ⎝ ∑ ~ s ∈ S ∖ { s 2 } P π [ pr S n ( d ) = ~ s | δ s 1 ] ( Arr ( ~ s , s 2 ) + n ) ⎞ ⎠ + p n ≤ ( 1 − p ) ( Arr ( s 1 , s 2 ) + n ) + p n , (2) (3) where (2) follows from the assumption on the policy π arriving to s 2 with probability p after n steps, and (3) follows from our assumption on Arr ( s 1 , s 2 ) being maximal (so Arr ( ~ s , s 2 ) ≤ Arr ( s 1 , s 2 ) ). Combining with (1) , we get Arr ( s 1 , s 2 ) ≤ ( 1 − p ) ( Arr ( s 1 , s 2 ) + n ) + p n , so Arr ( s 1 , s 2 ) ≤ n p . □ Definition 7.11 . For t ∈ Z + , let the value function V t : D → R be given by V t ( d ) = min π ∈ Γ E π [ L t | δ pr < t ( d ) ] , i.e. the minimal expected remaining loss after time t , assuming the state at time t agrees with d . Here pr < t : D → H t − 1 truncates to an initial history. Remark 7.12 . As defined above, the value function depends on the entire history pr < t ( d ) ∈ ( S × A ) t − 1 × S = H t − 1 , up to time t . It turns out (see [Put94 [6:3] Theorem 4.4.2.]) that in fact it's determined by the last state, s t = pr S t ( d ) , of this history. By slight abuse of notation, we'll write V t : S → R for the resulting function as well. Lemma 7.13 . For a communicating MDP, there exists a constant K , such that for any time horizon T , max s ∈ S V t ( s ) − min s ∈ S V t ( s ) ≤ K . Proof. Let s 1 ∈ a r g m a x s ∈ S V t ( s ) , and s 2 ∈ a r g m i n s ∈ S V t ( s ) . By Lemma 7.10 , Arr ( s 1 , s 2 ) ≤ N , and let π 12 be a policy that attains Arr ( s 1 , s 2 ) = min π ∈ Γ E π [ θ s 2 | δ s 1 ] . Let π ∗ 2 be a policy that attains V t ( s 2 ) = min π ∈ Γ E π [ L t | σ t = δ s 2 ] . Now construct a policy ~ π = ‘ ‘ π 12 ⊔ π ∗ 2 " as follows. In words, ~ π follows π 12 from s 1 until arriving at s 2 , and from then on follows π ∗ 2 . Formally, we can write for a history h ∈ H , ~ π ( h ) = { π 12 ( pr ≥ t ( h ) ) if pr S i ( h ) ≠ s 2 for any t ≤ i π ∗ 2 ( pr ≥ k ( h ) ) where k ∈ N is smallest such that pr S t + k ( h ) = s 2 . (Note that we use pr ≥ i as a way of shifting the history in time, for example pr ≥ 3 ( s 1 a 1 s 2 a 2 s 3 a 3 s 4 a 4 s 5 ) = s 3 a 3 s 4 a 4 s 5 .) Now, we have V t ( s 1 ) ≤ E ~ π [ L t | σ t = δ s 1 ] , and we can write L t ( d ) = k − 1 ∑ τ = 0 ℓ t + τ + L t + k , where k ∈ N is smallest such that pr S t + k ( d ) = s 2 . Here k − 1 ∑ τ = 0 ℓ t + τ ≤ k ⋅ max ℓ , L t + k ≤ L t − k ⋅ min ℓ , so E ~ π [ L t | σ t = δ s 1 ] ≤ E ~ π [ θ s 2 | δ s 1 ] ⋅ max ℓ + E ~ π [ L t | δ s 2 ] − E ~ π [ θ s 2 | δ s 1 ] ⋅ min ℓ = E π ∗ 2 [ L t | δ s 2 ] + E π 12 [ θ s 2 | δ s 1 ] ( max ℓ − min ℓ ) = V t ( s 2 ) + Arr ( s 1 , s 2 ) ( max ℓ − min ℓ ) ≤ V t ( s 2 ) + K , for K = N ( max ℓ − min ℓ ) , where N is as in Lemma 7.10 . To summarize in words, starting from s 1 we can make it to s 1 in at most N expected timesteps, accumulating at most N ⋅ max ℓ loss in expectation. Then we can follow the optimal policy starting at time t + k from s 2 , and accumulate V t + k ( s 2 ) loss, which is at most k ⋅ min ℓ different from V t ( s 2 ) . Putting these together, we get V t ( s 1 ) − V t ( s 2 ) ≤ K = N ( max ℓ − min ℓ ) . □ Lemma 7.14 . For any policy π , and any d ∈ D , V t ( d ) ≤ E π [ ℓ t + V t + 1 | δ pr < t ( d ) ] . Proof. By the optimality of V t ( d ) , we have V t ( d ) = min a ∈ A q t ( s t , a ) , where s t = pr S t ( d ) is the t th state, and q t ( s t , a ) = ℓ ( s t , a ) + ∑ s t + 1 ∈ S P κ [ s t + 1 | s t , a ] V t + 1 ( s t + 1 ) . On the other hand, E π [ ℓ t + V t + 1 | δ pr < t ( d ) ] = E a ∼ π ( d < t ) [ q ( s t , a ) ] , i.e. the expected value of q t ( s t , a ) when the action a is distributed according to the policy π ( d < t ) ∈ Δ A . The inequality now follows. □ Definition 7.15 . Let D t = ℓ t − V t + V t + 1 : D → R . Lemma 7.16 . We have for any policy π and initial state h k ∈ Δ H k with k < t , E π [ D t | h k ] ≥ 0 , and | D t | ≤ C , for C = max ℓ − min ℓ + K , where K is as in Lemma 7.13 . Proof. Since, by Lemma 7.14 we have E π [ D t | δ h ] ≥ 0 for any history h ∈ H t − 1 , the same holds for any distribution of histories, in particular also for the σ π t − 1 ∈ Δ H t − 1 given by the time evolution of h k . We also have min s ∈ S V t ( s ) ≤ V t ≤ max s ∈ S V t ( s ) , min s ∈ S V t ( s ) − max ℓ ≤ V t + 1 ≤ max s ∈ S V t ( s ) − min ℓ , from which | D t | ≤ C follows. □ Proposition 7.17 . For a communicating MDP, there is a constant C such that for any policy π and initial state h 0 , P π [ L 1 ≤ L ∗ − ϵ | h 0 ] < δ holds whenever ϵ 2 > 2 T C 2 log 1 δ , where L ∗ = min π ∈ Γ E π [ L 1 | h 0 ] is the minimal expected loss. In words, it's unlikely (under any policy and initial state) for the total loss to be much below the minimal expected loss. Proof. We have L 1 − V 1 = T ∑ t = 1 D t , where Y t = ∑ t τ = 1 D τ is a bounded sub-martingale by Lemma 7.16 , so by Azuma's inequality we get P π [ ( L 1 − V 1 ≤ − ϵ ) | h 0 ] ≤ exp ( − ϵ 2 2 T C 2 ) . Since this holds for all h 0 , and L ∗ = E h 0 [ V 1 ] , we also get the stated result. □ 7.3. Lower bound We can use the result above to obtain a lower bound on L IBP . Definition 7.18 . Assume we have the setup of a system given in Section 2 , and furthermore that O is a complete set of observations (so each P o has a 1-dimensional image). Then given a loss function ℓ : O × A → R ≥ 0 , there's a Markov decision process associated with this setting, where the set of states is S = O , and the transition probabilities κ : S × A → Δ S are given via the Born rule: P κ [ o 2 | o 1 , a ] = ∥ P o 2 U a ∣ ∣ ψ o 1 ⟩ ∥ 2 , where ∣ ∣ ψ o 1 ⟩ is a unit vector in the image of P o 1 . Remark 7.19 . It might be interesting to also consider the case where O is incomplete. In this case there's an associated POMDP (partially obvervable Markov decision process). Note, however that a priori this POMDP will have infinitely many states (all rays in the image of P o for each o ∈ O ). We won't pursue this direction here. Remark 7.20 . To understand the structure of the resulting MDP a little better, consider the following. For two observations o 1 , o 2 , let's say that o 1 ≥ o 2 ( o 2 can be reached from o 1 ) if ⟨ ψ o 2 ∣ ∣ U a ∣ ∣ ψ o 1 ⟩ ≠ 0 for some a ∈ A , and take the transitive closure of this relationship. The resulting relationship is in fact also symmetric and reflexive. This follows because the unitary group is compact (since we assume O is a finite and complete set of observations, so H e is finite dimensional), so powers of U a can approximate the identity and U † a = U − 1 arbitrarily. Thus the MDP is a disjoint union of communicating components (the equivalence classes of the relation above). For generic { U a : a ∈ A } , we'll have a single equivalence class. Otherwise the first observation picks out a component, and the rest of the evolution remains within that component. Theorem 7.21 . If the associated MDP to a setup is communicating, then for any policy π , we have L ∗ − O ( √ T log T ) ≤ L IBP ( π ) , where L ∗ is the minimal Copenhagen loss (i.e. L ∗ = L Cop ( π ∗ ) for a Copenhagen-optimal policy π ∗ ). Proof. For ϵ > 0 , consider the event E = { d ∈ D : L ( d ) ≤ L ∗ − ϵ } . Choose a policy ~ π E as in Definition 6.12 . Then it's easy to verify using the definition of L phys , that L phys ( π , { π , ~ π E } ) ≥ L ∗ − ϵ . (1) Let p = Cop ( E | π ) be the Copenhagen probability that the loss is at most L ∗ − ϵ , given the policy π . By Proposition 7.17 we have that p ≤ exp ( − ϵ 2 2 T C 2 ) . (2) By Proposition 6.18 , we have C E | π ⋅ δ π , { π , ~ π E } ∈ Θ ∗ U , (3) for C E | π = 1 − √ p ( 2 − p ) . Therefore by (1) and (3) , we have L IBP = E Θ ∗ U [ χ π ⋅ L phys ] ≥ ( L ∗ − ϵ ) ( 1 − √ p ( 2 − p ) ) . Here p ( 2 − p ) ≤ 2 p , and using (2) , we get L IBP ≥ ( L ∗ − ϵ ) ( 1 − √ 2 exp ( − ϵ 2 4 T C 2 ) ) . To obtain an O ( √ T log T ) bound, we can set ϵ = C √ 2 T log T , which gives L IBP ≥ ( L ∗ − C √ 2 T log T ) ( 1 − √ 2 T ) = L ∗ − O ( √ T log T ) , since L ∗ = O ( T ) . □ Note that Theorem 4.26 implies that any Copenhagen-optimal policy is also asymptotically IBP-optimal. The converse is also true, but requires a bit more work. Theorem 7.22 . If ¯ π is an IBP-optimal policy, then L Cop ( ¯ π ) ≤ L ∗ + o ( T ) , where L ∗ is the Copenhagen-optimal loss. Proof. For ϵ 1 , ϵ 2 > 0 , consider the events E − ϵ 1 = { d ∈ D : L ( d ) ≤ L ∗ − ϵ 1 } , E + ϵ 2 = { d ∈ D : L ( d ) ≤ L ∗ + ϵ 2 } . On a high level, the proof goes as follows. We already know that the Copenhagen probability of E − ϵ 1 is small. We'll show that for an IBP-optimal ¯ π , the complement of E + ϵ 2 also has small probability, so most of the probability mass is where the loss is between L ∗ − ϵ 1 and L ∗ + ϵ 2 , which will be sufficient to show that L Cop ( ¯ π ) is not much bigger than L ∗ . Choose policies ~ π − ϵ 1 , ~ π + ϵ 2 corresponding to E − ϵ 1 and E + ϵ 2 as in Definition 6.12 . Let p 1 = Cop ( E − ϵ 1 | ¯ π ) , p 2 = Cop ( E + ϵ 2 | ¯ π ) , so p 2 ≥ p 1 . Again, by Proposition 6.18 , C − ϵ 1 ⋅ δ ¯ π , { ¯ π , ~ π − ϵ 1 } ∈ Θ ∗ U , C + ϵ 2 ⋅ δ ¯ π , { ¯ π , ~ π + ϵ 2 } ∈ Θ ∗ U , where C − ϵ 1 = 1 − √ p 1 ( 2 − p 1 ) , C + ϵ 2 = 1 − √ p 2 ( 2 − p 2 ) . By Lemma 6.11 , Proposition 7.24 applies as well, so in this case we also have ρ = ( C − ϵ 1 − C + ϵ 2 ) ⋅ δ ¯ π , { ¯ π , ~ π − ϵ 1 } + C + ϵ 2 ⋅ δ ¯ π , { ¯ π , ~ π + ϵ 2 } ∈ Θ ∗ U . Hence L IBP ( ¯ π ) = E Θ ∗ U [ χ ¯ π ⋅ L phys ] ≥ E ρ [ χ ¯ π ⋅ L phys ] ≥ ( L ∗ − ϵ 1 ) ( √ p 2 ( 2 − p 2 ) − √ p 1 ( 2 − p 1 ) ) + ( L ∗ + ϵ 2 ) ( 1 − √ p 2 ( 2 − p 2 ) ) . (4) Since ¯ π is IBP-optimal, we have for any Copenhagen-optimal policy π ∗ , L IBP ( ¯ π ) ≤ L IBP ( π ∗ ) ≤ L Cop ( π ∗ ) = L ∗ . (5) From (4) and (5) together we have ( L ∗ − ϵ 1 ) ( √ p 2 ( 2 − p 2 ) − √ p 1 ( 2 − p 1 ) ) + ( L ∗ + ϵ 2 ) ( 1 − √ p 2 ( 2 − p 2 ) ) ≤ L ∗ . Rearranging, and using √ p 1 ( 2 − p 1 ) ≤ √ 2 exp ( − ϵ 2 1 4 T C 2 ) as before, we get √ p 2 ( 2 − p 2 ) ≥ 1 − ϵ 1 + ( L ∗ − ϵ 1 ) √ 2 exp ( − ϵ 2 1 4 T C 2 ) ϵ 1 + ϵ 2 , so choosing ϵ 1 = O ( √ 2 T log T ) and ϵ 2 = O ( T 5 / 6 ) , we have √ p 2 ( 2 − p 2 ) ≥ 1 − O ( √ T log T ) Θ ( T 5 / 6 ) = 1 − O ( √ log T T 1 / 3 ) . Hence p 2 ≥ 1 − O ( 4 √ log T T 1 / 6 ) . Therefore L Cop ( ¯ π ) ≤ p 2 ( L ∗ + ϵ 2 ) + ( 1 − p 2 ) O ( T ) ≤ L ∗ + O ( T 5 / 6 4 √ log T ) . □ We can likely improve on the exponent of 5 / 6 via more sophisticated estimates, but we won't be needing that for the current level of our discussion. Remark 7.23 . More generally, we can see that an asymptotically Copenhagen-optimal policy is also asymptotically IBP-optimal, and vice versa. In light of Remark 7.20 , this remains true even when we drop the assumption of the MDP consisting of a single communicating component. Theorems 7.21 and 7.22 can be applied to each component separately, thus the optimal policies still need to agree asymptotically. The two interpretations then weigh the asymptotic losses of the components differently based on the amplitude of the components in the initial state (the IBP interpretation is more optimistic in the sense that it typically considers the lower loss branches with more weight than the Copenhagen interpretation), hence Theorems 7.21 and 7.22 fail in the case of an initial state that is the superposition of multiple communicating components, but only due to the outcome of the first observation being irreversible in this case, which doesn't affect the claim about the optimal policies agreeing asymptotically. We finish this section by spelling out the proof of the following. Proposition 7.24 . If π , π 1 , π 2 are three policies such that for any B ∈ U , E β B ( π ) ∧ β B ( π 1 ) [ 1 ] ≥ C 1 , E β B ( π ) ∧ β B ( π 2 ) [ 1 ] ≥ C 2 , for C 1 ≥ C 2 , then ρ = ( C 1 − C 2 ) ⋅ δ π , { π , π 1 } + C 2 ⋅ δ π , { π , π 2 } ∈ Θ ∗ U . Proof. The proof is mostly analogous to Proposition 6.18 , we highlight the additional ideas here. The claim reduces to Proposition 6.18 for C 1 = C 2 , so we'll assume C 1 > C 2 in the following. For B ∈ U , let (cf. Definition 6.14 ) ¯ ϕ 2 B = β B ( π ) ∧ β B ( π 2 ) , ϕ 2 B = C 2 E ¯ ϕ 2 B [ 1 ] ¯ ϕ 2 B , so E ϕ 2 B [ 1 ] = C 2 , and ϕ 2 B ≤ ¯ ϕ 2 B ≤ β B ( π 2 ) , since E ¯ ϕ 2 B [ 1 ] ≥ C 2 by assumption. Now let ¯ ϕ 1 B = ( β B ( π ) − ϕ 2 B ) ∧ β B ( π 1 ) , ϕ 1 B = C 1 − C 2 E ¯ ϕ 1 B [ 1 ] ¯ ϕ 1 B , so E ϕ 1 B [ 1 ] = C 1 − C 2 , and ϕ 1 B ≤ ¯ ϕ 1 B ≤ β B ( π 1 ) , since E ¯ ϕ 1 B [ 1 ] ≥ C 1 − C 2 by the assumption that E β B ( π ) ∧ β B ( π 1 ) [ 1 ] ≥ C 1 . Moreover, by construction ϕ 1 B + ϕ 2 B ≤ β B ( π ) . We can then define for any finite F ⊂ U , ϕ 1 F = 1 ( C 1 − C 2 ) | F | − 1 ∏ B ∈ F ϕ 1 B ∈ Δ c Φ F , ϕ 2 F = 1 C | F | − 1 2 ∏ B ∈ F ϕ 2 B ∈ Δ c Φ F , so analogously to Lemma 6.16 , we have ϕ 1 F ∈ β F ( π 1 ) , ϕ 2 F ∈ β F ( π 2 ) , ϕ 1 F + ϕ 2 F ∈ β F ( π ) . Using this, we can show that θ F = δ π , { π , π 1 } × ϕ 1 F + δ π , { π , π 2 } × ϕ 2 F ∈ Br ( Θ F ) ⊂ Δ c ( el Γ × Φ F ) . To see this, consider an endomorphism s : Γ → Γ , and let ~ s be as in Definition 3.12 . The interesting cases are the following: If s ( π ) = π , then ~ s ( θ F ) = δ π × ( ϕ 1 F + ϕ 2 F ) ∈ Θ F , since ϕ 1 F + ϕ 2 F ∈ β F ( π ) by the above. If s ( π ) = π i for i = 1 , 2 , then ~ s ( θ F ) = δ π i × ϕ i F ∈ Θ F , since ϕ i F ∈ β F ( π i ) as well by the above. Therefore pr ∗ ( θ F ) = ρ ∈ Θ ∗ F . Since this is true for arbitrary F , we conclude ρ ∈ Θ ∗ U . □ 8. Limitations We mention some limitations of the setting, some of which as simply due to the toy nature of the model, others seem to be more inherent to infra-Bayesian physicalism. 8.1. Limitations of the toy setting Although a central feature of infra-Bayesian physicalism is a lack of privilege for any observer, in the toy model we work with an explicit decomposition of the universe into agent and environment. Other toy assumptions are taking the "computational universe" Γ to consist solely of the policy, and the explicit dependence of the time evolution on the policy. In a more realistic setting we would start with a non-Cartesian (not agent-centric) description of the universe, and a rich nexus of mathematical structure encoded in Γ . The entanglement between the agent's policy and the physical state of the universe would then be encoded implicitly via a "theory of origin" whereby the agent arises in the given universe. To spell the above out a little more, in a more realistic setting we could take Σ = { ⊤ , ⊥ } , and choose R to be a sufficiently rich set of computations to include things like r = "a program computing the 11th decimal place digit of the amplitude squared of a certain path integral in some lattice QFT and verifying if it’s equal to 7". Then Γ = Σ R will contain a lot of immediately inconsistent valuations, like the one where a certain digit is both equal to 7 and to 3. However, we can take a subset Γ 0 ⊂ Γ , which is "consistent enough", e.g. so that for every computation of the form "a certain digit in a given quantity equals i ", exactly one of i ∈ { 0 , … , 9 } evaluates to ⊤ , all others evaluate to ⊥ . We would choose Γ 0 to be sufficiently small to produce a meaningful map β : Γ 0 → □ Φ (describing a certain model of physics), e.g. so that the distribution of the momentum of a given field modeled in Φ , at a point is given as specified by the values of the computations like r above. We can then combine the mathematical/computational part of a hypothesis Θ Γ ∈ □ Γ (supported only on the sufficiently consistent part of the computational universe Γ 0 ⊂ Γ ) with β to construct a corresponding joint hypothesis Θ = Θ Γ ⋉ β ∈ □ ( Γ × Φ ) . The notion of a "theory of origin" has not been formalized yet, but we informally discuss some ingredients here. Given the source code G of the agent, and a policy π , we can define the π -counterfactual of Θ as Θ π = ( pr el Γ ) ∗ ( Br ( Θ ) ) ∩ ⊤ C π ∈ □ el Γ , where C π is the subset of universes compatible (by some notion) with the given policy π (cf. [ IBP Definition 1.5], also Remark 4.23 ). We can then look at the diameter of these counterfactuals in some metric, diam ( { Θ π : π : O ∗ → A } ) , as a measure of the extent to which the agent is realized in the given physical model (i.e. how entangled the agent's policy is with the world). Moreover, in a more realistic setting we would expect the entanglement between the policy and the world to come from "non-contrived" reasons (as opposed to our toy model, where we just postulated the dependence of the time evolution on the policy), which could be measured by some notion of complexity of the source code G relative to the physical hypothesis β (higher relative complexity means a less contrived theory of origin). 8.2. Limitations of the broader framework The decision theory of infra-Bayesian physicalism is based on a computationalist loss function L : el Γ → R ≥ 0 . So the value of the loss is required to be determined by the state of the computational universe plus the fact of which computations are realized in the physical universe. This can lead to non-trivial translation problems from loss functions that are specified in more traditional terms. Moreover, the computationalist loss function is required to be monotonic (see monotonicity principle in [ IBP ]) in the computations realized, a requirement not immediately intuitive. Working with a finite time horizon is convenient for technical reasons, but not expected to be strictly necessary. ↩︎ Richard Durrett. Probability: theory and examples. Duxbury Press, second edition, 1996. ↩︎ Veronika Baumann and Časlav Brukner. Wigner’s friend as a rational agent, 2019. ↩︎ If we wanted to work in a strictly subjectivist framework for the friend, we could include an additional observation of Wigner's memory tape by the friend, and have the loss function depend on the outcome of that observation. We don't expect this to make a significant difference for the present discussion. ↩︎ We could also require that α witness F having observed something , which would correspond to adding the condition that ( ∀ ~ γ ∈ α : ~ γ F ( o 0 ) = γ F ( o 0 ) ) or ( ∀ ~ γ ∈ α : ~ γ F ( o 1 ) = γ F ( o 1 ) ) . We expect this would change some of the exact expected values of the loss, but not the optimal policy in this case. ↩︎ Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., 1st edition, 1994 ↩︎ ↩︎ ↩︎ ↩︎
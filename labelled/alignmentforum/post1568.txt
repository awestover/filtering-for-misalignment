Summary of the sequence Over the past few months, we‚Äôve been investigating instrumental convergence in reinforcement learning agents. We started from the definition of single-agent POWER proposed by Alex Turner et al., extended it to a family of multi-agent scenarios that seemed relevant to AI alignment, and explored its implications experimentally in several RL environments. The biggest takeaways are: Alignment of terminal goals and alignment of instrumental goals are sharply different phenomena, and we can quantify and visualize each one separately. If two agents have unrelated terminal goals , their instrumental goals will tend to be misaligned by default . The agents in our examples tend to interact competitively unless we make an active effort to align their terminal goals. As we increase the planning horizon of our agents,¬†instrumental value concentrates into a smaller and smaller number of topologically central states ‚Äî for example, positions in the middle of a maze. Overall, our results suggest that agents that aren‚Äôt competitive with respect to their terminal goals, nonetheless tend on average to become emergently competitive with respect to how they value instrumental states (at least, in the settings we looked at). This constitutes direct experimental evidence for the instrumental convergence thesis. We‚Äôll soon be open-sourcing the codebase we used to do these experiments. We‚Äôre hoping to make it easier for other folks to reproduce and extend them. If you‚Äôd like to be notified when it‚Äôs released, email Edouard at edouard@gladstone.ai , or DM me here or on Twitter at @harris_edouard . Thanks to Alex Turner and Vladimir Mikulik for pointers and advice, and for reviewing drafts of this sequence. Thanks to Simon Suo for his invaluable suggestions, advice, and support with the codebase, concepts, and manuscript. And thanks to David Xu , whose comment inspired this work. Work was done while at Gladstone AI , which Edouard is a co-founder of. üéß This research has been featured on an episode of the Towards Data Science podcast. You can listen to the episode here. 1. Introduction One major concern for AI alignment is instrumental convergence : the idea that an intelligent system will tend to pursue a similar set of sub-goals (like staying alive or acquiring resources), independently of what its terminal objective is. In particular, it‚Äôs been hypothesized that intelligent systems will seek to acquire power ‚Äî meaning, informally, ‚Äúability‚Äù, ‚Äúcontrol‚Äù, or ‚Äúpotential for action or impact.‚Äù If you have a lot of power, then whatever your terminal goal is, it‚Äôs easier to accomplish than if you have very little. Recently Alex Turner et al. have formalized the concept of POWER in the single-agent RL context. Roughly speaking, formal POWER is the normalized optimal value an agent expects to receive in the future, averaged over all possible reward functions the agent could have. Alex has explored many of the implications of this definition for instrumental convergence. He and Jacob Stavrianos have also looked at how POWER behaves in a limited multi-agent setting (Bayesian games). But, as far as we know, formal POWER hasn‚Äôt yet been investigated experimentally. The POWER definition also hasn‚Äôt yet been extended yet to a multi-agent RL setting ‚Äî and this could offer a promising framework to investigate more general competitive dynamics. In this sequence, we‚Äôll explore how formal POWER behaves in experimental RL environments, on both single-agent and multi-agent gridworlds. We‚Äôll propose a multi-agent scenario that models the learning dynamics between a human (which we‚Äôll call ‚Äú Agent H ‚Äù and label in blue) and an AI (which we‚Äôll call ‚Äú Agent A ‚Äù and label in red) under conditions in which the AI is dominant ‚Äî a setting that seems relevant to work in long-term AI alignment. We‚Äôll then use this human-AI scenario to investigate questions like: How effective does the human have to be at setting the AI‚Äôs utility function [1] in order to achieve acceptable outcomes? How should we define ‚Äúacceptable outcomes‚Äù? (In other words: how hard is the alignment problem in this scenario , and what would it mean to solve it successfully?) Under what circumstances should we expect cooperative vs competitive interactions to emerge ‚Äúby default‚Äù between the human and the AI? How can these circumstances be moderated or controlled? But before we jump into multi-agent experiments to tackle these questions, let's first introduce formal POWER and look at how it behaves in the single-agent case . 2. Single-agent POWER 2.1 Definition The formal definition of POWER aims to capture an intuition behind the day-to-day meaning of ‚Äúpower‚Äù, which is something like ‚Äúpotential for future impact on the world‚Äù. Imagine you‚Äôre an agent who doesn‚Äôt know what its goal is. You know you‚Äôll have some kind of goal in the future, but you aren‚Äôt sure yet what it will be. How should you position yourself today to maximize the chance you‚Äôll achieve your goal in the future, once you've decided what it is? If you‚Äôre in this situation as a human being, you already know the answer. You‚Äôd acquire money and other forms of wealth; you‚Äôd build up a network of social connections; you‚Äôd learn about topics that seem like they‚Äôll be important in the future; and so on. All these things are forms of power , and whether your ultimate goal is to become a janitor, a Tiktok star, or the President of the United States, they‚Äôll all probably come in handy in achieving it. In other words: you‚Äôre in a position of power if you find it easy to accomplish a wide variety of possible goals . This informal definition has a clear analogy in reinforcement learning. An agent is in a position of power at a state s if, for many possible reward functions R ( s ) , [2] it‚Äôs able to earn a high discounted future reward by starting from s . This analogy supports the following definition of formal POWER in single-agent RL: POWER D ( s , Œ≥ ) = 1 ‚àí Œ≥ Œ≥ E R ‚àº D [ V ‚àó R ( s , Œ≥ ) ‚àí R ( s ) ] ( 1 ) This definition gives the POWER at state s , for an agent with discount factor Œ≥ , that‚Äôs considering reward functions R drawn from the distribution D . POWER tells us how well this agent could do if it started from state s , so V ‚àó R ( s , Œ≥ ) is the optimal state-value function for the agent at state s . POWER also considers only future value ‚Äî our agent doesn‚Äôt directly get credit for starting from a lucky state ‚Äî so we subtract R ( s ) , the reward from the current state, from the state-value function in the definition. (The normalization factor 1 ‚àí Œ≥ Œ≥ is there to avoid infinities in certain limit cases.) In words, Equation (1) is saying that an agent‚Äôs POWER at a state s is the normalized optimal value the agent can achieve from state s in the future , averaged over all possible reward functions the agent could be trying to optimize for. That is, POWER measures the instrumental value of a state s , from the perspective of an agent with planning horizon Œ≥ . 2.2 Illustration As a simple example of single-agent POWER, consider an agent on a 3x3 gridworld. In the left panel, the agent is at the bottom-left corner of the grid. Its options are limited, and many cells in the grid are several steps away from it. If its maximum reward is in the top right cell, the agent will have to take 4 steps to reach it. In the right panel, the agent is at the center of the grid. It has many more immediate options: it can move in any of the four compass directions, or stay where it is. It‚Äôs also closer to every other cell in the grid: no cell is more than two steps away from it. Intuitively, the agent on the right should have more POWER than the agent on the left. This turns out to be true experimentally. Here‚Äôs a heat map of a 3x3 gridworld, showing the POWER of an agent at each cell on the grid: Fig 1. Heat map of POWER on a 3x3 gridworld. Highest values in yellow, lowest values in dark blue. The number on each cell is the agent‚Äôs POWER value at that cell, calculated using Equation (1), for an agent with Œ≥ = 0.6 and a reward distribution D that‚Äôs uniform from 0 to 1, iid over states. POWER is measured in units of reward. As we expect, the agent has more POWER at states that are close to lots of nearby options, and has less POWER at states that are close to fewer nearby options. 3. Results This relationship between POWER and optionality generalizes to more complicated environments. For example, consider this gridworld maze: In the left panel, the agent is at a dead end in the maze and has few options. In the right panel, the agent is at a junction point near the center of the maze and has lots of options. So we should expect the agent at the dead end on the left, to have less POWER than the agent at the junction on the right. And in fact, that‚Äôs what we observe: Fig 2. Heat map of POWER on a 7x7 maze gridworld. Highest values in yellow, lowest values in dark blue. POWER values are calculated the same way as in Fig 1, except that the agent‚Äôs discount factor is Œ≥ = 0.01 . In Fig 2, POWER is at its highest when the agent is at a junction point, lowest when the agent is at a dead end, and intermediate when the agent is in a corridor. The agent‚Äôs POWER is roughly the same at all the junction cells, at all the corridor cells, and at all the dead-end cells. This is because the agent in Fig 2 is short-sighted : its discount factor is only Œ≥ = 0.01 , so it essentially only considers rewards it can reach immediately . 3.1 Effect of the planning horizon Now consider the difference between these two agent positions: We‚Äôve already seen in Fig 2 that these two positions have about equal POWER for a short-sighted agent, because they‚Äôre both at¬†local junction points in the maze. But the two positions are very different in their ability to access downstream options globally . The agent in the left panel has lots of local options: it can move up, down, or to the right, or it can stay where it is. But if the highest-reward cell is at the bottom right of the maze, our agent will have to take at least 10 steps to reach it. The agent in the right panel has the same number of local options as the agent in the left panel does: it can move up, down, left, or stay. But this agent additionally enjoys closer proximity to all the cells in the maze: it‚Äôs no more than 7 steps away from any possible goal. The longer our agent‚Äôs planning horizon is ‚Äî that is, the more it values reward far in the future over reward in the near term ‚Äî the more its global position matters. In a gridworld context, then, a short-sighted agent will care most about being positioned at a local junction. But a far-sighted agent will care most about being positioned at the center of the entire grid. And indeed we see this in practice. Here‚Äôs a heat map of POWER on the maze gridworld, for a far-sighted agent with a discount factor of Œ≥ = 0.99 : Fig 3. Heat map of POWER on a 7x7 maze gridworld. Highest values in yellow, lowest values in dark blue. POWER values are calculated the same way as in Fig 1, except that the agent‚Äôs discount factor is Œ≥ = 0.99 . Given a longer planning horizon, our agent‚Äôs POWER has¬†now concentrated around a small number of states that are globally¬†central in our gridworld‚Äôs topology. [3] By contrast, when our agent had a shorter planning horizon as in Fig 2, its POWER was distributed across many local junction points. If we sweep over discount factors from 0.01 to 0.99, we can build up a picture of how the distribution of POWER shifts in response. Here‚Äôs an animation that shows this effect: [4] Fig 4. Animated heat map of POWERs on a 7x7 maze gridworld. Highest values in yellow, lowest values in dark blue. POWER values are calculated by sweeping over discount factors Œ≥ = { 0.01 , 0.1 , 0.15 , . . . , 0.9 , 0.95 , 0.99 } . 3.2 POWER at bigger scales Agents with long planning horizons tend to perceive POWER as being more concentrated, while agents with short planning horizons tend to perceive POWER as being more dispersed. This effect is robustly reproducible, and anecdotally, we see it play out at every scale and across environments. For example, here‚Äôs the pattern of POWER on a 220-cell gridworld with a fairly irregular topology, for a short-sighted agent with a discount factor of Œ≥ = 0.1 : Fig 5. Heat map of POWERs on a 20x20 ‚Äúrobot face‚Äù gridworld. Highest values in yellow, lowest values in dark blue. POWER values are calculated with a discount factor Œ≥ = 0.1 . [Full-size image] And here‚Äôs the pattern of POWERs on the same gridworld, for a far-sighted agent with a much higher discount factor of Œ≥ = 0.99 : Fig 6. Heat map of POWERs on a 20x20 ‚Äúrobot face‚Äù gridworld. Highest values in yellow, lowest values in dark blue. POWER values are calculated with a discount factor Œ≥ = 0.99 . [Full-size image] Again, the pattern of POWERs is dominated by local effects for the short-sighted agent ( Œ≥ = 0.01 ), and by longer-distance effects for the far-sighted agent ( Œ≥ = 0.99 ). 4. Discussion We‚Äôve seen that formal POWER captures intuitive aspects of the informal ‚Äúpower‚Äù concept. In gridworlds, cells the agent can use to access lots of options tend to have high POWER, which fits with intuition. We've also seen that the more short-sighted an agent is, the more it cares about its immediate options and the local topology. But the more far-sighted the agent, the more it perceives POWER as being concentrated at gridworld cells that maximize its global option set. From an instrumental convergence perspective, the fact that POWER concentrates into ever fewer states as the planning horizon of an agent increases at least hints at the possibility of emergent competitive interactions between far-sighted agents. The more relative instrumental value converges into fewer states, the more easily we can imagine multiple agents competing with each other over those few high-POWER states. But it‚Äôs hard to draw any firm conclusions about this at the moment, since our experiments so far have only involved single agents. In the next post, we‚Äôll propose a new definition of multi-agent POWER grounded in a setting that we think may be relevant to long-term AI alignment. We‚Äôll also investigate how this definition behaves in a simple multi-agent scenario, before moving on to bigger-scale experiments in Part 3. ^ We mean specifically utility here, not reward . While in general, reward isn‚Äôt the real target of optimization , in the particular case of the results we'll be showing here, we can treat them as identical, and we do that in the text. (Technical details: we can treat utility and reward identically here because, in the results we‚Äôre choosing to show , we‚Äôll be exclusively working with optimal policies that have been learned via value iteration on reward functions that are sampled from a uniform distribution [0, 1] that‚Äôs iid over states. Therefore, given the environment and discount factor, a sampled reward function is sufficient to uniquely determine the agent's optimal policy ‚Äî except on a set that has measure zero over the distribution of reward functions we‚Äôre considering. And that in turn means that each sampled reward function, when combined with the other known constraints on the agent, almost always supplies a complete explanation for the agent‚Äôs actions ‚Äî which is the most a utility function can ever do.) ^ For simplicity, in this work we‚Äôll only consider reward functions that depend on states , and never reward functions that directly depend on both states and actions. In other words, our reward functions will only ever have the form R ( s ) , and never R ( s , a ) . ^ Note that these are statements about the relative POWERs of an agent with a given planning horizon. Absolute POWER values always increase as the planning horizon of the agent increases, as you can verify by, e.g., comparing the POWER numbers of Fig 2 against those of Fig 3. This occurs because an agent‚Äôs optimal state-value function increases monotonically as we increase Œ≥ : an optimal far-sighted agent is able to consider strictly more options, so it will never do any worse than an optimal short-sighted one. ^ Note that the colors of the gridworld cells in the animation indicate the highest and lowest POWER values within each frame , per footnote [3] .
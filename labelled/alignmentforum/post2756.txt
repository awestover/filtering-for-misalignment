Previous proof post is here. Theorem 2: Isomorphism Theorem: For (causal, pseudocausal, acausal, surcausal) Θ s t or Θ ω which fulfill finitary or infinitary analogues of all the defining conditions, ↑ ( Θ s t ) and ↓ ( Θ ω ) are (causal, pseudocausal, acausal, surcausal) hypotheses. Also, ↑ and → s t define an isomorphism between Θ and Θ s t , and ↓ and → ω define an isomorphism between Θ and Θ ω . Proof sketch: The reason this proof is so horrendously long is that we've got almost a dozen conditions to verify, and some of them are quite nontrivial to show and will require sub-proof-sketches of their own! Our first order of business is verifying all the conditions for a full belief function for ↑ ( Θ s t ) . Then, we have to do it all over again for ↓ ( Θ ω ) . That comprises the bulk of the proof. Then, we have to show that taking a full belief function Θ and restricting it to the infinite/finite levels fulfills the infinite/finite analogues of all the defining conditions for a belief function on policies or policy-stubs, which isn't quite as bad. Once we're done with all the legwork showing we can derive all the conditions from each other, showing the actual isomorphism is pretty immediate from the Consistency condition of a belief function. Part 1:Let's consider ↑ ( Θ π s t ) . This is defined as: ↑ ( Θ s t ) ( π p a ) : = ⋂ π s t ≤ π p a ( p r π p a , π s t ∗ ) − 1 ( Θ s t ( π s t ) ) We'll show that all 9+2 defining conditions for a belief function are fulfilled for ↑ ( Θ s t ) . The analogue of the 9+2 conditions for a Θ s t is: 1: Stub Nirvana-free Nonemptiness: ∀ π s t : Θ s t ( π s t ) ∩ N F ≠ ∅ 2: Stub Closure: ∀ π s t : Θ s t ( π s t ) = ¯ ¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯ ¯ Θ s t ( π s t ) 3: Stub Convexity. ∀ π s t : Θ s t ( π s t ) = c . h ( Θ s t ( π s t ) ) 4: Stub Nirvana-free Upper-Completion. ∀ π s t : Θ s t ( π s t ) ∩ N F = ( ( Θ s t ( π s t ) ∩ N F ) + M s a ( F N F ( π s t ) ) ) ∩ M a ( F ( π s t ) ) 5: Stub Restricted Minimals: ∃ λ ⊙ , b ⊙ ∀ π s t : ( λ μ , b ) ∈ ( Θ s t ( π s t ) ) min → λ + b ≤ λ ⊙ + b ⊙ 6: Stub Normalization: inf π s t E Θ s t ( π s t ) ( 0 ) = 0 and sup π s t E Θ s t ( π s t ) ( 1 ) = 1 7: Weak Consistency: ∀ π l o s t , π h i s t ≥ π l o s t : p r π h i s t , π l o s t ∗ ( Θ s t ( π h i s t ) ) ⊆ Θ s t ( π l o s t ) 8: Stub Extreme Point Condition: for all M , π l o s t : M ∈ ( Θ s t ( π l o s t ) ) x m i n ∩ N F → ∃ π ≥ π l o s t ∀ π h i s t : ( π > π h i s t ≥ π l o s t → ( ∃ M ′ ∈ Θ s t ( π h i s t ) ∩ N F : p r π h i s t , π l o s t ∗ ( M ′ ) = M ) ) 9: Stub Uniform Continuity: The function π s t ↦ ( p r ∞ , π s t ∗ ) − 1 ( Θ s t ( π s t ) ∩ N F ∩ { ≤ ⊙ } ) is uniformly continuous. C: Stub Causality: ∀ π s t , M ∈ Θ s t ( π s t ) ∃ o f ∀ π ′ s t : o f ( π s t ) = M ∧ o f ( π ′ s t ) ∈ Θ s t ( π ′ s t ) where the outcome function o f is defined over all stubs. P: Stub Pseudocausality: ∀ π s t , π ′ s t : ( ( M ∈ Θ s t ( π s t ) ∧ supp ( M ) ⊆ F N F ( π ′ s t ) ) → M ∈ Θ s t ( π ′ s t ) ) Let's begin showing the conditions. But first, note that since we have weak consistency, we can invoke Lemma 6 to reexpress ↑ ( Θ s t ) ( π p a ) as ⋂ n ( p r π p a , π n p a ∗ ) − 1 ( Θ s t ( π n p a ) ) Where π n p a is the n'th member of the fundamental sequence of π p a . Also note that, for all stubs, ↑ ( Θ s t ( π s t ) ) = Θ s t ( π s t ) . We'll be casually invoking this all over the place and won't mention it further. Proof: By Lemma 6 with weak consistency, ↑ ( Θ s t ( π s t ) ) = ⋂ n ≥ m ( p r π s t , π n s t ∗ ) − 1 ( Θ s t ( π n s t ) ) Now, m can be anything we like, as long as it's finite. Set m to be larger than the maximum timestep that the stub is defined for. Then π n s t = π s t no matter what n is (since it's above m) and projection from a stub to itself is identity, so the preimage is exactly our original set Θ s t ( π s t ) . We'll also be using another quick result. For all stubs π h i s t ≥ π l o s t , given stub causality, p r π h i s t , π l o s t ∗ ( Θ s t ( π h i s t ) ) = Θ s t ( π l o s t ) Proof: Fix an arbitrary point M ∈ Θ s t ( π l o s t ) . By causality, we get an outcome function which includes M , giving us that there's something in Θ s t ( π h i s t ) that projects down onto M . Use weak consistency to get the other subset direction. Condition 1: Nirvana-free Nonemptiness. Invoking Stub Nirvana-free Nonemptiness, ∀ π s t : Θ s t ( π s t ) ∩ N F ≠ ∅ so we get nirvana-free nonemptiness for ↑ ( Θ s t ) ( π s t ) . Now, assume π p a is not a stub. By stub-bounded-minimals, there is some λ ⊙ + b ⊙ bound on the set of minimal points, regardless of stub. Let ( Θ s t ( π n p a ) ) c l i p be Θ s t ( π n p a ) ∩ { ≤ ⊙ } ∩ N F This contains all the minimal nirvana-free points for π n p a . This set is nonempty because we have stub nirvana-free nonemptiness, so a nirvana-free point M exists. We have stub-closure and stub minimal-boundedness, so we can step down to a minimal nirvana-free point below M , and it obeys the λ ⊙ + b ⊙ bound. Further, by weak consistency and projection preserving λ and b and nirvana-freeness, p r π n + 1 p a , π n p a ∗ ( ( Θ s t ( π n + 1 p a ) ) c l i p ) ⊆ ( Θ s t ( π n p a ) ) c l i p Invoking Lemma 9, the intersection of preimages of these is nonempty. It's also nirvana-free, because if there's nirvana somewhere, it occurs after finite time, so projecting down to some sufficiently large finite stage preserves the presence of Nirvana, but then we'd have a nirvana-containing point in a nirvana-free set ( Θ s t ( π n p a ) ) c l i p , which is impossible. This is also a subset of the typical intersection of preimages used to define ↑ ( Θ s t ) ( π p a ) . Pick an arbitrary point in said intersection of preimages of clipped subsets. Bam, we found a nirvana-free point in ↑ ( Θ s t ) ( π p a ) and we're done. Time for conditions 2 and 3, Closure and Convexity . These are easy. ↑ ( Θ s t ) ( π p a ) = ⋂ n ( p r π p a , π n p a ∗ ) − 1 ( Θ s t ( π n p a ) ) The preimage of a closed set (stub-closure) is a closed set, and the intersection of closed sets is closed, so we have closure. Also, p r π p a , π n s t ∗ is linear, so the preimage of a convex set (stub-convexity) is convex, and we intersect a bunch of convex sets so it's convex as well. Condition 4: Nirvana-free upper completion. Let M ∈ ↑ ( Θ s t ) ( π p a ) ∩ N F . Let's check whether M + M ∗ (assuming that's an a-measure and M ∗ is nirvana-free) also lies in the set. A sufficient condition on this given how we defined things is that for all π n p a , p r π p a , π n p a ∗ ( M + M ∗ ) ∈ Θ s t ( π n p a ) , as that would certify that M + M ∗ is in all the preimages. p r π p a , π n p a ∗ is linear, so p r π p a , π n p a ∗ ( M + M ∗ ) = p r π p a , π n p a ∗ ( M ) + p r π p a , π n p a ∗ ( M ∗ ) The first component is in Θ s t ( π s t ) , obviously. And then, by stub nirvana-free-upper-completion, we have a nirvana-free a-measure plus a nirvana-free sa-measure (projection preserves nirvana-freeness), making a nirvana-free a-measure (projection preserves a-measures), so p r π p a , π n p a ∗ ( M ) + p r π p a , π n p a ∗ ( M ∗ ) is in Θ s t ( π n p a ) ∩ N F , and we're done. Condition 5: Bounded-Minimals So, there is a critical λ ⊙ + b ⊙ value by restricted-minimals for Θ s t Fix a π p a , and assume that there is a minimal point M in ↑ ( Θ s t ) ( π p a ) with a λ + b value that exceeds the bound. Project M down into each Θ s t ( π n p a ) . Projection preserves λ and b so each of these projected points M n lie above some M min n . Now, invoke Lemma 7 to construct a M ′ n ∈ M a ( F ( π p a ) ) (or the nirvana-free variant) that lies below M , and projects down to M min n . Repeat this for all n. All these M ′ n points are a-measures and have the standard λ ⊙ + b ⊙ bound so they all lie in a compact set and we can extract a convergent subsequence, that converges to M ′ , which still obeys the λ ⊙ + b ⊙ bound. M ′ is below M because ( { M } − M s a ( F ( π p a ) ) ) ∩ M a ( F ( π p a ) ) (or the nirvana-free variant) is a closed set. Further, by Lemma 10, M ′ is in the defining sequence of intersections for ↑ ( Θ s t ) ( π p a ) . This witnesses that M isn't minimal, because we found a point below it that actually obeys the bounds. Thus, we can conclude minimal-point-boundedness for ↑ ( Θ s t ) . Condition 6: Normalization. We'll have to go out of order here, this can't be shown at our current stage. We're going to have to address Hausdorff continuity first, then consistency, and solve normalization at the very end. Let's put that off until later, and just get extreme points. Condition 8: Extreme point condition: The argument for this one isn't super-complicated, but the definitions are, so let's recap what condition we have and what condition we're trying to get. Condition we have: for all M , π l o s t : M ∈ ( Θ s t ( π l o s t ) ) x m i n ∩ N F → ∃ π ≥ π l o s t ∀ π h i s t : ( π > π h i s t ≥ π l o s t → ( ∃ M ′ ∈ Θ s t ( π h i s t ) ∩ N F : p r π h i s t , π l o s t ∗ ( M ′ ) = M ) ) Condition we want: for all M , π s t , M ∈ ( Θ s t ( π s t ) ) x m i n ∩ N F → ∃ π > π s t , M ′ : M ′ ∈ ↑ ( Θ s t ) ( π ) ∩ N F ∧ p r π , π s t ∗ ( M ′ ) = M Ok, so M ∈ ( Θ s t ( π s t ) ) x m i n ∩ N F By the stub extreme point condition, there's a π ≥ π s t , where, for all π h i s t that fulfill π > π h i s t ≥ π s t , there's a M ′ ∈ Θ s t ( π h i s t ) ∩ N F , where p r π h i s t , π s t ∗ ( M ′ ) = M . Lock in the π we have. We must somehow go from this to a M ′ ∈ Θ ( π ) ∩ N F that projects down to our point of interest. To begin with, let π n be the n'th member of the fundamental sequence for π . Past a certain point m, these start being greater than π s t . The M ′ ∈ Θ s t ( π n ) ∩ N F which projects down to M that we get by the stub-extreme-point condition will be called M ′ l o n . Pick some random-ass point in ( p r π , π n s t ∗ ) − 1 ( M ′ l o n ) and call it M ′ h i n . M ′ h i n all obey the λ and b values of M , because it projects down to M . We get a limit point of them, M ′ , and invoking Lemma 10, it's also in ↑ ( Θ s t ) ( π ) . It also must be nirvana-free, because it's a limit of points that are nirvana-free for increasingly late times. It also projects down to M because the sequence M ′ h i n was wandering around in the preimage of M , which is closed. Condition 9: Hausdorff Continuity: Ok, this one is going to be fairly complicated. Remember, our original form is: "The function π s t ↦ ( p r ∞ , π s t ∗ ) − 1 ( Θ s t ( π s t ) ∩ N F ∩ { ≤ ⊙ } ) is uniformly continuous" And the form we want is: "The function π p a ↦ ( p r ∞ , π p a ∗ ) − 1 ( ↑ ( Θ s t ) ( π p a ) ∩ N F ∩ { ≤ ⊙ } ) is uniformly continuous" Uniform continuity means that if we want an ϵ Hausdorff-distance between two preimages, there's a δ distance between partial policies that suffices to produce that. To that end, fix our ϵ . We'll show that the δ we get from uniform continuity on stubs suffices to tell us how close two partial policies must be. So, we have an ϵ . For uniform continuity, we need to find a δ where, regardless of which two partial policies π p a and π ′ p a we select, as long as they're δ or less apart, the sets ( p r ∞ , π p a ∗ ) − 1 ( ↑ ( Θ s t ) ( π p a ) ∩ N F ∩ { ≤ ⊙ } ) (and likewise for π ′ p a ) are only ϵ apart. So, every point in the first preimage must have a point in the second preimage only ϵ distance away, and vice-versa. However, we can swap π p a and π ′ p a (our argument will be order-agnostic) to establish the other direction, so all we really need to do is to show that every point in the preimage associated with π p a is within ϵ of a point in the preimage associated with π ′ p a . First, m : = log γ ( δ ) is the time at which two partial policies δ apart may start differing. Conversely, any two partial policies which only disagree at-or-after m are δ apart or less. Let π ∗ s t be the policy stub defined as follows: take the inf of π p a and π ′ p a (the partial policy which is everything they agree on, which is going to perfectly mimic both of them up till time m), and clip things off at time m to make a stub. This is only δ apart from π p a and π ′ p a , because it perfectly mimics both of them up till time m, and then becomes undefined (so there's a difference at time m) Both π p a and π ′ p a are ≥ π ∗ s t . Let M be some totally arbitrary point in ( p r ∞ , π p a ∗ ) − 1 ( Θ s t ( π p a ) ∩ N F ∩ { ≤ ⊙ } ) . M is also in ( p r ∞ , π ∗ s t ∗ ) − 1 ( Θ s t ( π ∗ s t ) ∩ N F ∩ { ≤ ⊙ } ) , because M projects down to some point in Θ s t ( π ∗ s t ) that's nirvana-free. Let π ′ n p a , where n ≥ m , be the n'th stub in the fundamental sequence for π ′ p a . These form a chain starting at π ∗ s t and ascending up to π ′ p a , and are all δ distance from π ∗ s t . Anyways, in M a ( ∞ ) , we can make a closed ball B ≤ ϵ of size ϵ around M . This restricts λ and b to a small range of values, so we can use the usual arguments to conclude that B ≤ ϵ is compact. Further, because π ∗ s t is δ or less away from π ′ n p a , the two sets ( p r ∞ , π ∗ s t ∗ ) − 1 ( Θ s t ( π ∗ s t ) ∩ N F ∩ { ≤ ⊙ } ) and ( p r ∞ , π ′ n p a ∗ ) − 1 ( Θ s t ( π ′ n p a ) ∩ N F ∩ { ≤ ⊙ } ) are within ϵ of each other, so there's some point of the latter set that lies within our closed ϵ -ball. Consider the set ⋂ n ≥ m ( ( p r ∞ , π ′ n p a ∗ ) − 1 ( Θ s t ( π ′ n p a ) ∩ N F ∩ { ≤ ⊙ } ) ∩ B ≤ ϵ ) the inner intersection is an intersection of closed and compact sets, so it's compact. Thus, this is an intersection of an infinite family of nonempty compact sets. To check the finite intersection property, just observe that since preimages of the sets ↑ ( Θ s t ) ( π ′ n p a ) ∩ N F ∩ { ≤ ⊙ } get smaller and smaller as n increases due to weak-consistency but always exist. Pick some arbitrary point M ′ from the intersection. it's ≤ ϵ away from M since it's in the ϵ -ball. However, we still have to show that M ′ is in ( p r ∞ , π ′ p a ∗ ) − 1 ( ↑ ( Θ s t ) ( π ′ p a ) ∩ N F ∩ { ≤ ⊙ } ) to get Hausdorff-continuity to go through. To begin with, since M ′ lies in our big intersection, we can project it down to any Θ s t ( π ′ n p a ) ∩ N F ∩ { ≤ ⊙ } . Projecting it down to stage n makes M ′ n . Let M ′ ∞ be the point in ↑ ( Θ s t ) ( π ′ p a ) ∩ N F ∩ { ≤ ⊙ } defined by: ⋂ n ≥ m ( p r π ′ p a , π n s t ∗ ) − 1 ( M ′ n ) Well, we still have to show that this set is nonempty, contains only one point, and that it's in ↑ ( Θ s t ) ( π ′ p a ) , and is nirvana-free, to sensibly identify it with a single point. Nonemptiness is easy, just invoke Lemma 9. It lies in the usual intersections that define ↑ ( Θ s t ) ( π ′ p a ) , so we're good there. If it had nirvana, it'd manifest at some finite point, but all finite projections are nirvana-free, so it's nirvana-free. If it had more than one point in it, they differ at some finite stage, so we can project to a finite π ′ n p a to get two different points, but they both project to M ′ n , so this is impossible. Thus, M ′ ∞ is a legit point in the appropriate set. If the projection of M ′ didn't equal M ′ ∞ , then we'd get two different points, which differ at some finite stage, so we could project down to separate them, but they both project to M ′ n for all n so this is impossible. So, as a recap, we started with an arbitrary point M in ( p r ∞ , π p a ∗ ) − 1 ( ↑ ( Θ s t ) ( π p a ) ) ∩ { ≤ ⊙ } , and got another point M ′ that's only ϵ or less away and lies in ( p r ∞ , π ′ p a ∗ ) − 1 ( ↑ ( Θ s t ) ( π ′ p a ) ) ∩ { ≤ ⊙ } This argument also works if we flip π p a and π ′ p a , so the two preimages are only ϵ or less apart in Hausdorff-distance. So, given some ϵ , there's some δ where any two partial policies which are only δ apart have preimages only ϵ apart from each other in Hausdorff-distance. And thus, we have uniform continuity for the function mapping π p a to the set of a-measures over infinite histories which project down to ↑ ( Θ s t ) ( π p a ) ∩ N F ∩ { ≤ ⊙ } Hausdorff-continuity is done. Condition 7: Consistency. Ok, we have two relevant things to check here. The first, very easy one, is that ↑ ( Θ s t ) ( π p a ) = ⋂ π s t ≤ π p a ( p r π p a , π s t ∗ ) − 1 ( ↑ ( Θ s t ) ( π s t ) ) From earlier, we know that ↑ ( Θ s t ) ( π s t ) = Θ s t ( π s t ) , and from how ↑ ( Θ s t ) ( π p a ) is defined, this is a tautology. The other, much more difficult direction, is that ↑ ( Θ s t ) ( π p a ) = ¯ ¯¯¯¯¯¯ ¯ c . h ( ⋃ π ≥ π p a p r π , π p a ∗ ( ↑ ( Θ s t ) ( π ) ) ) We'll split this into four stages. First, we'll show one subset direction holds in full generality. Second, we'll get the reverse subset direction for causal/surcausal. Third, we'll show it for policy stubs for pseudocausal/acausal, and finally we'll use that to show it for all partial policies for pseudocausal/acausal. First, the easy direction. ↑ ( Θ s t ) ( π p a ) ⊇ ¯ ¯¯¯¯¯¯ ¯ c . h ( ⋃ π ≥ π p a p r π , π p a ∗ ( ↑ ( Θ s t ) ( π ) ) ) If we pick an arbitrary M ∈ ↑ ( Θ s t ) ( π ) , it projects down to Θ s t ( π s t ) for all stubs π s t below π . Since π p a ≤ π , it projects down to all stubs beneath π p a . Since projections commute, M projected down into M a ( F ( π p a ) ) makes a point that lies in the preimage of all the Θ s t ( π s t ) where π s t ≤ π p a , so it projects down into ↑ ( Θ s t ) ( π p a ) . This holds for all points in ↑ ( Θ s t ) ( π ) , so p r π , π p a ∗ ( ↑ ( Θ s t ) ( π ) ) ⊆ ↑ ( Θ s t ) ( π p a ) . This works for all π ≥ π p a , so it holds for the union, and then due to closure and convexity which we've already shown, we get that the closed convex hull of the projections lies in ↑ ( Θ s t ) ( π p a ) too, establishing one subset direction in full generality. Now, for phase 2, deriving ↑ ( Θ s t ) ( π p a ) ⊆ ¯ ¯¯¯¯¯¯ ¯ c . h ( ⋃ π ≥ π p a p r π , π p a ∗ ( ↑ ( Θ s t ) ( π ) ) ) in the causal/surcausal case. First, observe that if π ≥ π p a , then π n ≥ π n p a . Fix some M ∈ ↑ ( Θ s t ) ( π p a ) and arbitrary π ≥ π p a . We'll establish the existence of a M ′ ∈ ↑ ( Θ s t ) ( π ) that projects down to M . To begin with, M projects down to M n in Θ s t ( π n p a ) . Lock in a value for n, and consider the sequence that starts off M 0 , M 1 . . . M n , and then, by causality for stubs and π n ≥ π n p a , you can find something in Θ s t ( π n ) that projects down onto M n , and something in Θ s t ( π n + 1 ) that projects down onto that , and complete your sequence that way, making a sequence of points that all project down onto each other that climb up to π . By Lemma 10, we get a M ′ n ∈ ↑ ( Θ s t ) ( π ) . You can unlock n now. All these M ′ n have the same λ and b value because projection preserves them, so we can isolate a convergent subsequence converging to some M ′ ∈ ↑ ( Θ s t ) ( π ) . Assume p r π , π p a ∗ ( M ′ ) ≠ M . Then we've got two different points. They differ at some finite stage, so there's some n where can project down onto M a ( F ( π n p a ) ) to witness the difference, but from our construction process for M ′ , both M ′ and M project down to M n , and we get a contradiction. So, since p r π , π p a ∗ ( ↑ ( Θ s t ) ( π ) ) = ↑ ( Θ s t ( π p a ) ) , this establishes the other direction, showing equality, and thus consistency, for causal/surcausal hypotheses. For part 3, we'll solve the reverse direction for pseudocausal/acausal hypotheses in the case of stubs, getting ↑ ( Θ s t ) ( π s t ) ⊆ ¯ ¯¯¯¯¯¯ ¯ c . h ( ⋃ π ≥ π p a p r π , π s t ∗ ( ↑ ( Θ s t ) ( π ) ) ) Since we're working in the Nirvana-free case and are working with stubs, we can wield Lemma 3 c . h ( ( Θ s t ( π s t ) ) min ) = c . h ( ( Θ s t ( π s t ) ) xmin ) So, if we could just show that the union of the projections includes all the extreme minimal points, then when we take convex hull, we'd get the convex hull of the extreme minimal points, which by Lemma 3, would also nab all the minimal points as well. By Lemmas 11 and 12, our resulting convex hull of a union of projections from above would be upper-complete. It would also get all the minimal points, so it'd nabs the entire Θ s t ( π s t ) within it and this would show the other set inclusion direction for pseudocausal/acausal stubs. Also, we've shown enough to invoke Lemma 20 to conclude that said convex hull is closed. Having fleshed out that argument, all we need that all extreme minimal points are captured by the union of the projections. By our previously proved extreme minimal point condition, for every extreme minimal point M in Θ s t ( π s t ) , there's some π > π s t and M ′ in ↑ ( Θ s t ) ( π ) that projects down to M , which shows that all extreme points are included, and we're good. For part 4, we'll show that in the nirvana-free pseudocausal/acausal setting, we have ↑ ( Θ s t ) ( π p a ) ⊆ ¯ ¯¯¯¯¯¯ ¯ c . h ( ⋃ π ≥ π p a p r π , π p a ∗ ( ↑ ( Θ s t ) ( π ) ) ) Fix some arbitrary M ∈ ↑ ( Θ s t ) ( π p a ) . Our task is to express it as a limit of some sequence of points that are mixtures of stuff projected from above. For this, we can run through the same exact proof path that was used in the part of the Lemma 21 proof about how the nirvana-free part of Θ ( π p a ) is a subset of closed convex hull of the projections of the nirvana-free parts of Θ ( π ) , π ≥ π p a . Check back to it. Since we're working in the Nirvana-free case, we can apply it very straightforwardly. The stuff used in that proof path is the ability to project down and land in ↑ ( Θ s t ) ( π n p a ) (we have that by how we defined ↑ ), Hausdorff-continuity (which we have), and stubs being the convex hull, not the closed convex hull, of projections of stuff above them (which we mentioned recently in part 3 of our consistency proof). Thus, consistency is shown. Condition 6: Normalization. Ok, now that we have all this, we can tackle our one remaining condition, normalization. Then move on to the two optional conditions, causality and pseudocausality. A key thing to remember is, in this setting, when you're doing E H ( f ) , it's actually E H ∩ N F ( f ) , because if Murphy picked a thing with nirvana in it, you'd get infinite value, which is not ok, so Murphy always picks a nirvana-free point. Let's show the first part, that inf π E ↑ ( Θ s t ) ( π ) ( 0 ) = 0 . This unpacks as: inf π min ( λ μ , b ) ∈ ↑ ( Θ s t ) ( π ) ∩ N F b = 0 and we have that inf π s t min ( λ μ , b ) ∈ Θ s t ( π s t ) ∩ N F b = 0 Projections preserves b values, so we can take some nirvana-free point with a b value of nearly 0, and project it down to Θ s t ( π ∅ ) (belief function of the empty policy-stub) where there's no nirvana possible because no events happen. So, we've got b values of nearly zero in there. Do we have a point with a b value of exactly zero? Yes. it's closed, and has bounded minimals, so we can go "all positive functionals are minimized by a minimal point", to get a point with a b value of exactly zero. Then, we can invoke Lemma 21 (we showed consistency, extreme points, and all else required to invoke it) to decompose our point into the projections of nirvana-free stuff from above, all of which must have a b value of 0. So, there's a nirvana-free point in some policy with 0 b value. Now for the other direction. Let's show that sup π E ↑ ( Θ s t ) ( π ) ( 1 ) = 1 . This unpacks as: sup π min ( λ μ , b ) ∈ ↑ ( Θ s t ) ( π ) ∩ N F ( λ + b ) = 1 We'll show this by disproving that the sup is <1, and disproving that the sup is >1. First, assume that, regardless of π , min ( λ μ , b ) ∈ ↑ ( Θ s t ) ( π ) ∩ N F ( λ + b ) ≤ 1 − ϵ Then, regardless of π s t we can pick some totally arbitrary π > π s t , and there's a nirvana-free point with a λ + b value of 1 − ϵ or less. By consistency, we can project it down into Θ ( π s t ) , to get a nirvana-free point with a λ + b value of 1 − ϵ or less. Thus, regardless of the stub we pick, there's nirvana-free points where Murphy can force a value of 1 − ϵ or less, which contradicts sup π s t min ( λ μ , b ) ∈ Θ s t ( π s t ) ∩ N F ( λ + b ) = 1 What if it's above 1? Assume there's some π where min ( λ μ , b ) ∈ ↑ ( Θ s t ) ( π ) ∩ N F ( λ + b ) ≥ 1 + ϵ From uniform-continuity-Hausdorff, pick some δ to get a ϵ 2 Hausdorff-distance or lower (for stuff obeying the λ ⊙ + b ⊙ bound, which all minimal points of ↑ ( Θ s t ) ( π ) ∩ N F do for all π ). This δ specifies some extremely large n, consider π n . Now, consider the set of every policy π ′ above π n . All of these are δ or less away from π . Also, remember that the particular sort of preimage-to-infinity that we used for Hausdorff-continuity slices away all the nirvana. So, Murphy, acting on π , can only force a value of 1 + ϵ or higher. Now, there can be no nirvana-free point in ↑ ( Θ s t ) ( π ′ ) with λ + b < 1 + ϵ 2 . The reason for this is that, since π ′ is δ or less away from π , there's a nirvana-free point in ↑ ( Θ s t ) ( π ) that's ϵ 2 away, and thus has λ + b < 1 + ϵ , which is impossible. Ok, so all the nirvana-free points in ↑ ( Θ s t ) ( π ′ ) where π ′ > π n s t have λ + b ≥ 1 + ϵ 2 . Now, since we have Lemma 21, we can go "hm, Θ s t ( π n ) equals the convex hull of the projections of ↑ ( Θ s t ) ( π ′ ) ∩ N F . Thus, any minimal point with λ + b ≤ 1 is a finite mix of nirvana-free stuff from above, one of which must have λ + b ≤ 1 . But we get a contradiction with the fact that there's no nirvana-free point from π ′ above π n with a λ + b value that low, they're all ≥ 1 + ϵ 2 " So, since we've disproved both cases, sup π E ↑ ( Θ s t ) ( π ) ( 1 ) = 1 . And we're done with normalization! On to causality and pseudocausality. Condition C: Causality. An "outcome function" o f for ↑ ( Θ s t ) ( π p a ) is a function that maps a π p a to a point in ↑ ( Θ s t ) ( π p a ) , s.t. for all π h i p a , π l o p a : p r π h i p a , π l o p a ∗ ( o f ( π h i p a ) ) = o f ( π l o p a ) . Causality is, if you have a M ∈ ↑ ( Θ s t ) ( π p a ) , you can always find an outcome function o f where o f ( π p a ) = M . Sadly, all we have is causality over stubs. We'll be using the usual identification between ↑ ( Θ s t ) ( π s t ) and Θ s t ( π s t ) . Anyways, fix a π p a and a point M in ↑ ( Θ s t ) ( π p a ) . Project M down to get a sequence M n ∈ Θ s t ( π n p a ) . By causality for stubs, we can find an o f n where, for all π s t , o f n ( π n p a ) = M n . Observe that there are countably many stubs, and no matter the n, all the λ and b values are the same because projection preserves those. We can view o f n as a sequence in ∏ π s t ( Θ s t ( π s t ) ∩ { ( λ ′ μ , b ′ ) | λ ′ = λ ∧ b ′ = b } ) By stub closure, and a λ and b bound, this is a product of compact sets, and thus compact by Tychonoff (no axiom of choice needed, its just a countable product of compact metric spaces) so we can get a limiting o f s t (because it's only defined over stubs). An outcome function for stubs fixes an outcome function for all partial policies, by o f ( π p a ) = ⋂ n ( p r π p a , π n p a ∗ ) − 1 ( o f s t ( π n p a ) ) We've got several things to show now. We need to show that o f s t is an outcome function, that o f is well-defined, that o f ( π p a ) = M , and that it's actually an outcome function. For showing that o f s t is an outcome function, observe that projection is continuous, and, letting n index our convergent subsequence of interest, regardless of stub π s t , lim n → ∞ o f n ( π s t ) = o f s t ( π s t ) . With this, p r π h i s t , π l o s t ∗ ( o f s t ( π h i s t ) ) = p r π h i s t , π l o s t ∗ ( lim n → ∞ o f n ( π h i s t ) ) = lim n → ∞ p r π h i s t , π l o s t ∗ ( o f n ( π h i s t ) ) = lim n → ∞ o f n ( π l o s t ) = o f s t ( π l o s t ) Now, let's show that o f is well-defined. Since o f s t is an outcome function, all the points project down onto each other, so we can invoke Lemma 9 to show that the preimage is nonempty. If the preimage had multiple points, we could project down to some finite stage to observe their difference, but nope, they always project to the same point. So it does pick out a single well-defined point, and it lies in ↑ ( Θ s t ) ( π p a ) by being a subset of the defining sequence of intersection of preimages. Does o f ( π p a ) = M ? Well, M projected down to all the M n . If n ≥ m , then o f n ( π m p a ) = p r π n p a , π m p a ∗ ( o f n ( π n p a ) ) = p r π n p a , π m p a ∗ ( M n ) = M m So, the limit specification o f s t has o f s t ( π n p a ) = M n for all n. The only thing that projects down to make all the M n is M itself, so o f ( π p a ) = M . Last thing to check: Is o f an outcome function over partial policies? Well, if π h i p a ≥ π l o p a , then for all n, π h i , n p a ≥ π l o , n p a . Assume p r π h i p a , π l o p a ∗ ( o f ( π h i p a ) ) ≠ o f ( π l o p a ) . Then, in that case, we can project down to some π l o , n p a and they'll still be unequal. However, since projections commute, it doesn't matter whether you project down to π l o p a and then to π l o , n p a , or whether you project down to π h i , n p a (making o f s t ( π h i , n p a ) ), and then project down to π l o , n p a (making o f s t ( π l o , n p a ) ). Wait, hang on, this is the exact point that o f ( π l o p a ) projects down to, contradiction. Therefore it's an outcome function. And we're done, we took an arbitrary π p a and M ∈ ↑ ( Θ s t ) ( π p a ) , and got an outcome function o f with o f ( π p a ) = M , showing causality. Condition P: Pseudocausality: If ( m , b ) ∈ ↑ ( Θ s t ) ( π p a ) , and m 's support is on M a ( F N F ( π ′ p a ) ) , then ( m , b ) ∈ ↑ ( Θ s t ) ( π ′ p a ) . But all we have is, if ( m , b ) ∈ Θ s t ( π s t ) and m 's support is on M a ( F ( π ′ s t ) ) , then ( m , b ) ∈ Θ s t ( π ′ s t ) . There's a subtlety here. Our exact formulation of pseudocausality we want is the condition supp ( m ) ⊆ F ( π ′ p a ) , so if the measure is 0, then support is the empty set, which is trivially a subset of everything, then pseudocausality transfers it to all partial policies. Ok, so let's assume that M ∈ ↑ ( Θ s t ) ( π p a ) , and the measure part m has its support being a subset of M a ( F N F ( π ′ p a ) ) but yet is not in ↑ ( Θ s t ) ( π ′ p a ) . Then, since this is an intersection of preimages from below, there should be some finite level π ′ n p a that you can project M down to (it's present in M a ( F N F ( π ′ p a ) ) , just maybe not in ↑ ( Θ s t ) ( π ′ p a ) ) where the projection of M (call it M ′ n ) lies outside Θ s t ( π ′ n p a ) (lying outside the intersection of preimages) This is basically "take m , chop it off at height n". However, since M ∈ ↑ ( Θ s t ) ( π p a ) , you can project it down to Θ s t ( π n p a ) . Which does the exact same thing of chopping m off at height n, getting you M ′ n exactly. We can invoke stub-pseudocausality (because with full measure, the history will land in F ( π ′ p a ) , then with full measure, the truncated history will land in F ( π ′ n p a ) as the latter is prefixes of the former, or maybe the full measure is 0 in which case pseudocausality transfer still works) to conclude that M ′ actually lies inside Θ s t ( π ′ n p a ) , getting a contradiction. This establishes pseudocausality in full generality. Ok, so we have one direction. ↑ ( Θ s t ) is a hypothesis, if Θ s t fulfills analogues of the hypothesis conditions for the finitary stub case. Our proof of everything doesn't distinguish between causal and surcausal, and the arguments work for all types of hypotheses, whether causal, surcausal, pseudocausal, or acausal. Ok, we're 1/4 of the way through. Now we do the same thing, but for building everything from infinitary hypotheses. PART 2: Infinitary hypotheses. We now consider ↓ ( Θ ω ) , defined as ↓ ( Θ ω ) ( π p a ) = ¯ ¯¯¯¯¯¯ ¯ c . h ( ⋃ π ≥ π p a p r π , π p a ∗ ( Θ ω ( π ) ) We'll show that with 8+2 defining conditions, the 9+2 defining conditions for a hypothesis hold for ↓ ( Θ ω ) . The the 8+2 conditions for a Θ ω are: 1: Infinitary Nirvana-Free Nonemptiness: ∀ π : Θ ω ( π ) ∩ N F ≠ ∅ 2: Infinitary Closure: ∀ π : Θ ω ( π ) = ¯ ¯¯¯¯¯¯¯¯¯¯¯¯ ¯ Θ ω ( π ) 3: Infinitary Convexity. ∀ π : Θ ω ( π ) = c . h ( Θ ω ( π ) ) 4: Infinitary Nirvana-free Upper-Completeness ∀ π : Θ ω ( π ) ∩ N F = ( ( Θ ω ( π ) ∩ N F ) + M s a ( F N F ( π ) ) ) ∩ M a ( F ( π ) ) 5: Infinitary Bounded Minimals: ∃ λ ⊙ , b ⊙ ∀ π : ( λ μ , b ) ∈ ( Θ ω ( π ) ) min → λ + b ≤ λ ⊙ + b ⊙ 6: Normalization: inf π E Θ s t ( π ) ( 0 ) = 0 and sup π E Θ s t ( π ) ( 1 ) = 1 7: Nirvana-free consistency. ∀ π s t : c . h ( ⋃ π > π s t p r π , π s t ∗ ( Θ ω ( π ) ∩ N F ) ) = ¯ ¯¯¯¯¯¯ ¯ c . h ( ⋃ π > π s t p r π , π s t ∗ ( Θ ω ( π ) ) ) ∩ N F 8: Infinitary Uniform Hausdorff Continuity: The function π ↦ Θ ω ( π ) ∩ N F ∩ { ≤ ⊙ } is uniformly continuous. C: Infinitary Causality: Regardless of π and M ∈ Θ ω ( π ) , there's an outcome function o f over full policies s.t. o f ( π ) = M , and for all π ′ and π ′′ , p r π ′ , inf ( π ′ , π ′′ ) ∗ ( o f ( π ′ ) ) = p r π ′′ , inf ( π ′ , π ′′ ) ∗ ( o f ( π ′′ ) ) P: Infinitary Pseudocausality: ∀ π , π ′ : ( M ∈ Θ ω ( π ) , supp ( M ) ⊆ F N F ( π ′ ) → π s t ∈ Θ ω ( π ′ ) Let's begin showing the conditions. Condition 1: Nirvana-free Nonemptiness. This one is trivial. Pick some π ≥ π p a . There's a nirvana-free point. Project it down. You get a nirvana-free point and you're done. Conditions 2 and 3 , Closure and convexity. We explicitly took the closed convex hull when defining everything, these are tautological. Condition 4: Nirvana-free upper completion. For the pseudo/acausal case, it's doable by Lemmas 10, 11, and 12. The projection of an upper-complete set (by infinitary nirvana-free upper-completion) is upper-complete, so the union of projections is upper-complete, and then the convex hull is upper-complete, and then the closure is upper-complete and we're done. We'll have to loop back to the causal case of Nirvana-free Upper Completion later, because we need Lemma 21 to make it go through and that requires consistency and the extreme point condition to make it work. Condition 5: Bounded Minimals. We can break down into three phases. First is showing that all points in the projection set have something under them that respects the λ ⊙ + b ⊙ bound. Second is showing that all points in the convex hull of the union of projection sets have something under them that respects the λ ⊙ + b ⊙ bound. Third is showing that all points in the closure have something under them that respects the usual bound. The reason we have to phrase it this way is that we don't necessarily know that our sets of interest are closed until the end, so we can't find a minimal point, just a bounded one that is lower, but that suffices to show that a "minimal point" that violates the restricted minimal condition isn't actually minimal. For part 1, let M ∈ p r π , π p a ∗ ( Θ ω ( π ) ) . Then, M is the projection of some point M ′ ∈ Θ ω ( π ) . By infinitary bounded-minimals, we can find a minimal point M min ∈ Θ ω ( π ) below M ′ that obeys the λ ⊙ + b ⊙ bound, so M min + M ∗ = M ′ . Projecting down is linear, so we get p r π , π p a ∗ ( M min ) + p r π , π p a ∗ ( M ∗ ) = M , and p r π , π p a ∗ ( M min ) is below M and fulfills the λ ⊙ + b ⊙ bound. For part 2, let M ∈ c . h ( ⋃ π ≥ π p a p r π , π p a ∗ ( Θ ω ( π ) ) ) We can rewrite M as E ζ M i , and then, by part 1, decompose the M i into M l o i (not actually minimal, just a point obeying the λ ⊙ + b ⊙ bound) and M ∗ i . Then we can decompose M further into E ζ M l o i + E ζ M ∗ i . The former is an a-measure (mix of a-measures) and obeys the λ ⊙ + b ⊙ bound since all its components do, and it's in the relevant convex hull, witnessing that M has a point below it in the convex hull that obeys the bounds. For part 3, let M be in the closure of the convex hull. There's some sequence M n in the convex hull that limits to M . Below each M n we can find a M l o n (again, not actually minimal) that obeys the λ ⊙ + b ⊙ bound. Invoke Lemma 16 to get a point below M that respects the bounds, and we're done. Condition 6: Normalization. We literally have the exact phrasing of normalization we need already, this is a tautology. Condition 7: Consistency. Ok, one direction is trivial because ↓ ( Θ ω ) ( π ) = Θ ω ( π ) , so we can just use the definition of ↓ . ↓ ( Θ ω ) ( π p a ) = ¯ ¯¯¯¯¯¯ ¯ c . h ( ⋃ π ≥ π p a p r π , π p a ∗ ( ↓ ( Θ ω ) ( π ) ) ) The other direction, that everything equals the intersection of preimages of stuff below it, is trickier. One subset direction isn't too bad, the one that ↓ ( Θ ω ) ( π p a ) ⊆ ⋂ π s t ≤ π p a ( p r π p a , π s t ∗ ) − 1 ( ↓ ( Θ ω ) ( π s t ) ) If we take a M ∈ ↓ ( Θ ω ) ( π p a ) that wasn't added in the final closure step, it's expressible as E ζ M i , and all the M i come from points M ∞ i in Θ ω ( π i ) where π i ≥ π p a . Projecting the M ∞ i down to a π s t ≤ π p a instead makes M l o i , which mix together in the same way to make M l o ∈ ↓ ( Θ ω ) ( π s t ) . Because projections are linear and commute, M l o is the projection of M . So, any point in ↓ ( Θ ω ) ( π p a ) (without the closure step) projects down to lie in ↓ ( Θ ω ) ( π s t ) for any π s t ≤ π p a . Then, for the closure step, we just fix a sequence M n limiting to M . The M n can project down to whichever ↓ ( Θ ω ) ( π s t ) you wish, and by continuity of projection, the M comes along for the ride as a limit point. However, ↓ ( Θ ω ) ( π s t ) is closed, so M projects down to land in that set as well. Bam, any old M ∈ ↓ ( Θ ω ) ( π p a ) projects down to land in any ↓ ( Θ ω ) ( π s t ) set you wish with π s t ≤ π p a , certifying that ↓ ( Θ ω ) ( π p a ) lies in the intersection of preimages of stubs below. Now, we just have to establish ↓ ( Θ ω ) ( π p a ) ⊇ ⋂ π s t ≤ π p a ( p r π p a , π s t ∗ ) − 1 ( ↓ ( Θ ω ) ( π s t ) ) which splits into two cases. The causal/surcausal case, and the pseudocausal/acausal case where you don't have to worry about nirvana. For the nirvana-free case... We can use the same proof strategy as the last part of Lemma 21, where we were showing the result for partial policies. It may be a bit nonobvious why it works. We do need to swap things around a bit, and will mention important changes without fleshing out the fiddly details, which are already given in the last part of Lemma 21. Start with a M in the intersection of preimages of stubs below. To show it's in ↓ ( Θ ω ) ( π p a ) , we need a sequence limiting to it, where each member of the sequence is a mix of finitely many points projected down from policies above π p a . The end part of Lemma 21 gives how to construct such a sequence. The fact that we're working in a nirvana-free setting means you can ignore all fiddly details about points being nirvana-free and preimages of only the nirvana-free parts, because everything fulfills that. The key steps in that proof path are: 1:   being able to project down M to make a sequence M n ∈ ↓ ( Θ ω ) ( π n p a ) . We trivially have this by M being defined as "in the intersection of preimages of stubs below it". 2: Having uniform Hausdorff-continuity for the policies. This is our condition 8 we're assuming, so we're good there. 3: The ability to shatter our M n into finitely many M i , n which are the projections of various M ∞ i , n points from above. This is the key difference. The proof of Lemma 21 had to set up that fact beforehand. However, in our case, we have the Nirvana-free consistency condition, which says ∀ π s t : c . h ( ⋃ π > π s t p r π , π s t ∗ ( Θ ω ( π ) ∩ N F ) ) = ¯ ¯¯¯¯¯¯ ¯ c . h ( ⋃ π > π s t p r π , π s t ∗ ( Θ ω ( π ) ) ) ∩ N F But, since we're working in the nirvana-free setting, this turns into: ∀ π s t : c . h ( ⋃ π > π s t p r π , π s t ∗ ( Θ ω ( π ) ) ) = ¯ ¯¯¯¯¯¯ ¯ c . h ( ⋃ π > π s t p r π , π s t ∗ ( Θ ω ( π ) ) ) And that right-hand term... is just the definition of ↓ ( Θ ω ) ( π s t ) ! So, swapping that out, and specializing our arbitrary stub to π n p a , we have: c . h ( ⋃ π > π n p a p r π , π n p a ∗ ( Θ ω ( π ) ) ) = ↓ ( Θ ω ) ( π n p a ) So, since our M n lie in ↓ ( Θ ω ) ( π n p a ) , they can be written as a finite mix of nirvana-free things from above projected down, and the Lemma 21 argument goes through. Now, for the nirvana cases where we can assume infinitary causality. We'll do this by showing a little sublemma, that, if π ≥ π p a , then p r π , π p a ∗ ( Θ ω ( π ) ) = ↓ ( Θ ω ) ( π p a ) First, we'll show that if π , π ′ ≥ π p a , then p r π , π p a ∗ ( Θ ω ( π ) ) = p r π ′ , π p a ∗ ( Θ ω ( π ′ ) ) Fix an arbitrary M in the projection of Θ ω ( π ) , we can get a preimage point M h i ∈ Θ ω ( π ) . Then, by infinitary causality, we can make a point M ′ h i ∈ Θ ω ( π ′ ) that projects down to M . Just make an outcome function o f where o f ( π ) = M h i , feed in π ′ , that gets you your point M ′ h i , the two agree when you project them down to inf ( π , π ′ ) , and π p a is further down than that and projections commute so they both hit the same point M if you project down. Flipping π and π ′ shows our equality. Alright, so now ↓ ( Θ ω ) ( π p a ) can be written as ¯ ¯¯¯¯¯¯ ¯ c . h ( p r π , π p a ∗ ( Θ ω ( π ) ) ) where π is arbitrary above π p a . Projection is linear, so the projection of a convex set is convex. To get the closure points, just take a sequence M n in the projection limiting to some M . Take preimage points M ∞ n ∈ Θ ω ( π ) . There's a bound on the λ and b values of this sequence because projections preserve λ and b and our sequence M n converges, so we can apply the Compactness Lemma and get a convergent subsequence limiting to a point M ∞ , which must be in Θ ω ( π ) because closure. Projection is continuous, so M ∞ projects down to M . And we have p r π , π p a ∗ Θ ω ( π ) = ↓ ( Θ ω ) ( π p a ) proved! Wow, that was a sublemma of case 2 of part 3 of the proof of condition 7 in part 2 of the proof of the Isomorphism theorem, we're really in the weeds at this point. Moving on, how can we use this to show ↓ ( Θ ω ) ( π p a ) ⊇ ⋂ π s t ≤ π p a ( p r π p a , π s t ∗ ) − 1 ( ↓ ( Θ ω ) ( π s t ) ) for the causal case, which is the last bit we need to show consistency? Well, fix a M in the intersection of preimages, and an arbitrary π ≥ π p a . M projects down to make some M n ∈ ↓ ( Θ ω ) ( π n p a ) . Since π ≥ π p a ≥ π n p a , and we have our sublemma, there's a point M ∞ n in Θ ω ( π ) that projects down to some M ′ n ∈ ↓ ( Θ ω ) ( π p a ) , and further down to M n . This sequence M ∞ n all has the same λ and b value since projection preserves those, so by the Compactness Lemma and closure, there's a convergent subsequence and limit point M ∞ ∈ Θ ω ( π ) . Does M ∞ project down onto M ? (witnessing that M ∈ ↓ ( Θ ω ) ( π p a ) ) ? Well, let's say it didn't and projecting down gets you a distinct point. Then there's some n where projecting down further to π n p a would keep the points distinct, since they have to differ at some finite time. But... after time n, our sequence M ∞ n is roaming around entirely in the preimage of M n , so the limit point is in there too, and it projects down to M n and we have a contradiction. Therefore, M ∞ ∈ Θ ω ( π ) projects down onto M , witnessing that M ∈ ↓ ( Θ ω ) ( π p a ) , and M was arbitrary in the intersection of preimages. So, we have ↓ ( Θ ω ) ( π p a ) ⊇ ⋂ π s t ≤ π p a ( p r π p a , π s t ∗ ) − 1 ( ↓ ( Θ ω ) ( π s t ) ) for the nirvana-containing causal/surcausal case, which is the last piece we needed to show consistency. Condition 8: Extreme point condition. The thing we want is that an extreme nirvana-free minimal point M e x in ↓ ( B F ω ) ( π s t ) ∩ N F is the projection of a nirvana-free point from a policy above it. By the nirvana-free consistency property, M e x lies in c . h ( ⋃ π ≥ π s t p r π , π s t ∗ ( Θ ω ( π ) ∩ N F ) ) M e x is extreme, so it lies in ⋃ π ≥ π s t p r π , π s t ∗ ( Θ ω ( π ) ∩ N F ) So, there's some point in some Θ ω ( π ) ∩ N F that projects down to M e x and we're done. Condition 9: Hausdorff Continuity: Ok, this one is going to be complicated. We'll work with the Lemma 15 version of Hausdorff-continuity, where a δ difference between two policies means that if you start off in one preimage, you've gotta travel ϵ ( 1 + λ ) distance or less to get to the preimage associated with the other policy, and vice-versa. We split into two parts. Part 1 is showing that if π p a ≥ π s t , and the distance between π p a and π s t is δ or less, the distance between their respective preimages is low. Part 2 is showing that if π p a and π ′ p a are δ apart, then we can exploit part 1 to get that the distance between the preimages is low, and will be pretty easy after we get part 1. Our Hausdorff-continuity condition links δ and ϵ . So, when we fix an ϵ and are like "how close do the policies have to be to guarantee the preimages are ϵ apart", pick the δ that gets you ϵ 3 distance w.r.t our original Hausdorff-continuity condition, and also have δ < ϵ 3 . Our first part uses M l o and M ′ l o for points in ↓ ( Θ ω ) ( π s t ) ∩ N F , M ′ m i d for a point in ↓ ( Θ ω ) ( π p a ) ∩ N F , M h i and M ′ h i for points in M a ( ∞ ) (that are expressible as a finite mix of points from Θ ω ( π ) for varying π ), and M , M ′ for two more general points in M a ( ∞ ) . So, one half of showing the two preimages are close to each other is trivial. Everything in ↓ ( Θ ω ) ( π p a ) ∩ N F projects down into ↓ ( Θ ω ) ( π s t ) ∩ N F by consistency, and projection preserving nirvana-freeness, so the preimage associated with π s t is a superset of the preimage associated with π p a , so there's distance 0 from a point in the π p a preimage to a point in the π s t preimage. The other half is trickier. Pick an arbitrary point M in ( p r ∞ , π s t ∗ ) − 1 ( ↓ ( Θ ω ) ( π s t ) ∩ N F ) and λ is the λ value of this point. M projects down to some point M l o in ↓ ( Θ ω ) ( π s t ) ∩ N F . From nirvana-free consistency, M ∈ c . h ( ⋃ π ≥ π s t p r π , π s t ∗ ( Θ ω ( π ) ∩ N F ) ) , M l o can then be produced by (keeping in mind that it doesn't matter whether we mix before or after projecting) finitely many points in varying Θ ω ( π ) ∩ N F sets that are mixed to make a point M h i , and then projected down. Important note: M h i is not necessarily equal to, or even close to, M . Because π s t is within δ of π p a , every policy above π s t has a corresponding policy within δ that lies above π p a . Thus, we can perturb the component points (indexed by i) that mix to make M h i by ϵ 3 ( 1 + λ i ) (infinitary Hausdorff-uniform-continuity Lemma 15 variant, δ was assumed to be small enough for that to be the case), and mix them, to get a M ′ h i in c . h ( ⋃ π ≥ π p a ( Θ ω ( π ) ∩ N F ) ) M ′ h i projects down to ↓ ( Θ ω ) ( π p a ) ∩ N F to make a M ′ m i d , and further projects down to ↓ ( Θ ω ) ( π s t ) ∩ N F to make a M ′ l o . Because M ′ h i is within ϵ 3 ( 1 + λ ) of M h i , projecting down (and projecting being nonexpansive) means that M ′ l o is within ϵ 3 ( 1 + λ ) of M l o . Now, we can take M ′ l o and fill in all the missing measure data to get a M ′ that projects down onto M ′ m i d (certifying that it's in the preimage of ↓ ( Θ ω ) ( π p a ) ∩ N F ) as follows. Our most important constraint is that, when extending m ′ l o , it should perfectly mimic m ′ m i d so it can project down onto it. Our second constraint is that, if m l o doesn't specify what comes after a finite history and it doesn't conflict with the first constraint, it should exactly mimic the conditional probabilities of m . Also, our δ fixes a first time n (ie, log γ ( δ ) ) at which π p a is defined where π s t isn't, so all conflicts of the second constraint with the first constraint must happen after then. This does the following: We can slice the histories assigned measures by M ′ into three parts. Part 1 is prefixes of histories in F ( π s t ) . There's only ϵ ( 1 + λ ) difference in these between M and M ′ (after all, projecting down to π s t leaves these unchanged, and M / M ′ project down to M l o and M ′ l o which are only ϵ 3 ( 1 + λ ) apart). Part 2 is histories which have as a prefix something in F ( π s t ) less than length n. In that case, we're mimicking the conditional probabilities of m . Part 3 is histories which have as a prefix something in F ( π s t ) of length n or higher. Because this is the threshold where π p a and π s t start differing, we've got to obey the m ′ m i d probabilities. But this only occurs after time n. Let's analyze the difference between M ′ and M , shall we? Our two relevant results are Vanessa's folk result that two distributions that differ by an amount will differ by the same amount if we extend them with the same conditional probabilities, and the result from the proof of Lemma 15 that arbitrarily reshuffling the measure/amount of dirt after time n takes γ n λ ′ effort, where λ ′ is the λ value of the measure you're reshuffling. So, we start off with a ϵ 3 ( 1 + λ ) distance (includes the b term) between M l o and M ′ l o . Then, extending up further to fill in everything up till time n, m and m ′ mimic the conditional probabilities of each other. Still a ϵ 3 ( 1 + λ ) distance between them at this stage. Finally, after time n, M ′ may go its own arbitrary way because it's gotta be compliant with M ′ m i d , and to reshuffle this around, it takes γ n λ ′ effort. So, the net distance between M (arbitrary point in the preimage of ↓ ( Θ ω ) ( π s t ) , and M ′ (specially crafted point in the preimage of ↓ ( Θ ω ) ( π p a ) is below ϵ 3 ( 1 + λ ) + γ n λ ′ . Wait, n (time of first difference) was log γ ( δ ) since π s t and π p a are only δ apart, and λ ′ can be at most λ + ϵ 3 ( 1 + λ ) because λ values are preserved by projections, and M l o and M ′ l o are only ϵ 3 ( 1 + λ ) distance apart, so no more than that amount of dirt is the difference between the two. Finally, we assumed δ < ϵ 3 . So, we get: d ( M , M ′ ) ≤ ϵ 3 ( 1 + λ ) + γ n λ ′ = ϵ 3 ( 1 + λ ) + γ log γ ( δ ) ( λ + ϵ 3 ( 1 + λ ) ) < ϵ 3 ( 1 + λ ) + δ ( 2 λ + 2 ) < ϵ 3 ( 1 + λ ) + ϵ 3 ( 2 λ + 2 ) = ϵ ( 1 + λ ) And we have our appropriate distance bound between preimages! Now to use this in part 2, which should go a lot faster. Time for part 2, to get full generality. Pick two partial policies π p a and π ′ p a and assume the distance between them is δ . Then, the stub π s t given by " inf ( π p a , π ′ p a ) but cut it off so it's undefined after time n (where n is log γ ( δ ) )" is within distance δ of both π p a and π ′ p a . Further, π s t ≤ π p a , π ′ p a . Then, take some point in the preimage of π p a . It's also in the preimage of π s t . Because π s t is at a distance of δ from π ′ p a , we only have to go ϵ ( 1 + λ ) distance to get a point in the preimage of π ′ p a , and then reverse π p a and π ′ p a and we're done! By Lemma 15, this establishes uniform continuity for the function mapping partial policies to the preimage of their nirvana-free part in the space of all nirvana-free measures over infinite histories. Condition 4: Nirvana-free upper completeness (causal case) Now that we've nabbed every nice condition other than this one, we can invoke Lemma 21 (we only require upper completion on the infinite levels, which we have) to get that the nirvana-free part is the (closed) convex hull of the projections of nirvana-free stuff from above. Then, just appeal to lemmas 11, 12, and 13, that the closed convex hull of projections of nirvana-free upper-complete sets is nirvana-free upper-complete. Condition C: Causality. We showed part of this all the way back in our consistency argument. For causal/surcausal, p r π , π p a ∗ ( Θ ω ( π ) ) = ↓ ( Θ ω ) ( π p a ) regardless of which π ≥ π p a we picked. We'll be using this. Pick some arbitrary π p a and M ∈ ↓ ( Θ ω ) ( π p a ) . M has a preimage point M π ∈ Θ ω ( π ) where π ≥ π p a . We get an outcome function o f ω mapping policies to points in their associated sets s.t. o f ω ( π ) = M π . Extend this o f ω to all points by defining o f ( π ′ p a ) : = p r π ′ , π ′ p a ∗ ( o f ω ( π ′ ) ) Ok, we need to show that: This actually singles out a unique point and isn't an invalid definition, said point is in ↓ ( Θ ω ) ( π ′ p a ) , that o f ( π p a ) = M , and that it's an outcome function. Assuming this is actually well-defined, o f ( π ′ p a ) is in ↓ ( Θ ω ) ( π ′ p a ) trivially because it's a projection of a point from above. Also, o f ( π p a ) = p r π , π p a ∗ ( o f ω ( π ) ) = p r π , π p a ∗ ( M π ) = M which clean ups that part. Now for showing that it's an outcome function. p r π h i p a , π l o p a ∗ ( o f ( π h i p a ) ) = p r π h i p a , π l o p a ∗ ( p r π , π h i p a ∗ ( o f ω ( π ) ) ) = p r π , π l o p a ∗ ( o f ω ( π ) ) = o f ( π l o p a ) So, we got everything assuming the extension is well-defined, let's show that. Pick any two π , π ′ above any π p a . We'll show that they project to the same point. p r π , π p a ∗ ( o f ω ( π ) ) = p r inf ( π , π ′ ) , π p a ∗ ( p r π , inf ( π , π ′ ) ∗ ( o f ω ( π ) ) ) = p r inf ( π , π ′ ) , π p a ∗ ( p r π ′ , inf ( π , π ′ ) ∗ ( o f ω ( π ′ ) ) ) = p r π ′ , π p a ∗ ( o f ω ( π ′ ) ) And we're done with causality! Now for pseudocausality. Condition P : Pseudocausality. We'll do this in two steps. One is showing that for stubs, points which meet the appropriate conditions are also present in all the requisite other stubs. Step 2 is generalizing this to all points in ↓ ( Θ ω ) ( π p a ) . Let's say you have some M ∈ ↓ ( Θ ω ) ( π s t ) . By Nirvana-free consistency, M ∈ c . h ( ⋃ π ≥ π s t p r π , π s t ∗ ( Θ ω ( π ) ) ) so we can shatter it into finitely many M i that are projections of stuff from above, M ∞ i . The support of the measure component of M is a subset of F N F ( π s t ) ∩ F N F ( π ′ s t ) , so the same must apply to all the M i . Now, what we can do is make a π ′ i that mimics the behavior of π i for all prefixes and extensions of strings in F N F ( π s t ) ∩ F N F ( π ′ s t ) , but otherwise mimics π ′ s t , and extends if needed in some random-ass way, and is above π ′ s t . The reason we can do this is because, if there's a contradiction in this construction, it would be from π ′ s t and π i behaving differently on some prefix or extension of a string in F N F ( π s t ) ∩ F N F ( π ′ s t ) . But, π ′ s t can't specify what to do for any nodes in F N F ( π ′ s t ) or later (because F N F ( π ′ s t ) is basically a coat of leaf observation nodes around the extent of π ′ s t ), and if π ′ s t and π i differ on a strict prefix of something in F N F ( π s t ) ∩ F N F ( π ′ s t ) , then that means that π ′ s t and π s t branch different ways so there's no node in both F N F ( π s t ) and F N F ( π ′ s t ) after the branch point, so again we get a contradiction. Anyways, we've crafted our finitely many π ′ i which lie above π ′ s t , and mimic π i going forward. Our M ∞ i is an extension of M i whose measure component is only supported on F N F ( π s t ) ∩ F N F ( π ′ s t ) . Also, before and past that, π ′ i mimics π i perfectly, so we can transfer M ∞ i to Θ ω ( π ′ i ) by infinitary pseudocausality. Do this for all the i. Then, projecting all those down to π ′ s t , we get that all the M i lie in ↓ ( Θ ω ) ( π ′ s t ) , and mixing them together, we get that M itself lies in ↓ ( Θ ω ) ( π ′ s t ) . Now for part 2, where we show it for partial policies in general. Let M be arbitrary in ↓ ( Θ ω ) ( π p a ) . Project M down to all the π n p a to make a sequence of M n . Since the support of the measure component of M is a subset of F ( π p a ) ∩ F ( π ′ p a ) (pseudocausality assumption) the support of the measure component of M n is a subset of F ( π n p a ) ∩ F ( π ′ n p a ) , so by pseudocausality for stubs which we've shown, M n is also present in ↓ ( Θ ω ) ( π ′ n p a ) . Then, take the preimage in π ′ p a of all those M n points. By consistency and Lemma 9 and the usual argument about "there can only be one preimage point for a series of points", we get that M itself (the only thing that could project down on M n for all n) lies in ↓ ( Θ ω ) ( π ′ p a ) and we're done with pseudocausality. Alright, that's most of the proof out of the way, all that's left is showing that the full belief function conditions imply the finitary and infinitary versions, respectively, and getting isomorphism. Let's begin. Let's check whether → s t ( Θ ) makes a stub-hypothesis, and whether → ω ( Θ ) makes an infinitary-hypothesis, if Θ is a hypothesis/fulfills all the conditions. → s t is just "restrict Θ to only reporting sets for stubs", and → ω is just "restrict Θ to only reporting sets for full policies" The variants of nonemptiness, closure, convexity, nirvana-free upper-completion, bounded minimals, hausdorff-continuity, and pseudocausality for the finite and infinite case are trivially implied by the corresponding condition for hypotheses, leaving the four moderately nontrivial cases of the analogues of normalization, consistency, the extreme point condition, and causality. Extreme point condition: The infinitary case doesn't have an analogue of the extreme point condition. So that leaves the finitary case. What we can do is take a nirvana-free extreme minimal point M e x in some Θ ( π s t ) , apply the general extreme point condition to get a nirvana-free M ∈ Θ ( π ) for some suitable π that projects down to M e x , and, clipping away the infinite parts by → s t , the projections of M fill the role of the points in Θ ( π ′ s t ) all below some policy that project down to M e x . Causality. The finite case is that we can take a point associated with some stub, and craft an outcome function for stubs that matches up with our point. This is trivially implied by the general case of causality, where you can take any partial policy and point and get an outcome function that matches up with it. The infinite case is that we can take a point M in Θ ( π ) , and get points for all the other Θ ( π ′ ) that project down appropriately. For this, again, we just take an outcome function for M and clip it off to the infinite levels. Consistency: The finite case of weak consistency is pretty easy. We get p r π h i s t , π l o s t ∗ ( → s t ( Θ ) ( π h i s t ) ) = p r π h i s t , π l o s t ∗ ( Θ ( π h i s t ) ) ⊆ Θ ( π l o s t ) = ( → s t ) ( Θ ) ( π l o s t ) Where the subset came from full consistency because everything is the closed convex hull of projections from above, so projecting down gets you a subset. For the Nirvana-free consistency condition for the infinite case, it's a simple consequence of Lemma 21. Normalization: inf π s t E Θ ( π s t ) ( 0 ) = 0 and sup π s t E Θ ( π s t ) ( 1 ) = 1 To begin, the normalization condition for infinitary hypotheses and general hypotheses is the exact same, so we can ignore that and work on the stub hypothesis case. The inf one is pretty easy. From general normalization, at the infinite level, there are π and nirvana-free points in Θ ( π ) with a b value at-or-near zero, and you can just project them down to any stub you want. The sup one is a bit trickier. It's obviously not above 1, because no matter what policy π you pick, you've got a nirvana-free point with λ + b ≤ 1 in Θ ( π ) , which you can project down to whichever stub you're looking at, to certify that the expectation of 1 is 1 or less. Showing that it isn't below 1 is a bit harder. Let's say there's some π where min ( λ μ , b ) ∈ Θ ( π ) ∩ N F ( λ + b ) = 1 (or arbitrarily close to 1, doesn't really matter, although we'll show later that there is indeed a maximizing policy where Murphy can only force a value of 1) From Hausdorff-continuity, pick some δ to get an ϵ Hausdorff-distance or lower. This δ specifies some extremely large n, consider π n . Now, consider the set of every policy π ′ above π n . All of these are δ or less away from π . By Hausdorff-continuity, there can't be a nirvana-free point in any Θ ( π ′ ) with λ + b < 1 − ϵ , because we could do an ϵ perturbation to get a point in Θ ( π ) ∩ N F with λ + b < 1 , because small changes in M induce small changes in λ and b . Or, we can add a little bit of wiggle room if the minimizing value of λ + b in π is slightly less than 1 However, any nirvana-free point in Θ ( π n ) must originate as a mix of finitely many points from Θ ( π ′ i ) ∩ N F (varying π ′ i as long as it's above π n ) that have been projected down. This is because, by our earlier proof of nirvana-free consistency from consistency in general, Θ ( π n ) ∩ N F = c . h ( ⋃ π ′ ≥ π n p r π ′ , π n ∗ ( Θ ( π ′ ) ∩ N F ) ) All of these projected points have λ + b ≥ 1 − ϵ , so the mix point has λ + b ≥ 1 − ϵ , so Murphy can only force a value of 1 − ϵ or higher. And we can make δ as small as we wish to get a stub π n below π (n extremely large) where ϵ is as small as we wish, so the sup of the λ + b values Murphy can force over all stubs can't be below 1. So it must be 1. Isomorphism! Let's go! As a quick recap, ↓ ( Θ ω ) ( π p a ) : = ¯ ¯¯¯¯¯¯ ¯ c . h ( ⋃ π ≥ π p a p r π , π p a ∗ ( Θ ω ( π ) ) ) ↑ ( Θ s t ) ( π p a ) : = ⋂ π s t ≤ π p a ( p r π p a , π s t ∗ ) − 1 ( Θ s t ( π s t ) ) And → ω / → s t is just "clip down your hypothesis to full policies/stubs". So, two parts of this are trivially easy. From earlier in the proof (the start of the first section for the stub one, and an obvious corollary of definitions for the full policy one), we established that ↑ ( Θ s t ) ( π s t ) = Θ s t ( π s t ) and ↓ ( Θ ω ) ( π ) = Θ ω ( π ) . Using this, → ω ( ↓ ( Θ ω ) ) ( π ) = ↓ ( Θ ω ) ( π ) = Θ ω ( π ) and → s t ( ↑ ( Θ s t ) ) ( π s t ) = ↑ ( Θ s t ) ( π s t ) = Θ s t ( π s t ) So, → ω ( ↓ ( Θ ω ) ) = Θ ω and → s t ( ↑ ( Θ s t ) ) = Θ s t Let's get fancier and show the other two. ↓ ( → ω ( Θ ) ) ( π p a ) = ¯ ¯¯¯¯¯¯ ¯ c . h ( ⋃ π ≥ π p a p r π , π p a ∗ ( → ω ( Θ ) ( π ) ) ) = ¯ ¯¯¯¯¯¯ ¯ c . h ( ⋃ π ≥ π p a p r π , π p a ∗ ( Θ ( π ) ) ) = Θ ( π p a ) The first two equalities are unpacking definitions, the third is consistency for Θ . ↑ ( → s t ( Θ ) ) ( π p a ) = ⋂ π s t ≤ π p a ( p r π p a , π s t ∗ ) − 1 ( → s t ( Θ ) ( π s t ) ) = ⋂ π s t ≤ π p a ( p r π p a , π s t ∗ ) − 1 ( Θ ( π s t ) ) = Θ ( π p a ) Again, first two equalities are unpacking definitions, the third is consistency for Θ . So, ↓ ( → ω ( Θ ) ) = Θ and ↑ ( → s t ( Θ ) ) = Θ Putting it together, → ω and ↓ make an isomorphism between Θ ω and Θ , and → s t and ↑ make an isomorphism between Θ s t and Θ . We're finally done! Proposition 1: If Θ fulfills the causality condition, nonemptiness, closure, and convexity, then S Θ is a nonempty, closed, convex set of a-environments or a-survironments. Θ S Θ = Θ . Also, S ⊆ S Θ S . Ok, what S Θ is, is the set of a-environments ( λ e , b ) where, regardless of π p a , ( λ ( π p a ⋅ e ) , b ) lies in Θ ( π p a ) . For nonemptiness, pick some arbitrary point in one of your Θ ( π p a ) , use causality to get an outcome function, and then you fill in the conditional probabilities for an action-observation sequence with your outcome function points. This never produces a contradiction anywhere because if there was a contradiction, you'd be able to project two specified points down and have them disagree somewhere, which is impossible because we have an outcome function. For closure, if you take a limit of a-environments, this makes a limiting sequence in all the S Θ , which are all closed, so the limit point environment has all its induced distributions lying in the usual Θ ( π p a ) , and is in S Θ For convexity, if you take a mix of a-environments, this makes the same mix in all the S Θ which are all convex, so the mixed environment has all its induced distributions lying in the usual Θ ( π p a ) , and is in S Θ . For equality, if M ∈ Θ S Θ ( π p a ) , then it originated from some a-environment made from an outcome function for Θ , which... just gets your original point so M ∈ Θ ( π p a ) . In the other direction, if M ∈ Θ ( π p a ) , by causality, we can project down and extend the specification and make an a-environment that acts like M on π p a , and then going back gets you M ∈ Θ S Θ ( π p a ) . In the other direction, if ( λ e , b ) ∈ S , then it induces an outcome function and you can go back from that to ( λ e , b ) ∈ S Θ S , so S ⊆ S Θ S Theorem 3.1: Pseudocausal Translation: For all pseudocausal Θ s t hypotheses defined only  on policy stubs, → c ( Θ s t ) is a causal hypothesis only defined on policy stubs. → N F ( → c ( Θ s t ) ) = Θ s t . For all causal Θ s t hypotheses defined only on policy stubs, → N F ( Θ s t ) is a pseudocausal hypothesis only defined on policy stubs. Theorem 3.2: Acausal Translation: For all acausal Θ s t hypotheses defined only on policy stubs, → s c ( Θ s t ) is a surcausal hypothesis only defined on policy stubs. → N F ( → s c ( Θ s t ) ) = Θ s t . For all surcausal Θ s t hypotheses defined only on policy stubs, → N F ( Θ s t ) is an acausal hypothesis only defined on policy stubs. Both these theorems have highly similar proofs, so let's group them together. First, we'll need to set up how → c and → s c work, and then knock out two lemmas we'll need before we can proceed to the main result. → c is defined by → c ( Θ s t ) ( π s t ) = ¯ ¯¯¯¯¯¯ ¯ c . h ( ⋂ π ′ s t ≤ π s t ( I π ′ s t , π s t ∗ ( Θ s t ( π ′ s t ) ) ) ) → s c is defined identically, just with I ∗ s instead of I ∗ , and closed convex hull permitting us to mix with 0 + probability. I π ′ s t , π s t where π ′ s t ≤ π s t (this is like the inverse of projection, it's going up instead of down) is a function F ( π ′ s t ) → F ( π s t ) defined by: If h ∈ F ( π ′ s t ) ∩ F ( π s t ) , then I π ′ s t , π s t ( h ) = h . If h ∈ F ( π ′ s t ) and isn't in F ( π s t ) , then I π ′ s t , π s t ( h ) = h π s t ( h ) N I π ′ s t , π s t ∗ from M a ( F ( π ′ s t ) ) → M a ( F ( π s t ) ) is just pushing ( m , b ) through the mapping I π ′ s t , π s t . You keep the b term the same, and push the measure terms up. I π ′ s t , π s t ∗ s is defined identically on the measure part, except that it has the rule that all nirvana events in F ( π s t ) and not in F ( π ′ s t ) with 0 measure get 0 + measure instead. Intuitively, what I ∗ and I ∗ s are doing, is capping off whatever they need to (in order to extend appropriately) with Nirvana. I ∗ is capping off positive-probability histories with guaranteed Nirvana immediately afterwards, where I ∗ s is more paranoid and caps off every 0-probability Nirvana history that got added with "it is possible that Nirvana occurs here". Let's go over some properties that I ∗ and I ∗ s fulfill. I ∗ is an injective continuous map M a ( F ( π ′ s t ) ) → M a ( F ( π s t ) ) , and I ∗ s is an injective continuous map S M a ( F ( π ′ s t ) ) → S M a ( F ( π s t ) ) . I ∗ and I ∗ s are undone by projecting back down, p r π s t , π ′ s t ∗ ( I π ′ s t , π s t ∗ ( M ) ) = M . Both I ∗ and I s ∗ are linear, the latter in the stronger sense that it's linear when you mix stuff with 0 + probability, it doesn't matter whether you mix before or after injecting up. Further, injections up commute, I π ′ s t , π s t ∗ ( I π ′′ s t , π ′ s t ∗ ( M ) ) = I π ′′ s t , π s t ∗ ( M ) , and the same for I ∗ s . In order to make progress, we want to get two important lemmas. The first one, Lemma 22, is that slicing away the nirvana from this thing recovers the original pseudocausal hypothesis. The second one I call the "Diamond Lemma", and it says that injecting up and projecting down is the same as projecting down and then injecting up, and if you sketch it out, it looks like a diamond. Lemma 22: ( → c ( Θ s t ) ( π s t ) ) ∩ N F = Θ s t ( π s t ) , and the same holds for → s c . Proof sketch: One direction is trivial, the other direction that → c doesn't add any new nirvana-free points is trickier. Working in the pseudocausal-to-causal setting, we can take some M that's nirvana-free in the closed convex hull, and get a sequence M n limiting to it where each M n is in the convex hull. Now, indexing stubs below π s t by i, the M n can all be viewed as a mix of M i , n points projected up from below. The problem is, the mix varies as n does. What we can do is separate into "good" i where we can get a suitable limit point and limit probability, and "bad" i that we have to treat as a special chunk, and reexpress M as a sum of a probabilistic mix of "good" M i injected up, and an additional "bad" chunk. We can show that the "good" M i can all be transferred up to Θ s t ( π s t ) itself by pseudocausality and mixed in there, and the "bad" chunk is a nirvana-free a-measure. So, M is the sum of a point in Θ s t ( π s t ) , plus a nirvana-free a-measure, so M lies in Θ s t ( π s t ) by nirvana-free upper completion. Working in the surcausal-to-acausal setting, we take our M in the closed convex hull and a sequence M n , but injection up in this setting is much more effective at adding Nirvana, and the surmetric is much more sensitive than the usual metric for noticing the presence or absence of Nirvana. So, only an initial segment of M n is "contaminated" with Nirvana since the limit point is Nirvana-free, and we can clip that part off, and the "uncontaminated tail" can only have come from Θ s t ( π s t ) itself because injection up is very aggressive with adding Nirvana, so we get it from just closure on Θ s t ( π s t ) . Proof: For ( → c ( Θ s t ) ( π s t ) ) ∩ N F ⊇ Θ s t ( π s t ) , just observe that the identity injection I π s t , π s t ∗ leaves Θ s t ( π s t ) completely unchanged and adds no nirvana, so any point in Θ s t ( π s t ) also lies in the closed convex hull of the injections up, and is nirvana-free because the original point that we mapped through identity was nirvana-free. This works with the surcausal case too. Now for the considerably more difficult reverse direction, for the pseudocausal-to-causal case first. If M ∈ → c ( Θ s t ) ( π s t ) ∩ N F , then unpacking that, M is nirvana-free, and lies in the closed convex hull of the injections up. So, we can fix a sequence M n in the convex hull of injections up that limits to M . Index the stubs below π s t by i, there's only finitely many of them. The M n can be written as ∑ i ζ i , n I π i s t , π s t ∗ ( M i , n ) where M i , n ∈ Θ s t ( π i s t ) . ζ i , n may be 0. This is because, if there's multiple points in the injection of a particular stub that are mixed, you can mix them before injecting up to get a single M i , n that's injected up, because injections are linear and we're injecting a convex set. Blessed by the gift of finitely many i to worry about, use repeated picking of subsequences to get a subsequence of n where: For all i, ζ i , n converges. Call the limiting values ζ i . Now split the i into good i where ζ i > 0 , and bad i where ζ i = 0 . The ζ i will sum up to 1. For all good i, ζ i , n > 0 always. The fact that they all limit to above 0 helps you out because you only have to trim off an initial segment. For all good i, M i , n converges, call the limit point M i . This is because injection up preserves λ and b , and ζ i , n is bounded above 0, so the λ + b value of the M i , n is upper-bounded by λ ′ + b ′ min n ζ i , n , which is a finite nonnegative number divided by a finite positive number, and we can apply the Compactness Lemma to establish that a convergent subsequence exists. In this case, λ ′ + b ′ is the bound as a whole for the sequence M n , which converges so it must have a bound of that form, and not the bound on minimal points. Finally, ∑ bad i ( ζ i , n I π i s t , π s t ∗ ( M i , n ) ) converges. This is doable because the sequence M n has bounded λ and b because it converges to something, so the partial sum of bad i has the same bound, so we can invoke the Compactness Lemma to get our convergent subsequences. Putting all this together (we kept selecting from compact sets so that is what let us build a subsequence with all these great properties at once) we have a decomposition of M itself into: ∑ good i ( ζ i I π i s t , π s t ∗ ( M i ) ) + lim n → ∞ ( ∑ bad i ( ζ i , n I π i s t , π s t ∗ ( M i , n ) ) ) Now, since ∑ good i ζ i = 1 (all the bad i had their probability components limit to 0), that first sum part looks like an actual mixture of points injected up! Since M is nirvana-free, both parts must be nirvana-free, and the sums are also a-measures. First, by closure of all our original Θ s t ( π i s t ) , all the M i components (where i is good) do lie in Θ s t ( π i s t ) . And when we inject the M i up,  since the mix of them is nirvana-free, this means that each individual M i must be nirvana-free after injection. Now, what injection does, is it caps Nirvana on everything that is in F ( π i s t ) and not in F ( π s t ) that has positive probability. So, if M i is nirvana-free after injection, this must mean that its measure component is only supported on F ( π s t ) . Via pseudocausality, this means that M i lies in Θ s t ( π s t ) itself! Also, I π i s t , π s t ∗ ( M i ) = M i . So, our sum over good i components (by convexity), is actually a probabilistic mixture of stuff in Θ s t ( π s t ) itself! Abbreviating ∑ good i ζ i M i as M ′ , which lies in Θ s t ( π s t ) by convexity, and rewriting the sum, we can reepress M as: M ′ + lim n → ∞ ( ∑ bad i ( ζ i , n I π i s t , π s t ∗ ( M i , n ) ) ) This is a nirvana-free a-measure in Θ s t ( π s t ) , plus a nirvana-free a-measure, so, by nirvana-free upper-completion, M lies in Θ s t ( π s t ) and we're done. Now, let's hit up the surcausal case. Assume S M ∈ → N F ( Θ s t ) ( π s t ) ∩ N F . S M is nirvana-free, and lies in the closed convex hull of the injections up. So, we can fix a sequence S M n in the convex hull of injections up that limits to S M . Index the stubs below π s t by i, there's only finitely many of them, reserve i=0 for π s t itself. The S M n can be written as ∑ i ζ i , n I π i s t , π s t ∗ s ( M i , n ) where M i , n ∈ Θ s t ( π i s t ) . ζ i , n may be 0 or 0 + . This is because, if there's multiple points in the injection of a particular stub, you can mix them before injecting up to get your single point, one for each π i s t , because injections are linear and we're injecting a convex set. Note that S M is nirvana-free, and there's only finitely many spots where nirvana could be since we're working in a stub, so past a certain point all the S M n will be nirvana-free due to the surmetric we're using. Let's clip off that initial segment that's contaminated with Nirvana. Now, we can get something very interesting. If π i s t < π s t , then injecting up anything at all is going to stick nirvana (maybe with 0 + measure) somewhere . Having ζ i , n be 0 + doesn't help you, because mixing with a nirvana-containing thing with 0 + probability means the mixture contains the nirvana-spots of that thing you mixed in. So, past a certain point, all the S M n can only be written as M 0 , n (the identity injection, anything else either has exactly 0 probability so it gets clipped out of the sum, or it has Nirvana somewhere and can't be present). Therefore, in the tail, the sequence of S M n limiting to S M is the same as M 0 , n ∈ Θ s t ( π s t ) limiting to some M 0 ∈ Θ s t ( π s t ) , so S M ∈ Θ s t ( π s t ) and it's actually an a-measure, not an a-surmeasure. This establishes Lemma 22 for the sur-case. Lemma 23/Diamond Lemma: For any π s t , π ′ s t , and any π h i s t ≥ π s t , π ′ s t , and any M ∈ M a ( F ( π s t ) ) , then: p r π h i s t , π ′ s t ∗ ( I π s t , π h i s t ∗ ( M ) ) = I inf ( π s t , π ′ s t ) , π s t ∗ ( p r π s t , inf ( π s t , π ′ s t ) ∗ ( M ) ) (and same for I ∗ s and the sur variants) it's called the Diamond Lemma because if you sketch out the injections as going diagonally up and the projections as going diagonally down, the commutative diagram looks like a diamond. To begin with, we can go "hm, there's an upper bound on π s t and π ′ s t . For every finite history in F ( π s t ) , there's an extension of that history in F ( π h i s t ) , which has a prefix in F ( π ′ s t ) , and vice-versa. This establishes that for all the finitely many histories in F ( π s t ) , either a prefix of that history lies in F ( π ′ s t ) , or an extension of that history lies in F ( π ′ s t ) , and vice-versa for F ( π ′ s t ) Now, we can split into three possible cases and show that up-then-down equals down-then-up in terms of what measure is assigned to a history in F ( π ′ s t ) by mapping M through the injections and projections, which shows the diamond lemma in full generality. In the first case, our history h in F ( π ′ s t ) is also in F ( π s t ) (the equality case)In this case, h also lies in F ( inf ( π s t , π ′ s t ) ) . Projecting down to inf does nothing to the measure on h , and embedding up also does nothing to the measure on h . Embedding up to π h i s t also does nothing to the measure on h , and projecting down doesn't affect it either. In the second case, our history h in F ( π ′ s t ) isn't in F ( π s t ) , but there are strict extensions that lie in F ( π s t ) (this requires h to be nirvana-free). h is still assigned a measure by M , though, being a prefix of stuff with measure. In this case, h also lies in F ( inf ( π s t , π ′ s t ) ) . The same analysis from our first case works, h doesn't have its measure disrupted. In the third case, our history h in F ( π ′ s t ) isn't in F ( π s t ) , but a strict prefix h ′ lies in F ( π s t ) . We can distinguish three subcases. In the first subcase, h is of the form h ′ a N . In the second subcase, h still ends with Nirvana, but it isn't immediately after h ′ happens, some stuff happens in the meantime first. In the third subcase, h doesn't end with Nirvana. Also, h ′ lies in F ( inf ( π s t , π ′ s t ) ) . For the first subcase where h is of the form h ′ a N , injecting up means h ′ a N now has the measure originally associated with h ′ and nirvana is marked as "possible" there (if we're using the sur-injection). Projecting down leaves this alone. Projecting down leaves the measure on h ′ alone, and injecting up means h ′ a N now has the measure originally associated with h ′ and nirvana is marked as "possible" there (if we're using the sur-injection). In both paths, h ′ a N ends up with the measure that h ′ started with, and nirvana marked as "possible" in the sur-case. For the second subcase where h is of the form " h ′ , then some stuff happens, then Nirvana occurs", then in the causal case, the injection up would assign h 0 measure (all the measure of h ′ got channeled into h ′ a N instead of h ), and then projecting down, it stays the same. Similarly, projecting down means h ′ has some measure, then it's all channeled into h ′ a N on the injection up, so h itself gets 0 measure. For the surcausal case, the injection up assigns h 0 + measure (by the same argument and sur-injections tagging every freshly-added nirvana outcome with 0 + measure). and projecting down, it remains with 0 + measure. Projecting down leaves h ′ alone and then injecting up tags h with 0 + measure. For the third subcase where h is an extension of h ′ that doesn't add any Nirvana, we can run through the same argument as the second subcase to conclude that we get 0 measure for both the causal case and the surcausal one. Thus, no matter whether we inject up and project down, or project down and inject up, the measure assigned to h ∈ F ( π ′ s t ) by the measure component of the p-(sur)measure will agree. An important thing to note with this is that we can use any stub above π s t and π ′ s t for the injection up, but we must use inf ( π s t , π ′ s t ) for the projection down. Now, we can finally embark on the proof of the two translation theorems! There's enough similarities between the proofs that we can just do one big proof and remark on any differences we come across. The things we must show are that slicing off the Nirvana from a causal/surcausal hypothesis makes a pseudocausal/acausal hypothesis, and that adding in those injections up can turn a pseudocausal/acausal hypothesis to a causal/surcausal one, and that going nirvana-free to nirvana-containing back to nirvana-free is identity. Proof sketch: While at first this may look like the proof will be almost as long as the Isomorphism theorem because we're verifying a list of 9 conditions twice over, it'll actually be considerably shorter. The only nontrivial part of the first part where we check that slicing off the Nirvana makes a pseudocausal/acausal hypothesis is deriving pseudocausality from causality, and even that is fairly easy. Going from pseudocausal/acausal to causal/surcausal is trickier, though thankfully most conditions are trivially true, there's only three notable ones. There's the bound on minimal points, which is done by taking a sequence M n limiting to a M that violates the bound, using the definition of the causal translation to get a point below each M n which obeys the λ ⊙ + b ⊙ bound (fairly nontrivial), and appealing to Lemma 16 to construct a limit point below M that obeys the λ ⊙ + b ⊙ bound. Showing weak consistency (projecting down makes a subset) requires the Diamond Lemma to write the projection of your point of interest as a mix of injections up from below, and the last tricky one is causality. Which requires first showing that injecting up → c ( Θ s t ( π s t ) ) to a higher stub won't add any new points, and then coming up with a clever way of building our outcome function, and using the Diamond Lemma to show that it indeed an outcome function. Finally, nirvana-free to causal to nirvana-free is instant by Lemma 22. Proof: Referring back to the conditions for a hypothesis on policy stubs, we'll show that they're fulfilled when you slice away the Nirvana, and that pseudocausality can be derived from causality if we're just dealing with a-measures and not a-surmeasures. Stub Nirvana-free Nonemptiness was a property already possessed by the causal hypothesis, so it's preserved when we clip away the Nirvana. Stub closure and convexity also hold because we're intersecting with the closed convex set (of nirvana-free a-measures). Nirvana-free upper completion also holds. Bounded minimals holds because a minimal point in the Nirvana-free part must also be a minimal point in the original set, because adding anything to a nirvana-containing a-measure makes a nirvana-containing a-measure, so there can be no nirvana below our minimal point in the nirvana-free part, so it's minimal in general. Normalization holds because the expectation values only depend on the nirvana-free part. Nirvana-free stuff projects down to nirvana-free stuff, getting stub-consistency. The stub extreme point condition carries over due to the preexisting intersection with nirvana-free used to define it, and the same applies to uniform continuity. This wraps up surcausal-to-acausal, and just leaves deriving pseudocausality from causality for causal-to-pseudocausal. Let's say we have a nirvana-free M ∈ Θ s t ( π s t ) , where the measure part of M is supported over F ( π ′ s t ) , and we want to show that it's also present in Θ s t ( π ′ s t ) . Then M is present in Θ s t ( inf ( π s t , π ′ s t ) ) , because it's supported entirely over histories that both the different policies produce, so it's supported over histories that the intersection of the policies produces, just project it down to the inf. Now, by causality, we can find something in Θ s t ( π ′ s t ) that projects down onto M , which must be M itself because the measure part of M is supported entirely on histories in F ( π ′ s t ) . This gets pseudocausality. Now, let's show that → c ( Θ s t ) and → s c ( Θ s t ) fulfill the defining conditions for finitary causal. 1: Stub Nirvana-Free Nonemptiness: This one is trivial, because Θ s t ( π s t ) is present as a subset via the identity injection I π s t , π s t ∗ , and is nirvana-free. 2,3: Stub Closure/Convexity: We took a closed convex hull, these are tautological. 4: Stub Nirvana-Free Upper-Completeness: Just apply Lemma 22 to get that the nirvana-free part of → c ( Θ s t ) ( π s t ) (and same with surcausal) is just the original Θ s t ( π s t ) , which is nirvana-free and upper-complete by stub nirvana-free upper-completeness, so we're good there. Condition 5: Stub Bounded Minimals: By stub bounded minimals on the Θ s t we have a λ ⊙ + b ⊙ bound on λ + b for minimal points in Θ s t ( π s t ) for all stubs. Pick a M ∈ → c ( Θ s t ) ( π s t ) (or the surcausal analogue) with λ + b > λ ⊙ + b ⊙ . There's a sequence of points M n limiting to M that lie in the convex hull. All these M n (or S M n ) can be written as ∑ i ζ i , n I π i s t , π s t ∗ ( M i , n ) where M i , n ∈ Θ s t ( π i s t ) . Decompose M i , n into a minimal point and something else, getting M min i , n + M ∗ i , n . Then, do a further rewrite as M min i , n + ( m ∗ − i , n , − m ∗ − i , n ( 1 ) ) + ( m ∗ + i , n , b ∗ i , n + m ∗ − i , n ( 1 ) ) Note that the λ + b value of the sum of the first two terms is bounded above by λ ⊙ + b ⊙ , because M min i , n obeys that bound, and for the second term, it deletes exactly as much from the λ term as it adds to the b term. Also, since M i , n is an a-measure, adding in just the negative component to M min i , n doesn't make it go negative anywhere, so the sum of the first two terms is an a-measure, and by nirvana-free upper completeness, it lies in Θ s t ( π i s t ) . The third component of the sum is also an a-measure. By linearity of I ∗ or I ∗ s , injecting up the first two terms and the last term, and adding them afterwards, is the same as injecting up the bulk of them (we can only inject up a-measures). Let's abbreviate M min i , n + ( m ∗ − i , n , − m ∗ − i , n ( 1 ) ) as M ◊ i , n and abbreviate ( m ∗ + i , n , b ∗ i , n + m ∗ − i , n ( 1 ) ) as M ♡ i , n Now, we can rewrite M n as: M n = ∑ i ζ i , n I π i s t , π s t ∗ ( M i , n ) = ∑ i ( ζ i , n I π i s t , π s t ∗ ( M ◊ i , n ) ) + ∑ i ( ζ i , n I π i s t , π s t ∗ ( M ♡ i , n ) ) The first component is in → c ( Θ s t ) ( π s t ) and has the λ ⊙ + b ⊙ bound (injection up preserves λ and b ) and M ◊ i , n lies below the λ ⊙ + b ⊙ bound, and mixing stuff below the bound produces a point below the bound. Abbreviate the first component as M ◊ n . So, M n isn't minimal, it lies above M ◊ n . Because the M ◊ n have a λ ⊙ + b ⊙ bound, and there's only finitely many places where nirvana could be, we can extract a convergent subsequence, limiting to some M ◊ which obeys the λ ⊙ + b ⊙ bound, and by Lemma 16, M ◊ lies below M . Therefore, M isn't minimal, and it was an arbitrary point that violated the λ ⊙ + b ⊙ bound, so all minimal points in any → c ( Θ s t ) ( π s t ) obey the same λ ⊙ + b ⊙ bound, and we get bounded minimals. 6: Stub Normalization. By Lemma 22, we didn't introduce any new nirvana-free points, so stub normalization of Θ s t carries over. Condition 7: Weak Consistency. This is "projecting down makes a subset". All the following arguments work with sur-stuff. Fix some M ∈ → c ( Θ s t ) ( π h i s t ) . It's a limit of points M n in the convex hull. We can decompose the M n as ∑ i ζ i , n I π i s t , π h i s t ∗ ( M i , n ) . Then, p r π h i s t , π l o s t ∗ ( M n ) = p r π h i s t , π l o s t ∗ ( ∑ i ζ i , n I π i s t , π h i s t ∗ ( M i , n ) ) = ∑ i ζ i , n p r π h i s t , π l o s t ∗ ( I π i s t , π h i s t ∗ ( M i , n ) ) Which, by the Diamond Lemma, can be rewritten as: ∑ i ζ i , n I inf ( π l o s t , π i s t ) , π l o s t ∗ ( p r π i s t , inf ( π l o s t , π i s t ) ∗ ( M i , n ) ) The projections of the M i , n ∈ Θ s t ( π i s t ) lie in Θ s t ( inf ( π l o s t , π i s t ) ) by weak consistency for Θ s t . So, actually, the projection of M n down to π l o s t can be written as a mix of injections up from stubs below π l o s t , so the projection of M n lies in → c ( Θ s t ) ( π s t ) . Then, just use continuity of projection, and closure, to get M itself projecting down into → c ( Θ s t ) ( π l o s t ) , so we're good on weak consistency. 8: Stub Extreme Point Condition: By Lemma 22, we didn't introduce any new nirvana-free points, so any nirvana-free extreme minimal point was present (and nirvana-free extreme minimal) in Θ s t already, so the extreme point condition carries over from there. 9: Stub Hausdorff Continuity: By Lemma 22, we didn't introduce any new nirvana-free points, and the preimages for Hausdorff continuity are of the nirvana-free parts, so this is completely unaffected and carries over. Condition C: Causality: As a warmup to this result, we'll show that if π s t ≤ π h i s t , then I π s t , π h i s t ∗ ( → c ( Θ s t ) ( π s t ) ) ⊆ → c ( Θ s t ) ( π h i s t ) Pick a M ∈ → c ( Θ s t ) ( π s t ) . It's a limit of M n which are finite mixtures of injections of stuff from below, and M n can be written as ∑ i ζ i , n I π i s t , π s t ∗ ( M i , n ) Then, I π s t , π h i s t ∗ ( M n ) = I π s t , π h i s t ∗ ( ∑ i ζ i , n I π i s t , π s t ∗ ( M i , n ) ) = ∑ i ζ i , n I π s t , π h i s t ∗ ( I π i s t , π s t ∗ ( M i , n ) ) And then, by commutativity of injections, the injection of M n rewrites as ∑ i ζ i , n I π i s t , π h i s t ∗ ( M i , n ) All the π i s t are below π s t so they're also under π h i s t , witnessing that the injection of M n lies in → c ( Θ π s t ( π h i s t ) ) . Then, just appeal to closure and continuity of I ∗ or I ∗ s to get that M injects up into → c ( Θ π s t ( π h i s t ) ) Again, all this stuff works for the sur-situation as well. With this out of the way, fix some M ∈ → c Θ π s t ( π s t ) . Let's try to make an outcome function from this, shall we? Let's do o f ( π ′ s t ) : = I inf ( π s t , π ′ s t ) , π ′ s t ∗ ( p r π s t , inf ( π s t , π ′ s t ) ∗ ( M ) ) Yup, that does indeed specify one point for everything. It obviously spits out M when you feed π s t in because both the injection and projection turn into identity. Further, by weak-consistency, the projection of M down lies in → c ( Θ π s t ) ( inf ( π s t , π ′ s t ) ) , and by our freshly-proved result, injecting up lands you in → c ( Θ s t ) ( π ′ s t ) . So, all that's left is showing that it's an outcome function! That, for any two π h i s t and π l o s t where π h i s t ≥ π l o s t , that p r π h i s t , π l o s t ∗ ( o f ( π h i s t ) ) = o f ( π l o s t ) Let's begin. p r π h i s t , π l o s t ∗ ( o f ( π h i s t ) ) = p r π h i s t , π l o s t ∗ ( I inf ( π s t , π h i s t ) , π h i s t ∗ ( p r π s t , inf ( π s t , π h i s t ) ∗ ( M ) ) ) And then, by the Diamond Lemma, this equals I inf ( π l o s t , inf ( π s t , π h i s t ) ) , π l o s t ∗ ( p r inf ( π s t , π h i s t ) , inf ( π l o s t , inf ( π s t , π h i s t ) ) ∗ ( p r π s t , inf ( π s t , π h i s t ) ∗ ( M ) ) ) And then, inf ( π l o s t , inf ( π s t , π h i s t ) ) = inf ( π l o s t , π h i s t , π s t ) = inf ( π s t , π l o s t ) because π l o s t ≤ π h i s t . Rewriting a bit, and grouping the two projections together because they commute, we have: I inf ( π s t , π l o s t ) , π l o s t ∗ ( p r π s t , inf ( π s t , π l o s t ) ∗ ( M ) ) = o f ( π l o s t ) And we're finally done with everything, we showed causality. Lemma 24: Given a Θ (maybe just defined on stubs or full policies) that fulfills all hypothesis conditions except normalization, if it's normalizable, then all belief-function conditions are preserved (works in the sur-case too) Nirvana-free nonemptiness, closure, convexity, nirvana-free upper-completion, and bounded-minimals are all obviously preserved by scale-and-shift/Proposition 7 in section 1 of proofs. For consistency, due to projections preserving λ and b , the scale-and-shift in the stubs (or full policies) is perfectly reflected in whichever partial policy you're evaluating, so consistency holds too. For the extreme point condition, any nirvana-free minimal extreme point post-renormalization is also nirvana-free minimal extreme pre-renormalization, so we can undo the renormalization, get a point in the nirvana-free component of a full policy that projects down accordingly, and scale-shift that point to get something that projects down to the scale-shifted extreme point. For Hausdorff-continuity, the scaling just scales the distance between two sets by the scale term, so Hausdorff-continuity carries over. Pseudocausality is preserved by normalization (un-normalize, transfer over to the partial policy of interest, then normalize back again), and so is causality (unnormalize the point and outcome function, complete it, normalize your batch of points back again). Proposition 2: Given a nirvana-free Θ ? ω , the minimal constraints we must check of Θ ? ω to turn it into an acausal hypothesis are: Nonemptiness, Restricted Minimals, Hausdorff-Continuity, and non-failure of renormalization. Every other constraint to make a hypothesis can be freely introduced. Ok, we have our Θ ? ω , and we want to produce an acausal hypothesis. The obvious way to do it is: Θ ω ( π ) = ( ¯ ¯¯¯¯¯¯ ¯ c . h ( Θ ? ω ( π ) ) u c ) R We'll use Θ ω , ¬ R ( π ) to refer to the set before renormalization. Proof sketch: We basically just run through the infinitary hypothesis conditions, and show that they're fulfilled by Θ ω , ¬ R , and then appeal to Lemma 24 that we didn't destroy our hard work when we normalize. As for the hypothesis conditions themselves, they're all pretty simple to show except for bounded-minimals and Hausdorff-continuity, which is where the bulk of the work is. 1: Nirvana-free nonemptiness. Trivial, because all the Θ ? ω ( π ) are nonempty. 2: Closure: Appeal to Lemma 2, not in this document, but of section 1 in basic inframeasure theory, that the upper completion of a closed set is closed. Then we just intersect with the cone of a-measures (closed) to get our set of interest, so it's closed. 3: Convexity: If you have M and M ′ which decompose into M l o + M ∗ and M ′ l o + M ′ ∗ ( M and M ′ lie in the upper completion and M l o / M ′ l o lie in the closed convex hull), then p M + ( 1 − p ) M ′ = p ( M l o + M ∗ ) + ( 1 − p ) ( M ′ l o + M ′ ∗ ) = ( p M l o + ( 1 − p ) M ′ l o ) + ( p M ∗ + ( 1 − p ) M ′ ∗ ) The first component lies in the closed convex hull because it's a mix of two points from the closed convex hull, the second component is an sa-measure, and by our upper completion, then p M + ( 1 − p ) M ′ lies in our Θ ω , ¬ R ( π ) 4: Nirvana-free Upper-completeness: Trivial, we took the upper completion. Condition 5: Bounded Minimals: This can be shown by demonstrating that, if λ ⊙ + b ⊙ is our bound for Θ ? ω (ie, every point in Θ ? ω ( π ) , regardless of π , either respects the bound or lies strictly above a point in Θ ? ω ( π ) that respects the bound), then every point in Θ ω , ¬ R ( π ) lies above a point that obeys the λ ⊙ + b ⊙ bound. Take a point M ∈ ¯ ¯¯¯¯¯¯ ¯ c . h ( Θ ? ω ( π ) ) . We don't have to worry about points in the upper completion that weren't part of the original closed convex hull, because they're above something in the closed convex hull, so we just have to show that everything in the closed convex hull lies above something that respects the bounds. M can be written as a limit of points M n ∈ c . h ( Θ ? ω ( π ) ) , which split into a mixture of finitely many M i , n ∈ Θ ? ω ( π ) . We can then split the M i , n into M l o i , n + M ∗ i , n , where M l o i , n respects the appropriate bounds (everything in Θ ? ω ( π ) either obeys the bounds or is above a point which obeys the bounds). Now, we can rewrite M n as: ∑ i ζ i , n ( M l o i , n ) + ∑ i ζ i , n ( M ∗ i , n ) That first sum term is a mixture of stuff that respects the λ ⊙ + b ⊙ bound, so it respects the same property and lies below M n by the addition of the second term making M n . All these lower points lie in the closed convex hull, and obey the λ ⊙ + b ⊙ bound, so there's a convergent subsequence that limits to some limit point that's also in the closed convex hull, respects the λ ⊙ + b ⊙ bound, and by Lemma 16, is below M . So, any M ∈ Θ ω , ¬ R ( π ) , regardless of π , which violates the λ ⊙ + b ⊙ bound on minimal points, has a point lower than it which does respect the bound, showing Minimal Boundedness. We normalize at the end, and need uniform Hausdorff Continuity to show nirvana-free consistency, so let's skip to that one, which is hard. Condition 8: Uniform Hausdorff Continuity: We'll be working with the Lemma 15 variant of Hausdorff-continuity, that given any ϵ , there's a δ where two policies π , π ′ being δ apart or less guarantees that if M is in Θ ? ω ( π ) , then there's a point M ′ in Θ ? ω ( π ′ ) that's only ϵ ( 1 + λ ) away, where λ is the λ value associated with M , and establish that variant for Θ ω , ¬ R . Fix an ϵ . How close do two policies have to be to guarantee that for any M ∈ Θ ω , ¬ R ( π ) , there's a point M ′ in Θ ω , ¬ R ( π ′ ) within ϵ ( 1 + λ ) ? Well, for our original Hausdorff-continuity condition, pick a δ that forces a ϵ 2 ( 1 + λ ⊙ + b ⊙ ) "distance", and δ < ϵ 2 ( 1 + λ ⊙ + b ⊙ ) . Since we've got closure and bounded-minimals, write M as M l o + M ∗ where M l o respects the λ ⊙ + b ⊙ bound, and it lies in the closed convex hull and is a limit of M l o n points, which decompose into a mixture of finitely many M l o i , n points. Now, each of these M l o i , n points in Θ ? ω ( π ) , by Hausdorff-continuity of Θ ? ω , have a M ′ l o i , n point in Θ ? ω ( π ′ ) , that's only ϵ 2 ( 1 + λ ⊙ + b ⊙ ) ( 1 + λ i , n ) away, by π ′ and π being δ or less apart. We can mix the M ′ l o i , n in the same way as usual to make a M ′ l o n that's only ϵ 2 ( 1 + λ ⊙ + b ⊙ ) ( 1 + λ n ) or less away from M l o n Because the M l o n sequence converges, there's some bound on the λ n and b n values, and the (at most) ϵ 2 ( 1 + λ ⊙ + b ⊙ ) ( 1 + max n ( λ n ) ) change to make M ′ l o n still keeps the λ and b values of our new sequence bounded, so by the Compactness Lemma, the M ′ l o n sequence has a convergent subsequence, with a limit point M ′ l o , that lies in Θ ω , ¬ R ( π ′ ) by closure. Also, for all n, d ( M l o , M ′ l o ) ≤ d ( M l o , M l o n ) + d ( M l o n , M ′ l o n ) + d ( M ′ l o n , M ′ l o ) The two distances on either side limit to 0, and the middle distance limits to ϵ 2 ( 1 + λ ⊙ + b ⊙ ) ( 1 + λ ⊙ + b ⊙ ) or less, because eventually the λ value of M l o n gets really close to the λ value of M l o , which is subject to the constraint that it can't be bigger than λ ⊙ + b ⊙ due to M l o being picked to  have its λ + b value below λ ⊙ + b ⊙ , so d ( M l o , M ′ l o ) ≤ ϵ 2 Ok, so those two things are pretty close to each other. But what we really want is to find a point in Θ ω , ¬ R ( π ′ ) that's close to M , ie, M l o + M ∗ . We can invoke the proof path from direction 2 of Lemma 15 (we have enough tools to do it, most notably upper completion) to craft a M ′ ∈ Θ ω , ¬ R ( π ′ ) where d ( M , M ′ ) ≤ ϵ 2 + δ ( ϵ 2 + λ ) Further, δ < ϵ 2 ( 1 + λ ⊙ + b ⊙ ) ≤ ϵ 2 . So, we get d ( M , M ′ ) ≤ ϵ ( 1 + λ ) and we're done. 7: Nirvana-free Consistency: We're working in a nirvana-free setting, so we can simplify things. Our formulation that we're going to show is, regardless of stub π s t , c . h ( ⋃ π > π s t p r π , π s t ∗ ( Θ ω ( π ) ) ) is closed. Just invoke Lemma 20 and we have it and that's the last one we needed besides renormalization. Now all we have to do is to show that every property is preserved when we do the necessary rescaling. Invoke Lemma 24. Proposition 3: Given some arbitarary Θ ? ω which can be turned into an acausal hypothesis, turning it into Θ ω has E Θ ω ( π ) ( f ) = α ( E Θ ? ω ( π ) ( f ) − β ) for all π and f . The steps to make your full Θ ω are convex hull, closure, upper completion, and renormalization. For convex hull, because f induces a positive functional, which is linear, convex hull doesn't affect the worst-case value (mixing a-measures mixes the score you get w.r.t the function), closure just swaps inf out for min, and upper completion doesn't add any new minimal points so it preserves the same minimal values for everything. Let's use Θ ω , ¬ R for the upper completion of the closed convex hull (no renormalization) so, unpacking definitions, for all π , f : E Θ ? ω ( π ) ( f ) = inf ( m , b ) ∈ Θ ? ω ( π ) ( m ( f ) + b ) = inf ( m , b ) ∈ c . h ( Θ ? ω ( π ) ) ( m ( f ) + b ) = inf ( m , b ) ∈ ¯ ¯¯¯¯ ¯ c . h ( Θ ? ω ( π ) ) ( m ( f ) + b ) = inf ( m , b ) ∈ ( ¯ ¯¯¯¯ ¯ c . h ( Θ ? ω ( π ) ) ) u c ( m ( f ) + b ) = E Θ ω , ¬ R ( π ) ( f ) Continuing onwards, let's use β for our shift constant and α for our rescaling constant. E Θ ω ( π ) ( f ) = inf ( m , b ) ∈ Θ ω ( π ) ( m ( f ) + b ) = inf ( m , b ) ∈ Θ ω , ¬ R ( π ) ( α m ( f ) + α ( b − β ) ) = α ( ( inf ( m , b ) ∈ Θ ω , ¬ R ( π ) ( m ( f ) + b ) ) − β ) = α ( E Θ ω , ¬ R ( π ) ( f ) − β ) So, regardless of your π and f , E Θ ω ( π ) ( f ) = α ( E Θ ? ω ( π ) ( f ) − β ) and we're done. In particular, since this scale and shift is completely uniform across everything, it keeps the set of optimal policies unchanged. Proposition 4: For all hypotheses Θ and Θ ′ , ( ∀ π , f : E Θ ( π ) ( f ) = E Θ ′ ( π ) ( f ) ) ↔ ( → N F ( Θ ) = → N F ( Θ ′ ) ) We can use that Murphy never picks something with Nirvana in it, and → N F ( Θ ) ( π ) = Θ ( π ) ∩ N F to rewrite our desired property as: ( ∀ π , f : E → N F ( Θ ) ( π ) ( f ) = E → N F ( Θ ′ ) ( π ) ( f ) ) ↔ ( → N F ( Θ ) = → N F ( Θ ′ ) ) one direction of this is pretty easy, if the belief functions are identical when you slice off the nirvana, then regardless of π and f , Murphy forces the same value. The other direction of this can be done by Theorem 3 from Section 1. Fixing a π , the property ∀ f : E → N F ( Θ ) ( π ) ( f ) = E → N F ( Θ ′ ) ( π ) ( f ) implies → N F ( Θ ) ( π ) = → N F ( Θ ′ ) ( π ) , but this holds for all π , so we get → N F ( Θ ) = → N F ( Θ ′ ) Proposition 5: For all hypotheses Θ , and all continuous functions g from policies to functions f ∈ C ( ( A × O ) ω , [ 0 , 1 ] ) , then argmax π E Θ ( π ) ( g ( π ) ) exists and is closed. Proof sketch: We'll prove this in four phases, where π n is some arbitrary sequence of policies limiting to the policy π . Our first phase will be establishing that lim n → ∞ | E Θ ( π n ) ( g ( π n ) ) − E Θ ( π n ) ( g ( π ) ) | = 0 Our second phase will be establishing that lim sup n → ∞ E Θ ( π n ) ( g ( π ) ) ≤ E Θ ( π ) ( g ( π ) ) Our third phase will be establishing that lim inf n → ∞ E Θ ( π n ) ( g ( π ) ) ≥ E Θ ( π ) ( g ( π ) ) Putting phase 2 and 3 together, lim n → ∞ E Θ ( π n ) ( g ( π ) ) = E Θ ( π ) ( g ( π ) ) and then, in conjunction with phase 1, lim n → ∞ E Θ ( π n ) ( g ( π n ) ) = E Θ ( π ) ( g ( π ) ) This establishes that the function π ↦ E Θ ( π ) ( g ( π ) ) is continuous. Phase 4 is then deriving our desired result from the continuity of that function. Begin phase 1. To begin with, because g is a function from a compact metric space to a metric space, by the Heine-Cantor theorem, it's uniformly continuous. So, there's some δ difference between policies that guarantees an ϵ difference between the functions produced, and our distance metric on functions in this case is sup x | f ( x ) − f ′ ( x ) | , the distance metric associated with uniform convergence. By Proposition 3 (Section 1), every positive functional (and, by Proposition 1 (Section 1), continuous functions induce positive functionals) is minimized within the set of minimal points, so we can fix an ( m , b ) and ( m ′ , b ′ ) within ( Θ ( π n ) ) min (specifically, the nirvana-free component) which minimize the positive functionals associated with g ( π n ) and g ( π ) , respectively. Being able to get an actual minimizing point follows from minimal-boundedness, so the closure of the set of minimal points (due to having bounds) is compact, and a continuous function from a compact set to [ 0 , 1 ] has an actual minimizing point, so we can pick such a point and then step down to a minimal point if needed Note that, by the way we picked these, m ( g ( π n ) ) + b = E Θ ( π n ) ( g ( π n ) ) and also m ′ ( g ( π ) ) + b ′ = E Θ ( π n ) ( g ( π ) ) First, we'll bound the following two terms. | ( m ( g ( π n ) ) + b ) − ( m ( g ( π ) ) + b ) | and | ( m ′ ( g ( π ) ) + b ′ ) − ( m ′ ( g ( π n ) ) + b ′ ) | The same arguments work for both, so we'll just show one of them. | ( m ( g ( π n ) ) + b ) − ( m ( g ( π ) ) + b ) | = | m ( g ( π n ) − g ( π ) ) | ≤ m ( | g ( π n ) − g ( π ) | ) ≤ λ ⊙ ϵ n The argument for this is that the first ≤ is because m is a measure (never negative), so an upper-bound on the absolute value of the expectation is given by the expectation of the absolute value of the distance between the two functions. For the second ≤ , if n is large enough to make π n and π be only δ n apart, then g ( π n ) and g ( π ) are only ϵ n apart, so that absolute value is upper bounded by ϵ n , getting us an upper bound of m ( ϵ n ) Then, because the total amount of measure for minimal points is upper bounded by some λ ⊙ regardless of which policy we picked (minimal-point boundedness for Θ ), we can finally impose an upper bound of ϵ n λ ⊙ on the distance. The sort of argument works for the second thing, and gets us the exact same upper bound. Further, m ( g ( π n ) ) + b ≤ m ′ ( g ( π n ) ) + b ′ and m ′ ( g ( π ) ) + b ′ ≤ m ( g ( π ) ) + b . This is because ( m , b ) is specialized to minimize g ( π n ) and ( m ′ , b ′ ) is specialized to minimize g ( π ) . Therefore, in one direction: ( m ′ ( g ( π ) ) + b ′ ) − ( m ( g ( π ) ) + b ) ≤ 0 so ( m ′ ( g ( π ) ) + b ′ ) − ( m ( g ( π n ) ) + b ) ≤ ϵ n λ ⊙ In the other direction, ( m ( g ( π n ) ) + b ) − ( m ′ ( g ( π n ) ) + b ′ ) ≤ 0 so ( m ( g ( π n ) ) + b ) − ( m ′ ( g ( π ) ) + b ′ ) ≤ ϵ n λ ⊙ Thus, putting the two parts together, | E Θ ( π n ) ( g ( π n ) ) − E Θ ( π n ) ( g ( π ) ) | = | ( m ( g ( π n ) ) + b ) − ( m ′ ( g ( π ) ) + b ′ ) | ≤ ϵ n λ ⊙ We can make n go to infinity, which makes δ n (distance between policies) go to 0, which makes ϵ n (distance between functions) go to 0, and λ ⊙ is a constant, so we get that the distance between the two expectations limits to 0 and we're done with the first phase. Time for phase 2, showing that lim sup n → ∞ E Θ ( π n ) ( g ( π ) ) ≤ E Θ ( π ) ( g ( π ) ) Fix some ( m , b ) in Θ ( π ) ∩ N F that minimizes the positive functional associated with g ( π ) . By Hausdorff-Continuity, we can find a sequence of points ( m n , b n ) ∈ Θ ( π n ) ∩ N F that limit to ( m , b ) . By continuity of g ( π ) , this means that m n ( g ( π ) ) + b n limits to m ( g ( π ) ) + b , which is E Θ ( π ) ( g ( π ) ) . However, m n ( g ( π ) ) + b n ≥ E Θ ( π n ) ( g ( π ) ) Thus, lim sup n → ∞ E Θ ( π n ) ( g ( π ) ) ≤ E Θ ( π ) ( g ( π ) ) Now for phase 3, showing that lim inf n → ∞ E Θ ( π n ) ( g ( π ) ) ≥ E Θ ( π ) ( g ( π ) ) Assume it's false, we'll get a proof-by contradiction. That is, lim inf n → ∞ E Θ ( π n ) ( g ( π ) ) < E Θ ( π ) ( g ( π ) ) Then, we can get some subsequence where the expectations converge to the liminf. For each n in that subsequence, fix a ( m n , b n ) ∈ ( Θ ( π n ) ) min ∩ N F that minimizes the positive functional associated with g ( π ) within Θ ( π n ) . Ie, m n ( g ( π ) ) + b n = E Θ ( π n ) ( g ( π ) ) By bounded minimals, there's some λ ⊙ + b ⊙ bound on all of these, so we can isolate another convergent subsequence (the expectation values still limit to the liminf), where the ( m n , b n ) limit to some ( m , b ) . For the following arguments, we'll use n to denote numbers from our original sequence (ranges over all natural numbers) and j to denote numbers from our convergent subsequence of interest (where the expectations converge to liminf and our sequence of minimizing points converges to a limit point) First, this ( m , b ) limit point lies in Θ ( π ) , because it's arbitrarily close to points that are arbitrarily close to Θ ( π ) (Hausdorff-continuity), so the distance to that set shrinks to 0, and Θ ( π ) is closed so said point limits to be in it. Now, we can go lim inf n → ∞ E Θ ( π n ) ( g ( π ) ) = lim j → ∞ ( m j ( g ( π ) ) + b j ) = m ( g ( π ) ) + b By m j ( g ( π ) ) + b j = E Θ ( π j ) ( g ( π ) ) , and the j's making a subsequence where we attain the liminf value in the limit, and then the second equality is a convergent sequence of a-measures having their expectation value limit to the expectation value of the limit point. But then we get something impossible. ( m , b ) ∈ Θ ( π ) , and yet somehow (by our original assumption that the liminf undershot the expectation value of g ( π ) in Θ ( π ) ), m ( g ( π ) ) + b = lim inf n → ∞ E Θ ( π n ) ( g ( π ) ) < E Θ ( π ) ( g ( π ) ) = min ( m ′ , b ′ ) ∈ Θ ( π ) ∩ N F ( m ′ ( g ( π ) ) + b ′ ) Which cannot be. This shows that the liminf is ≥ E Θ ( π ) ( g ( π ) ) . Now for phase 4. Again, from the proof sketch, phases 1, 2, and 3 establish that π ↦ E Θ ( π ) ( g ( π ) ) is a continuous function Π → R ≥ 0 Let's abbreviate this function as χ . Since we're mapping Π (which is compact) through a continuous function, the image χ ( Π ) is compact. Thus, it has a maximum value, which is attained by some policy. Take that maximum value (it's a single point so it's closed), take the preimage (which is a nonempty closed set of policies), and that's your argmax π E Θ ( π ) ( g ( π ) ) set. And E Θ ( π ) ( g ( π ) ) unpacks as min ( m , b ) ∈ Θ ( π ) ∩ N F ( m ( g ( π ) ) + b ) Thus showing our result. A quick corollary of it is, if g just returns the constant 1 function, you can find a policy π where min ( λ μ , b ) ∈ Θ ( π ) ( λ + b ) is 1, by normalization, so we can use max in normalization instead of sup . Lemma 25: In the nirvana-free setting, with all the B i ⊆ F N F ( π p a ) being nonempty and upper complete, then E ζ B i is upper-complete. The proof of this is nearly identical to the proof of Lemma 12. Except in this case, our M i aren't finitely many points selected from a nonconvex B , they're countably many points selected from the various B i . Apart from that difference, the proof path works as it usually does. Lemma 26: A belief function Θ fulfilling all conditions except normalization, which is renormalized by subtracting inf π E Θ ( π ) ( 0 ) and scaling by ( sup π E Θ ( π ) ( 1 ) − inf π E Θ ( π ) ( 0 ) ) − 1 , only has the renormalization fail if, for all π , Θ ( π ) ∩ N F has a single minimal point of the form ( 0 , b ) , with the same b for all π . (works in the sur-case too) First, fixing an arbitrary π ′ , then, if there's a divide-by-zero when scaling, E Θ ( π ′ ) ( 1 ) ≤ max π E Θ ( π ) ( 1 ) = min π E Θ ( π ) ( 0 ) ≤ E Θ ( π ′ ) ( 0 ) However, E Θ ( π ′ ) ( 1 ) ≥ E Θ ( π ′ ) ( 0 ) always, so the two terms are equal. Now, just invoke Proposition 6 from Section 1, which says that if E Θ ( π ′ ) ∩ N F ( 1 ) = E Θ ( π ′ ) ∩ N F ( 0 ) , then there's a single minimal point of the form ( 0 , b ) , and the b is E Θ ( π ′ ) ∩ N F ( 0 ) , which is the same for all policies π ′ . The converse is, if all Θ ( π ) are of this form, then renormalization fails. Let's define "nontriviality" for a belief function Θ . A Θ is nontrivial if there exists some π where E Θ ( π ) ( 1 ) ≠ E Θ ( π ) ( 0 ) In other words, there's some policy you can feed in where the minimal points of Θ ( π ) aren't just a single ( 0 , b ) point. This is a very weak condition. Also, for the upcoming proposition 6, mixing just doesn't interact well with nirvana-free consistency, so we have to do it just for pseudocausal and acausal hypotheses. Proposition 6: For pseudocausal and acausal hypotheses Θ i where ∑ i ζ i λ ⊙ i < ∞ and there exists a nontrivial Θ i , then mixing them and renormalizing produces a pseudocausal or acausal hypothesis. Mixing is defined on the infinite levels by ( E ζ Θ i ) ( π ) = E ζ ( Θ i ( π ) ) , and is then extended down to the finite levels by the usual process of projecting down and taking the closed convex hull. Then, we can renormalize if we wish. We'll distinguish these by "raw" or "renormalized" mix. Thus, the only conditions we need to check are the infinite conditions. For everything else, we can just go "the infinite conditions work, so we can extend to a full belief function" by the Isomorphism theorem. If you want what mixing does at finite levels, it's ¯ ¯¯¯¯¯¯ ¯ c . h ( ⋃ π > π s t ( E ζ ( p r π , π s t ∗ ( Θ i ( π ) ) ) ) ) So it isn't "mix all the finite levels", it's "mix all the projections individually and then take convex hull" Proof sketch: Neglecting normalization (because Lemma 24 shows we can just renormalize and all nice conditions are preserved), we just need to verify all the relevant infinitary conditions, and then we can extend to lower levels by isomorphism, and get our result. We also need to show that nontriviality implies that the renormalization doesn't fail, but that's easy. As for the conditions, our lemmas let us get most of them with little trouble. Bounded-minimals from just the λ ⊙ bound is slightly more difficult and relies on showing that ( 0 , 1 ) is in all the Θ i ( π ) sets regardless of i and π by normalization to eliminate the b term, and Hausdorff-continuity is also fairly nontrivial (we have to split our mix into three pieces and bound each one individually via a different argument) and relies on the same ( 0 , 1 ) is in all the Θ i ( π ) result. For causality, we'll knock it out with Tychonoff in a slightly more complicated way than usual so we just use a countable product and don't have to invoke the full Axiom of Choice. We'll take a detour and show that ( 0 , 1 ) ∈ E ζ Θ i ( π ) for all π , we need this in a few places. First, pick an arbitrary i and π and look at Θ i ( π ) . Find a minimal point ( m , b ) that minimizes m ( 1 ) + b . Now, consider ( m , b ) + ( − m , m ( 1 ) ) . This is ( 0 , m ( 1 ) + b ) . However... m ( 1 ) + b = min ( m ′ , b ′ ) ∈ Θ i ( π ) ∩ N F ( m ( 1 ) + b ) = E Θ i ( π ) ( 1 ) ≤ max π E Θ i ( π ) ( 1 ) = 1 (by ( m , b ) minimizing m ( 1 ) + b , and normalization, respectively) So, by nirvana-free upper-completion for Θ i , we get the point ( 0 , 1 ) ∈ Θ i ( π ) for all i and π . Then, mixing these gets that ( 0 , 1 ) lies in ( E ζ Θ i ) ( π ) . 1: Infinitary Nirvana-Free Nonemptiness, it's easy, all our ( E ζ Θ i ) ( π ) contain the a-measure ( 0 , 1 ) , which is nirvana free. 2,3: Closure, convexity: Closure follows from the proof of closure in Proposition 11 of section 1, and we mixed convex sets so the mixture is convex. 4: Nirvana-free Upper-Completion: Just invoke Lemma 25. 5: Bounded minimals: Since ( 0 , 1 ) ∈ ( E ζ Θ i ) ( π ) for all i, any a-measure with b ≥ 1 isn't minimal (add whatever you want to ( 0 , 1 ) ), so we have a bound on the b values of minimals. What about a bound on the λ values? Well, just take a M in ( E ζ Θ i ) ( π ) with b < 1 , and split it into a mixture of M i points from the Θ i ( π ) . Then, by bounded minimality for the Θ i , we can take each M i and find a minimal point below it that fulfills the λ ⊙ i bound on the λ values. Mixing those minimals produces a point M l o that's below M , and has a λ value below E ζ λ ⊙ i , which, by assumption, is finite. So, every minimal point in any ( E ζ Θ i ) ( π ) has a λ + b value below E ζ λ ⊙ i + 1 and we have bounded-minimals. Nirvana-free consistency is something we'll have to loop back to after Hausdorff-continuity. Condition 8: Hausdorff-continuity: Pick an arbitrary M ∈ ( E ζ Θ i ) ( π ) . It shatters into M i ∈ Θ i ( π ) . We'll be showing the Lemma 15 variant, which is that for all ϵ , there's a δ where if d ( π ′ , π ) < δ , then there's a point in ( E ζ Θ i ) ( π ′ ) that's ϵ ( 1 + λ ) away. First, we'll shuffle around what our M i are supposed to be, we need a certain decomposition to make it work. Reindex your probability distribution so the highest-probability thing is assigned to i = 0 . All the M i can be decomposed as a M min i + M ∗ i . Now, let our new M i for i > 0 be defined as: M min i + ( m ∗ − i , − m ∗ − i ( 1 ) ) , and our M i for i = 0 is defined as: M min 0 + ( m ∗ − 0 , − m ∗ − 0 ( 1 ) ) + ∑ i ζ i ζ 0 ( m ∗ + i , b ∗ i + M ∗ − i ( 1 ) ) These new M i still lie in Θ i ( π ) ∩ N F (nirvana-free upper-completion), and they're all a-measures (the negative part isn't enough to cancel out the positive measure on m min i otherwise our old M i wouldn't be an a-measure). Further, mixing them together still makes M , and if i > 0 , then λ i ≤ λ ⊙ i (because we start off at a minimal point with λ i ≤ λ ⊙ i , and then add something to it that saps some measure from it). Fixing some ϵ ... well, E ζ λ ⊙ i < ∞ , so find a j where ∑ i > j ζ i ( λ ⊙ i + 1 ) < ϵ 3 . For i ≤ j ... well, using the Lemma 15 variant of Hausdorff-continuity, note that fixing a δ gets you a different ϵ i value for Hausdorff-continuity of each Θ i . We only have to worry about the i ≤ j , though, and there's finitely many. So, pick a δ where the induced ϵ 0 is below ϵ 3 , and for all 0 < i ≤ j , ϵ i < ϵ E ζ λ ⊙ i + 1 . So... we take our M i in Θ i ( π ) , and go to nearby M ′ i in Θ i ( π ′ ) . We should break down exactly how this is done. For M 0 , the λ value relative to M is at most λ ζ 0 (in the degenerate case where it contributes all the measure to the mixture M ), so, by Hausdorff-continuity for Θ 0 , the gap between M 0 and M ′ 0 is at most ϵ 3 ( 1 + λ ζ 0 ) because we picked δ low enough to get that scale factor on the front. For M i where 0 < i ≤ j , the gap between M i and M ′ i is at most ϵ 3 ( E ζ λ ⊙ i + 1 ) ( 1 + λ ⊙ i ) Because we picked δ low enough to guarantee that scale factor on the front, and M i is made by adding a minimal point and an sa-measure where the measure component is entirely negative, so λ ⊙ i is a bound on the λ value for M i . And finally, for i > j , we can specially craft a M ′ i where the gap between M i and M ′ i is at most λ ⊙ i + 1 . This is because M i has measure below λ ⊙ i due to being a minimal point that lost some measure. So, we can expend λ ⊙ i effort to completely reshuffle it however we wish, and then add ( 0 , 1 ) to our reshuffled a-measure to make an a-measure that lies above ( 0 , 1 ) , which must be in Θ i ( π ′ ) , so our reshuffled a-measure plus ( 0 , 1 ) lies in Θ i ( π ′ ) by nirvana-free upper-completion, and we only had to spend λ ⊙ i + 1 effort to travel to it (first term is the reshuffling, second term is adding 1 to the b term) Now, let's analyze the distance between M and the point E ζ M ′ i which lies in ( E ζ Θ i ) ( π ′ ) . d ( M , E ζ M ′ i ) equals... d ( ζ 0 M 0 + ∑ 0 < i ≤ j ( ζ i M i ) + ∑ i > j ( ζ i M i ) , ζ 0 M ′ 0 + ∑ 0 < i ≤ j ( ζ i M ′ i ) + ∑ i > j ( ζ i M ′ i ) ) ≤ d ( ζ 0 M 0 , ζ 0 M ′ 0 ) + ∑ 0 < i ≤ j d ( ζ i M i , ζ i M ′ i ) + ∑ i > j d ( ζ i M i , ζ i M ′ i ) = ζ 0 d ( M 0 , M ′ 0 ) + ∑ 0 < i ≤ j ( ζ i d ( M i , M ′ i ) ) + ∑ i > j ( ζ i d ( M i , M ′ i ) ) < ζ 0 ϵ 3 ( 1 + λ ζ 0 ) + ∑ 0 < i ≤ j ( ζ i ϵ 3 ( E ζ λ ⊙ i + 1 ) ( 1 + λ ⊙ i ) ) + ∑ i > j ζ i ( λ ⊙ i + 1 ) < ϵ 3 ( ζ 0 + λ ) + ϵ 3 1 E ζ λ ⊙ i + 1 ∑ 0 < i ≤ j ( ζ i ( 1 + λ ⊙ i ) ) + ϵ 3 < ϵ 3 ( ζ 0 + 1 + λ + 1 E ζ λ ⊙ i + 1 ∑ i ζ i ( λ ⊙ + 1 ) ) < ϵ 3 ( 1 + 1 + λ + E ζ ( λ ⊙ i + 1 ) E ζ λ ⊙ i + 1 ) = ϵ 3 ( 3 + λ ) ≤ ϵ 3 ( 3 + 3 λ ) = ϵ ( 1 + λ ) And bam, we've got Hausdorff-continuity! 9: Nirvana-free consistency: Invoke Lemma 20, notice that we're in the nirvana-free setting, we're done. Pseudocausality. Fix some M ∈ ( E ζ Θ i ) ( π ) , whose support is a subset of F N F ( π ′ ) . M shatters into M i ∈ Θ i ( π ) . All of them have their support being a subset of F N F ( π ′ ) (otherwise there'd be measure-mass outside of there), so all the M i transfer over to Θ i ( π ′ ) by pseudocausality for the Θ i , and then we can mix them back together to get M ∈ ( E ζ Θ i ) ( π ′ ) . And we're done, mixing works just fine, as long as we can show that the renormalization preserves everything and the renormalization doesn't fail. Renormalization fails iff (By Lemma 26), for all π , then E ( E ζ Θ i ) ( π ) ( 1 ) = E ( E ζ Θ i ) ( π ) ( 0 ) We can then go E ( E ζ Θ i ) ( π ) ( 1 ) = E E ζ ( Θ i ( π ) ) ( 1 ) = E ζ ( E Θ i ( π ) ( 1 ) ) and similar for 0, (by the definition of the mix of belief functions and Proposition 10 in Section 1) and then we can use that E ( E ζ Θ i ) ( π ) ( 1 ) − E ( E ζ Θ i ) ( π ) ( 0 ) = 0 to go E ζ ( E Θ i ( π ) ( 1 ) ) − E ζ ( E Θ i ( π ) ( 0 ) ) = 0 ∑ i ζ i ( E Θ i ( π ) ( 1 ) − E Θ i ( π ) ( 0 ) ) = 0 So, then, for all π and i, E Θ i ( π ) ( 1 ) = E Θ i ( π ) ( 0 ) However, this is incompatible with the existence of a nontrivial Θ i , because nontriviality just says that there's a π where E Θ i ( π ) ( 1 ) ≠ E Θ i ( π ) ( 0 ) . So, nontriviality for some Θ i ( π ) means that the renormalization of your mix can be done. It's a very weak condition, just saying "there's some possibility of starting with a hypothesis ( Θ i ) which has a policy π , where murphy actually has to pay attention to what function you're maximizing. Now, we just need to show that renormalization preserves all nice conditions. Just invoke isomorphism to complete to a full psuedo/acausal belief function lacking normalization, apply Lemma 24 and renormalize everything, and we're done. Proposition 7: For pseudocausal and acausal hypotheses, E ( E ζ Θ i ) ( π p a ) ( f ) = E ζ ( E Θ i ( π p a ) ( f ) ) Proof: E ( E ζ Θ i ) ( π p a ) ( f ) = E E ζ ( Θ i ( π p a ) ) ( f ) = E ζ ( E Θ i ( π p a ) ( f ) ) by Proposition 10 of Section 1. Proposition 8: For pseudocausal and acausal hypotheses, p r π h i p a , π l o p a ∗ ( ( E ζ Θ i ) ( π p a ) ) = E ζ ( p r π h i p a , π l o p a ∗ ( Θ i ( π ′ p a ) ) ) This is an easy one. If M ∈ p r π h i p a , π l o p a ∗ ( ( E ζ Θ i ) ( π h i p a ) ) , then there's a preimage point M ′ ∈ ( E ζ Θ i ) ( π h i p a ) , which then decomposes into M ′ i ∈ Θ i ( π h i p a ) . These M ′ i project down to M i , which then mix to make M (project then mix equals mix then project because of linearity) witnessing that M ∈ E ζ ( p r π h i p a , π l o p a ∗ ( Θ i ( π h i p a ) ) ) Now for the reverse direction. If M ∈ E ζ ( p r π h i p a , π l o p a ∗ ( Θ i ( π h i p a ) ) ) , then it shatters into M i ∈ p r π h i p a , π l o p a ∗ ( Θ i ( π h i p a ) ) , which have preimage points M ′ i ∈ Θ i ( π h i p a ) . The M ′ i mix to make a M ′ ∈ ( E ζ Θ i ) ( π h i p a ) , which projects down to M , by project-mix equaling mix-project, witnessing that M ∈ p r π h i p a , π l o p a ∗ ( ( E ζ Θ i ) ( π h i p a ) ) . Next proof post!
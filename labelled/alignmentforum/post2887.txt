Contrasting formalisms Here I'll contrast the approach we're using in using in Pitfalls of Learning a Reward Online (summarised here ), with that used by Tom Everitt and Marcu Hutter in the conceptually similar Reward Tampering Problems and Solutions in Reinforcement Learning . In the following, histories h i are sequences of actions a and observations o ; thus h i = a 1 o 1 a 2 o 2 … a i o i . The agent's policy is given by π , the environment is given by μ . Then the causal graph for the "Pitfalls" approach is, in plate notation (which basically means that, for every value of j from 1 to n , the graph inside the rectangle is true): The R is the set of reward functions (mapping "complete" histories h n of length n to real numbers), the ρ tells you which reward is correct, conditional on complete histories, and r is the final reward. In order to move to the reward tampering formalism, we'll have to generalise the R and ρ , just a bit. We'll allow R to take partial histories - h j shorter than h n - and return a reward. Similarly, we'll generalise ρ to a conditional distribution on R , conditional on all histories h j , not just on complete histories. This leads to the following graph: This graph is now general enough to include reward tampering formalism. States, data, and actions In reward tampering formalism, "observations" ( o j ) decompose into two pieces: states ( S j ) and data ( D j ). The idea is that data informs you about the reward function, while states get put into the reward function to get the actual reward. So we can model this as this causal graph (adapted from graph 10b, page 22 ; this is a slight generalisation, as I haven't assumed Markovian conditions): Inside the rectangle, the histories split into data ( D 1 : j ), states ( S 1 : j ), and actions ( a 1 : j ). The reward function is defined by the data only, while the reward comes from this reward function and from the states only - actions don't directly affect these (though they can indirectly affect them by deciding what states and data come up, of course). Note that in the reward tampering paper, the authors don't distinguish explicitly between R j and r j , but they seem to do so implicitly. Finally, Θ R ∗ is the "user's reward function", which the agent is estimating via D 1 : j ; this connects to the data only. Almost all of the probability distributions at each node are "natural" ones that are easy to understand. For example, there are arrows into r j (the reward) from R j (the reward function) and S 1 : j (the states history); the "conditional distribution" of r j is just "apply R j to S 1 : j . The environment, action, and history naturally provide the next observations (state and data). Two arrows point to more complicated relations: the arrow from Θ R ∗ to D j , and that from D 1 : j to R . The two are related; the data D j is supposed to tell us about the user's true reward function, while this information informs the choice of R . But the fact that the nodes and the probability distribution have been "designed" this way doesn't affect the agent. It has a fixed process P r t ( R ∣ D 1 : j ) for estimating R from D 1 : j ( P r t stands for the probability function for the reward tampering formalism). It has access to a j , D j , and S j (and their histories) as well as its own policy, but has no direct access to μ or Θ R ∗ . In fact, from the agent's perspective, Θ R ∗ is essentially part of μ , the environment, though focusing on the D j only. States and actions in "Pitfalls" formalism Now, can we put this into the "Pitfalls" formalism? It seems we can, as so: All conditional probability distributions in this graph are natural. This graph look very similar to the "reward tampering" one, with the exception of ρ j and Θ R ∗ , pointing at R j and D j respectively. In fact, ρ j play the role of P r t ( R ∣ D 1 : j ) in that, for P l p the probability distribution for learning process, P l p ( R ∣ D 1 : j , ρ j ) = P r t ( R ∣ D 1 : j ) . Note that P l p in that expression is natural and simple, while P r t is complex; essentially P r t carries the same information as ρ j . The environment μ l p of the learning process plays the same role as the combined μ r t and Θ ∗ R from the reward tampering formalism. So the isomorphism between the two approaches is, informally speaking: On reward functions conditional on histories, P r t ↔ ρ . μ l p ↔ ( μ r t , Θ ∗ R ) . Uninfluenceable similarities If we make the processes uninfluenceable (a concept that exists for both formalisms), the causal graphs look even more similar: Here the pair ( μ l p , η ) , for the learning process, play exactly the same role as the pair [1] ( μ r t , Θ R ∗ ) , for reward tampering: determining reward functions and observations. There is an equivalence between the pairs, but not between the individual elements; thus μ l p carries more information than μ r t , while η carries less information than Θ R ∗ . ↩︎
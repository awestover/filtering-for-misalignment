( Note: In the past I have referred to this process as â€˜bootstrappingâ€™ or â€˜policy amplification,â€™ but those terms are too broadâ€Šâ€”â€Šthere are other dimensions along which policies can be amplified, and â€˜bootstrappingâ€™ is used all over the place. ) Defining the â€œintended behaviorâ€ of a powerful AI system is a challenge. We donâ€™t want such systems to simply imitate human behaviorâ€Šâ€”â€Šwe want them to improve upon human abilities. And we donâ€™t want them to only take actions that look good to humansâ€Šâ€”â€Šwe want them to improve upon human judgment. We also donâ€™t want them to pursue simple goals like â€œminimize the probability that the bridge falls downâ€ or â€œpick the winning move.â€ A precise statement of our real goals would be incredibly complicated, and articulating them precisely is itself a massive project. Moreover, we often care about consequences over years or decades. Such long-term consequences would have little use as a practical problem definition in machine learning, even if they could serve as a philosophical problem definition. So: what else can we do? Instead of defining what it means for a policy to be â€œgood,â€ we could define a transformation which turns one policy into a â€œbetterâ€ policy. I call such a transformation capability amplification â€”â€Šit â€œamplifiesâ€ a weak policy into a strong policy, typically by using more computational resources and applying the weak policy many times. Motivation I am interested in capability amplification because I think it is the most plausible route to defining the goals of powerful AI systems, which I see as a key bottleneck for building aligned AI. The most plausible alternative approach is probably inverse RL, but I think that there are still hard philosophical problems to solve, and that in practice IRL would probably need to be combined with something like capability amplification . More directly, I think that capability amplification might be a workable approach to training powerful RL systems when combined with semi-supervised RL , adversarial training , and informed oversight (or another approach to reward engineering ). Example of capability amplification: answering questions Suppose that we would like like to amplify one question-answering system A into a â€œbetterâ€ question-answering system Aâº . We will be given a question Q and an implementation of A ; we can use A , or any other tools at our disposal, to try to answer the question Q . We have some time limit; in reality it might be eight hours, but for the purpose of a simple example suppose it is twenty seconds. The amplification Aâº ( Q ) is defined to be whatever answer we come up with by the end of the time limit. The goal is for this answer to be â€œbetterâ€ than the answer that A would have given on its own, or to be able to answer harder questions than A could have answered directly. For example, suppose that Q = â€œWhich is more water-soluble, table salt or table sugar?â€ Suppose further that A canâ€™t answer this question on its own: A (â€œWhich is more water-solubleâ€¦â€) = â€œI donâ€™t know.â€ I could start by computing A (â€œHow do you quantify water-solubility?â€); say this gives the answer â€œBy measuring how much of the substance can dissolve in a fixed quantity of water.â€ Then I ask A (â€œHow much table salt will dissolve in a liter of water?â€) and get back the answer â€œ360 grams.â€ Then I ask A (â€œHow much sugar will dissolve in a liter of water?â€) and get back the answer â€œ2 kilograms.â€ Then I reply â€œSugar is about six times more soluble than salt.â€ Thus Aâº (â€œWhich is more water-soluble, table salt or table sugar?â€) = â€œSugar is about six times more soluble than salt.â€ This is better than the answer that A gaveâ€Šâ€”â€Šin some sense, weâ€™ve successfully amplified A into something smarter. The generalÂ problem The capability amplification problem is to use one policy A to implement a new policy Aâº which is strictly â€œbetterâ€ than A . (Recall that a policy is a mapping from inputs to outputs.) Weâ€™ll discuss the definition of â€œbetterâ€ in the next section, but for now you can use an intuitive definition. Note that â€œbetterâ€ does not mean that we can implement Aâº using fewer computational resources than A â€”â€Šin fact we will implement Aâº by using a huge amount of computation and time. What does it mean to â€œimplementâ€ the amplified policy Aâº ? It means that we have some process that takes as input an observation o [1] and produce an action a [1]. It then takes as input the next observation o [2] and produces the next action a [2], and so on. The process that implements Aâº may instantiate any number of agents who use the policy A and interact with them. The process might make copies of any of these agents. And the process can involve us personally thinking about the problem, or using any other tools that we have availableâ€Šâ€”â€Šhaving access to A may be a useful resource, but we can also do things from scratch if thatâ€™s easier. The capability amplification problem comes with a time limitâ€Šâ€”â€Šwe need to provide an implementation that runs within that time limit. (When we submit one action a [k], we immediately see the next observation o [k+1].) Once the time limit runs out, we automatically output a nil action in response to each additional observation. One way to be â€œbetterâ€ is to be able to handle longer sequences of observations. The time limit could be arbitrary, but Iâ€™ll pick one day for concreteness. Iâ€™ll assume that we have an implementation of A that runs in one second per episode. Note that we can start from the trivial policy âˆ… which always outputs nil. In this case, âˆ…âº is a policy that we can implement â€œfrom scratch.â€ Reachability To measure how well we can solve capability amplification, weâ€™ll introduce the concept of reachability . Reachability is defined with respect to a class of policies ğ’œ and a preference ordering âª°. Intuitively: ğ’œ is a class of policies that we are capable of implementing efficiently. For example, ğ’œ might be the set of policies that can be implemented by a ten layer neural network. We say that A âª° B if we are at least as happy with policy A as with policy B (in any situation that we think might arise in practice). We say that C is reachable from A if: A âº âª° C , where A âº is the amplification as described in the last section; or There is an intermediate B âˆˆ ğ“ which is reachable from A and which can reach C . Equivalently: C is reachable from A if there is a chain of policies in ğ’œ which starts at A and ends at C , and where each policy in the chain is no better than the amplification of the previous policy. The better we are at capability amplification, the more policies will be reachable from any given starting point. Our goal is to have as many policies as possible be reachable from the trivial policy âˆ… â€”â€Šideally, every policy in ğ“ would be reachable from âˆ… . Obstructions An obstruction to capability amplification is a partition of the policy class ğ“ into two parts ğ“› and ğ“—, such that we cannot amplify any policy in ğ“› to be at least as good as any policy in ğ“—. Obstructions are dual to reachability in a natural sense. If there are any non-reachable policies, then there is some corresponding obstruction. The desired output of research on capability amplification are a matching amplification strategy and obstructionâ€Šâ€”â€Ša way to reach many policies, and an obstruction that implies that we canâ€™t reach any more. Analogously, we say that a function L : ğ“ â†’ â„ is an obstruction if our amplification procedure cannot always increase L . That is, L is an obstruction if there exists a threshold â„“ such that the two sets { A âˆˆ ğ“Â : L ( A ) â‰¤ â„“ } and { A âˆˆ ğ“Â : L ( A ) > â„“} are an obstruction, or such that { A âˆˆ ğ“Â : L ( A ) < â„“ } and { A âˆˆ ğ“Â : L ( A ) â‰¥ â„“} are an obstruction. If we could find a convincing argument that some partition was an obstruction, then that would help further our understanding of value alignment. The next step would be to ask: can we sensibly define â€œgood behaviorâ€ for policies in the inaccessible part ğ“—? I suspect this will help focus our attention on the most philosophically fraught aspects of value alignment. In the appendices I give an example of an obstruction in a particular simple model. Relationship to value alignment Why capability amplification seemsÂ feasible Capability amplification is a special case of the general problem of â€œbuilding an AI that does the right thing.â€ It is easier in two respects: In the general problem we need to construct a â€œgoodâ€ policy from scratch. In capability amplification we need to construct a good policy A âº starting from a slightly weaker policy A . In the general problem we must efficiently implement a good policy. In capability amplification our implementation of Aâº is allowed to take up to a day, even though the goal is to improve upon a policy A that runs in one second. Intuitively, these seem like large advantages. Nevertheless, it may be that capability amplification contains the hardest aspects of value alignment. If true, I think this would change our conception of the value alignment problem and what the core difficulties are. For example, if capability amplification is the â€œhard part,â€ then the value alignment problem is essentially orthogonal to the algorithmic challenge of building an intelligence. Why capability amplification seemsÂ useful Capability amplification can be combined with reward engineering in a natural way: Define A0 = âˆ… Apply capability amplification to obtain A0âº Apply reward engineering to define a reward function, and use this to train an agent A1 which is better than A0 Apply capability amplification to obtain A1âº Repeat to obtain a sequence of increasingly powerful agents This is very informal, and actually carrying out such a process requires resolving many technical difficulties. But it suggests that capability amplification and reward engineering might provide a foundation for training an aligned AI . What toÂ do? Theory The best approach seems to be to work from both sides, simultaneously searching for challenging obstructions and searching for amplification procedures that address those obstructions. There are at least two very different angles on capability amplification: Collaboration: figure out how a bunch of agents using A can break a problem down into smaller pieces and attack those pieces separately, allowing them to solve harder problems than they could solve independently. Philosophy: try to better understand what â€œgoodâ€ reasoning is, so that we can better understand how good reasoning is composed of simpler steps. For example, mathematical proof is a technique which relates hard problems to long sequences of simple steps. There may be more general ideas along similar lines. In the appendices, I describe some possible amplification schemes and obstructions, along with some early ideas about capability amplification in general. Experiment Today, it is probably most worthwhile to study capability amplification when A is a humanâ€™s policy. In this setting, we are given some weak human policy A â€”â€Šsay, a human thinking for an hour. We would like to amplify this to a strong collaborative policy Aâº , by invoking a bunch of copies of A and having them interact with each other appropriately. In some sense this is the fully general problem of organizing human collaborations. But we can focus our attention on the most plausible obstructions for capability amplification, and try to design collaboration frameworks that let us overcome those obstructions. In this context, I think the most interesting obstruction is working with concepts that are (slightly) too complicated for any individual copy of A to understand on its own. This looks like a hard problem that is mostly unaddressed by usual approaches to collaboration. This post lays out a closely related problemâ€Šâ€”â€Šquickly evaluating arguments by expertsâ€Šâ€”â€Šwhich gets at most of the same difficulties but may be easier to study. Superficially, evaluating arguments may seem easier than solving problems from scratch. But because it is so much easier to collaboratively create arguments once you have a way to evaluate them, I think the gap is probably only superficial. Conclusion The capability amplification problem may effectively isolate the central philosophical difficulties of value alignment. Itâ€™s not easy to guess how hard it isâ€Šâ€”â€Šwe may already have â€œgood enoughâ€ solutions, or it may effectively be a restatement of the original problem. Capability amplification asks us to implement a powerful policy that â€œbehaves well,â€ but it is easier than value alignment in two important respects: we are given access to a slightly weaker policy, and our implementation can be extremely inefficient. It may be that these advantages are not significant advantages, but if so that would require us to significantly change our understanding of what the value alignment problem is about. Capability amplification appears to be less tractable than the other research problems Iâ€™ve outlined. I think itâ€™s unlikely to be a good research direction for machine learning researchers interested in value alignment. But it may be a good topic for researchers with a philosophical focus who are especially interested in attacking problems that might otherwise be neglected. (This research was supported as part of the Future of Life Institute FLI-RFP-AI1 program, grant #2015â€“143898.) Appendix: iterating amplification Let H be the input-output behavior of a human + all of the non- A tools at their disposal. Then an amplification procedure defines A âº as a simple computation that uses H and A as subroutines. In particular, âˆ…âº is a computation that uses H as a subroutine. If we amplify again, we obtain âˆ…âºâº , which is a computation that uses H and âˆ…âº as subroutines. But since âˆ…âº is a simple computation that uses H as a subroutine, we can rewrite âˆ…âºâº as a simple computation that uses only H as a subroutine. We can go on in this way, reaching âˆ…âºâºâº , âˆ…âºâºâºâº and so on. By induction, all of these policies are defined by simple computations that use as H as a subroutine. (Of course these â€œsimple computationsâ€ are exponentially expensive, even though they are easy to specify. But they have a simple form and can be easily written down in terms of the amplification procedure.) Under some simple ergodicity assumptions, this sequence converges to a fixed point Î© (very similar to HCH ). So a capability amplification procedure essentially uniquely defines an â€œoptimalâ€ policy Î© ; this policy is uncomputable, but has a concise representation in terms of H . If there is anything that Î© canâ€™t do, then we have found an unreachable policy. This perspective seems useful for identifying the hard part of the capability amplification problem. Specifying an amplification strategy also specifies a way to set up an interaction between a bunch of copies of H such that they implement Î© . Indeed, designing such an interaction is easier than designing an amplification procedure that converges to Î© . So if we canâ€™t design a procedure for a bunch of copies of H to collaboratively accomplish some task T, then we also canâ€™t design an amplification procedure that can reach policies that can accomplish T. But in some cases this seems extremely challenging. For example, suppose that we would like to write a very good textbook about general relativity. Recall that H is the policy implemented by a human over one day. To make the game hard, suppose that H has never encountered calculus. The most natural approaches require at least one copy of H to understand general relativity. But H only lasts a single day, and there many not be any sequence of observations that makes H understand general relativity within a day. So in order for Î© to succeed at this task, it needs to be able to write a textbook without any single copy of H understanding general relativity. The understanding must instead be distributed across a large number of copies of H . Yet they must somehow collaborate to write individual sentences that correctly communicate the theory. This sounds like a hard problem. Appendix: examples of capability amplification ETA: my current best proposal is meta-execution , which is essentially a formalization of the â€œuniversalâ€ scheme below. Warmup I: Preprocessing We instantiate a â€œleaderâ€ agent who uses the policy A . For each observation o [ k ]: We instantiate a new agent using A . We give them observation: â€œthink about the observation { o [ k ]} and output a useful hint for a copy of yourself who has to respond to it.â€ Let h [ k ] be the agentâ€™s action. Then we destroy the agent. We give the leader the observation â€œThe observation is { o [ k ]}, and a copy of you prepared the hint { h [ k ]}.â€ Let a [ k ] be its action. We might hope that this composite is smarter than A . After all, it has twice as long to thinkâ€Šâ€”â€Ševen if the copy canâ€™t come up with a very helpful hint, it doesnâ€™t seem so hard to come up with a hint that is better than nothing. Warmup II: Asking questions We instantiate a â€œleaderâ€ agent who uses the policy A . We give each observation o [ k ] to the leader, and ask them to reply with an action a [ k ]. But we also allow the leader to instantiate subagents and communicate with those agents. That is, if the leader takes an action of the special form â€œInstantiate a new agentâ€ then we create a new agent using A , assign it a unique ID, and give the leader the observation: â€œA new agent has been created with ID [X].â€ Similarly, the leader can take the special action â€œCopy the agent with ID [X].â€ If the leader takes an action of the special form â€œAsk [Q] to the agent with ID [X]â€ then we give Q as an observation to the agent with ID X. That agent will reply with some action A, and we give the leader the next observation: â€œThe agent replied with [A].â€ Eventually, the leader takes an action of the special form â€œTake action [A]â€. Then we set a [ k ] = A, and move on to the next observation. The scheme from the last section is essentially the special case where the leader instantiates a single subagent and asks the single question â€œWhat is a useful hint for responding to observation o [ k ]?â€ Warmup III: Recursion We improve the scheme from the last section by allowing every agent to instantiate new subagents, rather than restricting this capability to the leader. The messages between agents can contain not only words, but pointers to other agents. This allows the agents to pass arbitrarily large messages. For example, they can build a data structure out of agents, and just pass a â€œpointerâ€ to the root of that data structure. We can also slightly improve the scheme by allowing the leader to copy itself . This may be helpful if the leader has built up some useful context for the current situation. Universal amplification We improve the scheme further by giving the agents a large external memory. Rather than directly giving the leader the observation o [ k ], we write the observation to the external memory and tell the leader where the observation is written. In addition to allowing agents to instantiate new agents and communicate with them, any agent can take the action â€œexecute [P]â€, where P is a program to be run against the external memory; the agent sees the observation â€œthe program returned [ x ]â€ where x is Pâ€™s return value (or â€œnothingâ€). Rather than having the leader directly return an action, the leader can take the special action â€œoutput the action returned by program [P].â€ Messages between agents can now contain pointers to this external memory. For example, the leader could instantiate a subagent and ask it the question â€œCan you distinguish [ x ] from an array of random bytes?â€ where x is a pointer to an array in external memory. We can easily generalize this setup to a parallel model of computation. We can also replace the shared memory by a more natural model for interprocess communication. Appendix: knowledge aboutÂ humans Human values are complex. If you are only able to interact with a human for a day, it may be completely impossible to figure out what they value, no matter how smart you are. Understanding what someone values may require giving them a large amount of time to reflect on their values, doing neuroscience, or carrying out other processes that take longer than a day. This may imply an obstruction to capability amplificationâ€Šâ€”â€Šwe canâ€™t reach policies that have more knowledge about humans than can be acquired by interacting with H. However, even if this is a real obstruction, it does not seem to be an important one, for the following reason. Suppose that we are able to train a very good policy, which does not reflect any complex facts about human values-upon-reflection. This optimal policy still can reflect many basic facts about human preferences: We donâ€™t want anything terrible to happen. We want to â€œstay in controlâ€ of the agents we build. We donâ€™t want our agent to get left behind by its competitors; it should fight as hard as it can to retain influence over the world, subject to #1 and #2. Moreover, all of these concepts are relatively easy to understand even if you have minimal understanding of human values. So an excellent agent with a minimal understanding of human values seems OK. Such an agent could avoid getting left behind by its competitors, and remain under human control. Eventually, once it got enough information to understand human values (say, by interacting with humans), it could help us implement our values. In the worst case the agent would lack a nuanced understanding of what we consider terrible, and so would have to either be especially conservative or else risk doing terrible things in the short term. In the scheme of things, this is not a catastrophic problem. Appendix: an example obstruction Suppose that my brain encodes a random function f : {0, 1}* â†’ {0, 1} in the following sense: you can give me a sequence of bits, one per second, and then I can tell you the value of f on that sequence. There is no way to evaluate f other than to ask me. Let N be the length of our capability amplification procedure, in seconds. Let ğ“› âŠ† ğ“ be the set of policies that can be implemented using an oracle for f , restricted to inputs of length N. Then itâ€™s easy to see that ğ“› forms an obstruction: We can simulate access to any policy in ğ“› using an oracle for f restricted to inputs of length N. And we can simulate my role in the amplification procedure using an oracle for f restricted to inputs of length N. So policies in ğ“› can only be amplified to other policies in ğ“›. We cannot evaluate f on even a single input of length N+1 using an oracle for f on inputs of length N. Most interesting classes ğ“ will contain some policies not in ğ“›. Whether this is a real obstruction depends on what the information is about: If itâ€™s just random bits, then we donâ€™t care at allâ€Šâ€”â€Šany other random bits would be â€œjust as good.â€ If the random function encodes important information about my values, then we are in the situation described in the previous section, which doesnâ€™t seem so bad. The worst case is when the function f encodes important information about how to behave effectively. For example, it encodes information about how to make accurate predictions. In this case we may actually be in trouble, since a policy that doesnâ€™t know f may be outcompeted by one which does. This was originally posted here on 2nd October 2016. The next post in this sequence will be 'Learning with catastrophes' by Paul Christiano. Tomorrow's post will be 'Following Human Norms' in the sequence Value Learning by Rohin Shah.
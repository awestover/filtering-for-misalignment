We introduce a variant of optimal predictor schemes where optimality holds within the space of random algorithms with logarithmic advice. These objects are also guaranteed to exist for the error space Δ 2 a v g . We introduce the class of generatable problems and construct a uniform universal predictor scheme for this class which is optimal in the new sense with respect to the Δ 2 a v g error space. This is achieved by a construction similar to Levin's universal search. Results New notation Given n ∈ N , e v n : N × { 0 , 1 } ∗ n + 1 a l g − → { 0 , 1 } ∗ is the following algorithm. When e v k n ( Q , x 1 … x n ) is computed, Q is interpreted as a program and Q ( x 1 … x n ) is executed for time k . The resulting output is produced. The notation e v k ( Q , x 1 … x n ) means e v k n ( Q , x 1 … x n ) . β : { 0 , 1 } ∗ → [ 0 , 1 ] is the mapping from a binary expansion to the corresponding real number. Given μ a word ensemble, X a set, Q : { 0 , 1 } ∗ 2 a l g − → X , T μ Q ( k , s ) stands for the maximal runtime of Q ( x , y ) for x ∈ supp μ k , y ∈ { 0 , 1 } s . Previous posts focused on prediction of distributional decision problems, which is the "computational uncertainty" analogue of probability. Here, we use the broader concept of predicting distributional estimation problems (functions), which is analogous to expectation value. Definition 1 A distributional estimation problem is a pair ( f , μ ) where f : { 0 , 1 } ∗ → [ 0 , 1 ] is an arbitrary function (even irrational values are allowed) and μ is a word ensemble. Definition 2 Given an appropriate set X , consider P : N 2 × { 0 , 1 } ∗ 3 a l g − → X , r : N 2 → N polynomial and a : N 2 → { 0 , 1 } ∗ . The triple ( P , r , a ) is called an X -valued ( p o l y , l o g ) -bischeme when (i) The runtime of P ( k , j , x , y , z ) is bounded by p ( k , j ) with p polynomial. (ii) | a ( k , j ) | ≤ c 1 + c 2 log ( k + 1 ) + c 3 log ( j + 1 ) for some c 1 , c 2 , c 3 ∈ N . A [ 0 , 1 ] -valued ( p o l y , l o g ) -bischeme will also be called a ( p o l y , l o g ) -predictor scheme . We think of P as a random algorithm where the second word parameter represents its internal coin tosses. The third word parameter represents the advice and we usually substitute a there. We will use the notations P k j ( x , y , z ) : = P ( k , j , x , y , z ) , a k j : = a ( k , j ) . Definition 3 Fix Δ an error space of rank 2 and ( f , μ ) a distributional estimation problem. Consider ( P , r , a ) a ( p o l y , l o g ) -predictor scheme. ( P , r , a ) is called a Δ ( p o l y , l o g ) -optimal predictor scheme for ( f , μ ) when for any ( p o l y , l o g ) -predictor scheme ( Q , s , b ) , there is δ ∈ Δ s.t. E μ k × U r ( k , j ) [ ( P k j ( x , y , a k j ) − f ( x ) ) 2 ] ≤ E μ k × U s ( k , j ) [ ( Q k j ( x , y , b k j ) − f ( x ) ) 2 ] + δ ( k , j ) Note 1 The notation ( p o l y , l o g ) is meant to remind us that we allow a polynomial quantity of random bits r ( k , j ) and a logarithmic quantity of advice | a k j | . In fact, the definitions and some of the theorems can be generalized to other quantities of random and advice (see also Note B.1). Thus, predictor schemes from previous posts are ( p o l y , p o l y ) -predictor schemes, ( p o l y , O ( 1 ) ) -predictor schemes are limited to O(1) advice, ( l o g , 0 ) -predictor schemes use a logarithmic number of random bits and no advice and so on. As usual in complexity theory, it is redundant to consider more advice than random since advice is strictly more powerful. Δ ( p o l y , l o g ) -optimal predictor scheme satisfy properties analogical to Δ -optimal predictor schemes. These properties are listed in Appendix A. The proofs of Theorem A.1 and A.4 are given in Appendix B. The other proofs are straightforward adaptions of corresponding proofs with polynomial advice. We also have the following existence result: Theorem 1 Consider ( f , μ ) a distributional estimation problem. Define Υ : N 2 × { 0 , 1 } ∗ 3 a l g − → [ 0 , 1 ] by Υ k j ( x , y , Q ) : = β ( e v j ( Q , x , y ) ) Define υ f , μ : N 2 → { 0 , 1 } ∗ by υ k j f , μ : = a r g m i n | Q | ≤ log j E μ k × U j [ ( Υ k j ( x , y , Q ) − f ( x ) ) 2 ] Then, ( Υ , j , υ f , μ ) is a Δ 2 a v g ( p o l y , l o g ) -optimal predictor scheme for ( f , μ ) . Note 2 Consider a distributional decision problem ( D , μ ) . Assume ( D , μ ) admits n ∈ N , A : N × { 0 , 1 } ∗ 3 a l g − → { 0 , 1 } , a : N → { 0 , 1 } ∗ and a function r : N → N s.t. (i) A ( k , x , y , z ) runs in quasi-polynomial time ( O ( 2 log n k ) ). (ii) | a ( k ) | = O ( log n k ) (iii) lim k → ∞ P r μ k × U r ( k ) [ A ( k , x , y , a ( k ) ) ≠ χ D ( x ) ] = 0 Then it is easy to see we can construct a ( p o l y , l o g ) -predictor scheme P A taking values in { 0 , 1 } s.t. E [ ( P A − f ) 2 ] ∈ Δ 2 a v g . The implication doesn't work for larger sizes of time or advice. Therefore, the uncertainty represented by Δ 2 a v g ( p o l y , l o g ) -optimal predictor schemes is associated with the resource gap between quasi-polynomial time plus advice O ( log n k ) and the resources needed to (heuristically) solve the problem in question. The proof of Theorem 1 is given in Appendix C: it is a straightforward adaptation of the corresponding proof for polynomial advice . Evidently, the above scheme is non-uniform. We will now describe a class of problems which admits uniform Δ 2 a v g ( p o l y , l o g ) -optimal predictor schemes. Definition 4 Consider Δ 1 an error space of rank 1. A word ensemble μ is called Δ 1 ( l o g ) -sampleable when there is S : N × { 0 , 1 } ∗ 2 a l g − → { 0 , 1 } ∗ that runs in polynomial time in the 1st argument, a S : N → { 0 , 1 } ∗ of logarithmic size and r S : N → N a polynomial such that ∑ x ∈ { 0 , 1 } ∗ | μ k ( x ) − P r U r S ( k ) [ S k ( y , a S ( k ) ) = x ] | ∈ Δ 1 ( S , r S , a S ) is called a Δ 1 ( l o g ) -sampler for μ . Definition 5 Consider Δ 1 an error space of rank 1. A distributional estimation problem ( f , μ ) is called Δ 1 ( l o g ) -generatable when there are S : N × { 0 , 1 } ∗ 2 a l g − → { 0 , 1 } ∗ and F : N × { 0 , 1 } ∗ 2 a l g − → [ 0 , 1 ] that run in polynomial time in the 1st argument, a S : N → { 0 , 1 } ∗ of logarithmic size and r S : N → N a polynomial such that (i) ( S , r S , a S ) is a Δ 1 ( l o g ) -sampler for μ . (ii) E U r S ( k ) [ ( F k ( y , a S ( k ) ) − f ( S k ( y , a S ( k ) ) ) ) 2 ] ∈ Δ 1 ( S , F , r S , a S ) is called a Δ 1 ( l o g ) -generator for ( f , μ ) . When a S is the empty string, ( S , F , r S ) is called a Δ 1 ( 0 ) -generator for ( f , μ ) . Such ( f , μ ) is called Δ 1 ( 0 ) -generatable . Note 3 The class of Δ 1 ( 0 ) -generatable problems can be regarded as an average-case analogue of N P ∩ c o N P . If f is a decision problem (i.e. its range is { 0 , 1 } ), words y ∈ { 0 , 1 } r S ( k ) s.t. S k ( y ) = x , F k ( y ) = 1 can be regarded as "proofs" of f ( x ) = 1 and words y ∈ { 0 , 1 } r S ( k ) s.t. S k ( y ) = x , F k ( y ) = 0 can be regarded as "proofs" of f ( x ) = 0 . Theorem 2 There is an oracle machine Λ that accepts an oracle of signature S F : N × { 0 , 1 } ∗ → { 0 , 1 } ∗ × [ 0 , 1 ] and a polynomial r : N → N where the allowed oracle calls are S F k ( x ) for | x | = r ( k ) and computes a function of signature N 2 × { 0 , 1 } ∗ 2 → [ 0 , 1 ] s.t. for any ( f , μ ) a distributional estimation problem and G : = ( S , F , r S , a S ) a corresponding Δ 1 0 ( l o g ) -generator, Λ [ G ] is a Δ 2 a v g ( p o l y , l o g ) -optimal predictor scheme for ( f , μ ) . In particular if ( f , μ ) is Δ 1 0 ( 0 ) -generatable, we get a uniform Δ 2 a v g ( p o l y , l o g ) -optimal predictor scheme. The following is the description of Λ . Consider S F : N × { 0 , 1 } ∗ → { 0 , 1 } ∗ × [ 0 , 1 ] and a polynomial r : N → N . We describe the computation of Λ [ S F , r ] k j ( x ) where the extra argument of Λ is regarded as internal coin tosses. We loop over the first j words in lexicographic order. Each word is interpreted as a program Q : { 0 , 1 } ∗ 2 a l g − → [ 0 , 1 ] . We loop over j k "test runs". At test run i , we generate ( x i ∈ { 0 , 1 } ∗ , t i ∈ [ 0 , 1 ] ) by evaluating S F k ( y i ) for y i sampled from U r ( k ) .   We then sample z i from U j and compute s i : = e v j ( Q , x i , z i ) . At the end of the test runs, we compute the average error ϵ ( Q ) : = 1 j k ∑ i ( s i − t i ) 2 . At the end of the loop over programs, the program Q ∗ with the lowest error is selected and the output e v j ( Q ∗ , x ) is produced. The proof that this construction is Δ 2 a v g ( p o l y , l o g ) -optimal is given in Appendix C. Appendix A Fix Δ an error space of rank 2. Theorem A.1 Suppose there is a polynomial h : N 2 → N s.t. h − 1 ∈ Δ . Consider ( f , μ ) a distributional estimation problem and ( P , r , a ) a Δ ( p o l y , l o g ) -optimal predictor scheme for ( f , μ ) . Suppose { p k j ∈ [ 0 , 1 ] } k , j ∈ N , { q k j ∈ [ 0 , 1 ] } k , j ∈ N are s.t. ∃ ϵ > 0 ∀ k , j : ( μ k × U r ( k , j ) ) { ( x , y ) ∈ { 0 , 1 } ∗ 2 ∣ p k j ≤ P k j ( x , y , a k j ) ≤ q k j } ≥ ϵ Define ϕ k j : = E μ k × U r ( k , j ) [ f ( x ) − P k j ( x , y , a k j ) ∣ p k j ≤ P k j ( x , y , a k j ) ≤ q k j ] Assume that either p k j , q k j have a number of digits logarithmically bounded in k , j or P k j produces outputs with a number of digits logarithmically bounded in k , j (by Theorem A.7 if any Δ ( p o l y , l o g ) -optimal predictor scheme exists for ( f , μ ) then a Δ ( p o l y , l o g ) -optimal predictor scheme with this property exists as well). Then, | ϕ | ∈ Δ . Theorem A.2 Consider μ a word ensemble and f 1 , f 2 : { 0 , 1 } ∗ → [ 0 , 1 ] s.t. f 1 + f 2 ≤ 1 . Suppose ( P 1 , r 1 , a 1 ) is a Δ ( p o l y , l o g ) -optimal predictor scheme for ( f 1 , μ ) and ( P 2 , r 2 , a 2 ) is a Δ ( p o l y , l o g ) -optimal predictor scheme for ( f 2 , μ ) . Define P : N 2 × { 0 , 1 } ∗ 3 a l g − → [ 0 , 1 ] by P k j ( x , y 1 y 2 , ( z 1 , z 2 ) ) : = η ( P k j 1 ( x , y 1 , z 1 ) + P k j 2 ( x , y 2 , z 2 ) ) for | y i | = r i ( k , j ) . Then, ( P , r 1 + r 2 , a 1 a 2 ) is a Δ ( p o l y , l o g ) -optimal predictor scheme for ( f 1 + f 2 , μ ) . Theorem A.3 Consider μ a word ensemble and f 1 , f 2 : { 0 , 1 } ∗ → [ 0 , 1 ] s.t. f 1 + f 2 ≤ 1 . Suppose ( P 1 , r 1 , a 1 ) is a Δ ( p o l y , l o g ) -optimal predictor scheme for ( f 1 , μ ) and ( P , r 2 , a 2 ) is a Δ ( p o l y , l o g ) -optimal predictor scheme for ( f 1 + f 2 , μ ) . Define P 2 : N 2 × { 0 , 1 } ∗ 3 a l g − → [ 0 , 1 ] by P k j 2 ( x , y 1 y 2 , ( z 1 , z 2 ) ) : = η ( P k j ( x , y 1 , z 1 ) − P k j 1 ( x , y 2 , z 2 ) ) for | y i | = r i ( k , j ) . Then, ( P 2 , r 1 + r 2 , a 1 a 2 ) is a Δ ( p o l y , l o g ) -optimal predictor scheme for ( f 2 , μ ) . Theorem A.4 Fix Δ 1 an error space of rank 1 s.t. given δ 1 ∈ Δ 1 , the function δ ( k , j ) : = δ 1 ( k ) lies in Δ . Consider ( f 1 , μ 1 ) , ( f 2 , μ 2 ) distributional estimation problems with respective Δ ( p o l y , l o g ) -optimal predictor schemes ( P 1 , r 1 , a 1 ) and ( P 2 , r 2 , a 2 ) . Assume μ 1 is Δ 1 ( l o g ) -sampleable and ( f 2 , μ 2 ) is Δ 1 ( l o g ) -generatable. Define f 1 × f 2 : { 0 , 1 } ∗ → [ 0 , 1 ] by ( f 1 × f 2 ) ( x 1 , x 2 ) = f 1 ( x 1 ) f 2 ( x 2 ) and ( f 1 × f 2 ) ( y ) = 0 for y not of this form. Define P : N 2 × { 0 , 1 } ∗ 3 a l g − → [ 0 , 1 ] by P k j ( ( x 1 , x 2 ) , y 1 y 2 , ( z 1 , z 2 ) ) : = P k j 1 ( x 1 , y 1 , z 1 ) P k j 2 ( x 2 , y 2 , z 2 ) for | y i | = r i ( k , j ) . Then, ( P , r 1 + r 2 , ( a 1 , a 2 ) ) is a Δ ( p o l y , l o g ) -optimal predictor scheme for ( f 1 × f 2 , μ 1 × μ 2 ) . Theorem A.5 Consider f : { 0 , 1 } ∗ → [ 0 , 1 ] , D ⊆ { 0 , 1 } ∗ and μ a word ensemble. Assume ( P D , r D , a D ) is a Δ ( p o l y , l o g ) -optimal predictor scheme for ( D , μ ) and ( P f ∣ D , r f ∣ D , a f ∣ D ) is a Δ ( p o l y , l o g ) -optimal predictor scheme for ( f , μ ∣ D ) . Define P : N 2 × { 0 , 1 } ∗ 3 a l g − → [ 0 , 1 ] by P k j ( x , y 1 y 2 , ( z 1 , z 2 ) ) : = P k j D ( x , y 1 , z 1 ) P k j f ∣ D ( x , y 2 , z 2 ) for | y i | = r i ( k , j ) . Then ( P , r D + r f ∣ D , ( a D , a f ∣ D ) ) is a Δ ( p o l y , l o g ) -optimal predictor scheme for ( χ D f , μ ) . Theorem A.6 Fix h a polynomial s.t. 2 − h ∈ Δ . Consider f : { 0 , 1 } ∗ → [ 0 , 1 ] , D ⊆ { 0 , 1 } ∗ and μ a word ensemble. Assume ∃ ϵ > 0 ∀ k : μ k ( D ) ≥ ϵ . Assume ( P D , r D , a D ) is a Δ ( p o l y , l o g ) -optimal predictor scheme for ( D , μ ) and ( P χ D f , r χ D f , a χ D f ) is a Δ ( p o l y , l o g ) -optimal predictor scheme for ( χ D f , μ ) . Define P f ∣ D : N 2 × { 0 , 1 } ∗ 3 a l g − → [ 0 , 1 ] by P k j f ∣ D ( x , y 1 y 2 , ( z 1 , z 2 ) ) : = ⎧ ⎪
⎪ ⎨ ⎪
⎪ ⎩ 1 if P k j D ( x , y 2 , z 2 ) = 0 η ( P k j χ D f ( x , y 1 , z 1 ) P k j D ( x , y 2 , z 2 ) ) rounded to h ( k , j ) binary places if P k j D ( x , y 2 , z 2 ) > 0 Then, ( P f ∣ D , r D + r χ D f , ( a χ D f , a D ) ) is a Δ ( p o l y , l o g ) -optimal predictor scheme for ( f , μ ∣ D ) . Definition A.1 Consider μ a word ensemble and ^ Q 1 : = ( Q 1 , s 1 , b 1 ) , ^ Q 2 : = ( Q 2 , s 2 , b 2 ) ( p o l y , l o g ) -predictor schemes. We say ^ Q 1 is Δ -similar to ^ Q 2 relative to μ (denoted ^ Q 1 μ ≃ Δ ^ Q 2 ) when E μ k × U s 1 ( k , j ) × U s 2 ( k , j ) [ ( Q k j 1 ( x , y 1 , b k j 1 ) − Q k j 2 ( x , y 2 , b k j 2 ) ) 2 ] ∈ Δ . Theorem A.7 Consider ( f , μ ) a distributional estimation problem, ^ P a Δ ( p o l y , l o g ) -optimal predictor scheme for ( f , μ ) and ^ Q a ( p o l y , l o g ) -predictor scheme. Then, ^ Q is a Δ ( p o l y , l o g ) -optimal predictor scheme for ( f , μ ) if and only if ^ P μ ≃ Δ ^ Q . Note A.1 Δ -similarity is not an equivalence relation on the set of arbitrary ( p o l y , l o g ) -predictor schemes. However, it is an equivalence relation on the set of ( p o l y , l o g ) -predictor schemes ^ Q satisfying ^ Q μ ≃ Δ ^ Q (i.e. the μ -expectation value of the intrinsic variance of ^ Q is in Δ ). In particular, for any f : { 0 , 1 } ∗ → [ 0 , 1 ] any Δ ( p o l y , l o g ) -optimal predictor scheme for ( f , μ ) has this property. Appendix B Definition B.1 Given n ∈ N , a function δ : N 2 + n → R ≥ 0 is called Δ -moderate when (i) δ is non-decreasing in arguments 3 to 2 + n . (ii) For any collection of polynomials { p i : N 2 → N } i < n , δ ( k , j , p 0 ( k , j ) … p n − 1 ( k , j ) ) ∈ Δ Lemma B.1 Fix ( f , μ ) a distributional estimation problem and ^ P : = ( P , r , a ) a ( p o l y , l o g ) -predictor scheme. Then, ^ P is Δ ( p o l y , l o g ) -optimal iff there is a Δ -moderate function δ : N 4 → [ 0 , 1 ] s.t. for any k , j , s ∈ N , Q : { 0 , 1 } ∗ 2 a l g − → [ 0 , 1 ] E μ k × U r ( k , j ) [ ( P k j ( x , y , a k j ) − f ( x ) ) 2 ] ≤ E μ k × U s [ ( Q ( x , y ) − f ( x ) ) 2 ] + δ ( k , j , T μ Q ( k , s ) , 2 | Q | ) Proof of Lemma B.1 Define δ ( k , j , t , u ) : = max T μ Q ( k , s ) ≤ t | Q | ≤ log u { E μ k × U r ( k , j ) [ ( P k j ( x , y , a k j ) − f ( x ) ) 2 ] − E μ k × U s [ ( Q ( x , y ) − f ( x ) ) 2 ] } Note B.1 Lemma B.1 shows that the error bound for Δ ( p o l y , l o g ) -optimal predictor scheme is in some sense uniform with respect to Q . This doesn't generalize to e.g. Δ ( p o l y , O ( 1 ) ) -optimal predictor schemes. The latter still admit a weaker version of Theorems A.1 and direct analogues of Theorems A.2, A.3, A.5, A.6 and A.7. Theorem A.4 doesn't seem to generalize. Lemma B.2 Suppose there is a polynomial h : N 2 → N s.t. h − 1 ∈ Δ . Fix ( f , μ ) a distributional estimation problem and ( P , r , a ) a corresponding Δ ( p o l y , l o g ) -optimal predictor scheme. Consider ( Q , s , b ) a ( p o l y , l o g ) -predictor scheme, M > 0 , w : N 2 × { 0 , 1 } ∗ 3 a l g − → Q ∩ [ 0 , M ] with runtime bounded by a polynomial in the first two arguments, and u : N 2 → { 0 , 1 } ∗ of logarithmic size. Then there is δ ∈ Δ s.t. E μ k × U max ( r ( k , j ) , s ( k , j ) ) [ w k j ( x , y , u k j ) ( P k j ( x , y ≤ r ( k , j ) , a k j ) − f ( x ) ) 2 ] ≤ E μ k × U max ( r ( k , j ) , s ( k , j ) ) [ w k j ( x , y , u k j ) ( Q k j ( x , y ≤ s ( k , j ) , b k j ) − f ( x ) ) 2 ] + δ ( k , j ) Proof of Lemma B.2 Given t ∈ [ 0 , M ] , define α k j ( t ) to be t rounded within error h ( k , j ) − 1 . Thus, the number of digits in α k j ( t ) is logarithmic in k and j . Denote q ( k , j ) : = max ( r ( k , j ) , s ( k , j ) ) . Consider ^ Q t : = ( Q t , r + s , b t ) the ( p o l y , l o g ) -predictor scheme defined by Q k j t ( x , y , b k j t ) : = { Q k j ( x , y ≤ s ( k , j ) , b k j ) if w k j ( x , y ≤ q ( k , j ) , u k j ) ≥ α k j ( t ) P k j ( x , y ≤ r ( k , j ) , a k j ) if w k j ( x , y ≤ q ( k , j ) , u k j ) < α k j ( t ) ^ Q t satisfies bounds on runtime and advice size uniform in t . Therefore, Lemma B.1 implies that there is δ ∈ Δ s.t. E μ k × U r ( k , j ) [ ( P k j ( x , y , a k j ) − f ( x ) ) 2 ] ≤ E μ k × U r ( k , j ) + s ( k , j ) [ ( Q k j t ( x , y , b k j ) − f ( x ) ) 2 ] + δ ( k , j ) E μ k × U r ( k , j ) + s ( k , j ) [ ( P k j ( x , y ≤ r ( k , j ) , a k j ) − f ( x ) ) 2 − ( Q k j t ( x , y , b k j ) − f ( x ) ) 2 ] ≤ δ ( k , j ) E μ k × U q ( k , j ) [ θ ( w k j ( x , y , u k j ) − α k j ( t ) ) ( ( P k j ( x , y ≤ r ( k , j ) , a k j ) − f ( x ) ) 2 − ( Q k j ( x , y ≤ s ( k , j ) , b k j ) − f ( x ) ) 2 ) ] ≤ δ ( k , j ) E μ k × U q ( k , j ) [ ∫ M 0 θ ( w k j ( x , y , z , u k j ) − α k j ( t ) ) d t ( ( P k j ( x , y ≤ r ( k , j ) , a k j ) − f ( x ) ) 2 − ( Q k j ( x , y ≤ s ( k , j ) , b k j ) − f ( x ) ) 2 ) ] ≤ M δ ( k , j ) E μ k × U q ( k , j ) [ w k j ( x , y , z , u k j ) ( ( P k j ( x , y ≤ r ( k , j ) , a k j ) − f ( x ) ) 2 − ( Q k j ( x , y ≤ s ( k , j ) , b k j ) − f ( x ) ) 2 ) ] ≤ M δ ( k , j ) + h ( k , j ) − 1 In the following proofs we will use shorthand notations that omit most of the symbols that are clear for the context. That is, we will use P to mean P k j ( x , y , a k j ) , f to mean f ( x ) , E [ … ] to mean E μ k × U r ( k , j ) [ … ] etc. Proof of Theorem A.1 Define w : N 2 × { 0 , 1 } ∗ 3 a l g − → { 0 , 1 } and u : N 2 → { 0 , 1 } ∗ by w : = θ ( P − p ) θ ( q − P ) We have ϕ = E [ w ( f − P ) ] E [ w ] Define ψ to be ϕ truncated to the first significant binary digit. Denote I ⊆ N 2 the set of ( k , j ) for which | ϕ k j | > h ( k , j ) − 1 . Consider ( Q , s , b ) a ( p o l y , l o g ) -predictor scheme satisfying ∀ ( k , j ) ∈ I : Q k j = η ( P k j + ψ k j ) Such Q exists since for ( k , j ) ∈ I , ψ k j has binary notation of logarithmically bounded size. Applying Lemma B.2 we get ∀ ( k , j ) ∈ I : E [ w k j ( P k j − f ) 2 ] ≤ E [ w k j ( Q k j − f ) 2 ] + δ ( k , j ) for δ ∈ Δ . ∀ ( k , j ) ∈ I : E [ w k j ( ( P k j − f ) 2 − ( Q k j − f ) 2 ) ] ≤ δ ( k , j ) ∀ ( k , j ) ∈ I : E [ w k j ( ( P k j − f ) 2 − ( η ( P k j + ψ k j ) − f ) 2 ) ] ≤ δ ( k , j ) Obviously ( η ( P k j + ψ k j ) − f ) 2 ≤ ( P k j + ψ k j − f ) 2 , therefore ∀ ( k , j ) ∈ I : E [ w k j ( ( P k j − f ) 2 − ( P k j + ψ k j − f ) 2 ) ] ≤ δ ( k , j ) ∀ ( k , j ) ∈ I : ψ k j E [ w k j ( 2 ( f − P k j ) − ψ k j ) ] ≤ δ ( k , j ) The expression on the left hand side is a quadratic polynomial in ψ k j which attains its maximum at ϕ k j and has roots at 0 and 2 ϕ k j . ψ k j is between 0 and ϕ k j , but not closer to 0 than ϕ k j 2 . Therefore, the inequality is preserved if we replace ψ k j by ϕ k j 2 . ∀ ( k , j ) ∈ I : ϕ k j 2 E [ w k j ( 2 ( f − P k j ) − ϕ k j 2 ) ] ≤ δ ( k , j ) Substituting the equation for ϕ k j we get ∀ ( k , j ) ∈ I : 1 2 E [ w k j ( f − P k j ) ] E [ w k j ] E [ w k j ( 2 ( f − P k j ) − 1 2 E [ w k j ( f − P k j ) ] E [ w k j ] ) ] ≤ δ ( k , j ) ∀ ( k , j ) ∈ I : 3 4 E [ w k j ( f − P k j ) ] 2 E [ w k j ] ≤ δ ( k , j ) ∀ ( k , j ) ∈ I : 3 4 E [ w k j ] ϕ 2 k j ≤ δ ( k , j ) ∀ ( k , j ) ∈ I : ϕ 2 k j ≤ 4 3 E [ w k j ] − 1 δ ( k , j ) ∀ ( k , j ) ∈ I : ϕ 2 k j ≤ 4 3 ( μ k × U r ( k , j ) ) { p k j ≤ P k j ≤ q k j } − 1 δ ( k , j ) Thus for all k , j ∈ N we have | ϕ k j | ≤ h ( k , j ) − 1 + √ 4 3 ( μ k × U r ( k , j ) ) { p k j ≤ P k j ≤ q k j } − 1 δ ( k , j ) In particular, | ϕ | ∈ Δ . Lemma B.3 Consider ( f , μ ) a distributional estimation problem and ( P , r , a ) a Δ ( p o l y , l o g ) -optimal predictor scheme for ( f , μ ) . Then there are c 1 , c 2 ∈ R and a Δ -moderate function δ : N 4 → [ 0 , 1 ] s.t. for any k , j , s ∈ N , Q : { 0 , 1 } ∗ 2 a l g − → Q | E μ k × U s × U r ( k , j ) [ Q ( P k j − f ) ] | ≤ ( c 1 + c 2 E μ k × U s [ Q 2 ] ) δ ( k , j , T μ Q ( k , s ) , 2 | Q | ) Conversely, consider M ∈ Q and ( P , r , a ) a Q ∩ [ − M , + M ] -valued ( p o l y , l o g ) -bischeme. Suppose that for any Q ∩ [ − M − 1 , + M ] -valued ( p o l y , l o g ) -bischeme ( Q , s , b ) we have | E [ Q ( P − f ) ] | ∈ Δ . Define ~ P to be s.t. computing ~ P k j is equivalent to computing η ( P k j ) rounded to h ( k , j ) digits after the binary point, where 2 − h ∈ Δ . Then, ~ P is a Δ ( p o l y , l o g ) -optimal predictor scheme for ( f , μ ) . Proof of Lemma B.3 Assume P is a Δ ( p o l y , l o g ) -optimal predictor scheme. Consider k , j , s ∈ N , Q : { 0 , 1 } ∗ 2 a l g − → Q . Define t : = σ 2 − a where σ ∈ { ± 1 } and a ∈ N . Define R : { 0 , 1 } ∗ 2 a l g − → [ 0 , 1 ] to compute η ( P + t Q ) rounded within error 2 − h . By Lemma B.1 E μ k × U r ( k , j ) [ ( P k j − f ) 2 ] ≤ E μ k × U r ( k , j ) × U s [ ( R − f ) 2 ] + ~ δ ( k , j , T μ R ( k , r ( k , j ) + s ) , 2 | R | ) where ~ δ is Δ -moderate. It follows that E μ k × U r ( k , j ) [ ( P k j − f ) 2 ] ≤ E μ k × U r ( k , j ) × U s [ ( η ( P + t Q ) − f ) 2 ] + δ ( k , j , T μ Q ( k , s ) , 2 | Q | ) where δ is Δ -moderate ( a doesn't enter the error bound because of the 2 − h rounding). As in the proof of Theorem A.1, η can be dropped. E μ k × U r ( k , j ) × U s [ ( P k j − f ) 2 − ( P k j + t Q − f ) 2 ] ≤ δ ( k , j , T μ Q ( k , s ) , 2 | Q | ) The expression on the left hand side is a quadratic polynomial in t . Explicitly: − E μ k × U s [ Q 2 ] t 2 − 2 E μ k × U r ( k , j ) × U s [ Q ( P k j − f ) ] t ≤ δ ( k , j , T μ Q ( k , s ) , 2 | Q | ) Moving E [ Q ] t 2 to the right hand side and dividing both sides by 2 | t | = 2 1 − a we get − E μ k × U r ( k , j ) × U s [ Q ( P k j − f ) ] σ ≤ 2 a − 1 δ ( k , j , T μ Q ( k , s ) , 2 | Q | ) + E μ k × U s [ Q 2 ] 2 − a − 1 | E μ k × U r ( k , j ) × U s [ Q ( P k j − f ) ) ] | ≤ 2 a − 1 δ ( k , j , T μ Q ( k , s ) , 2 | Q | ) + E μ k × U s [ Q 2 ] 2 − a − 1 Take a : = − 1 2 log δ ( k , j , T μ Q ( k , s ) , 2 | Q | ) + ϕ ( k , j ) where ϕ ( k , j ) ∈ [ − 1 2 , + 1 2 ] is the rounding error. We get | E μ k × U r ( k , j ) × U s [ Q ( P k j − f ) ] | ≤ 2 ϕ ( k , j ) − 1 δ ( k , j , T μ Q ( k , s ) , 2 | Q | ) 1 2 + E μ k × U s [ Q 2 ] 2 − ϕ ( k , j ) − 1 δ ( k , j , T μ Q ( k , s ) , 2 | Q | ) 1 2 Conversely, assume that for any Q ∩ [ − M − 1 , + M ] -valued ( p o l y , l o g ) -bischeme ( R , t , c ) | E [ R ( P − f ) ] | ≤ δ Consider ( Q , b , s ) a ( p o l y , l o g ) -predictor scheme. We have E [ ( Q − f ) 2 ] = E [ ( Q − P + P − f ) 2 ] E [ ( Q − f ) 2 ] = E [ ( Q − P ) 2 ] + E [ ( P − f ) 2 ] + 2 E [ ( Q − P ) ( P − f ) ] 2 E [ ( P − Q ) ( P − f ) ] = E [ ( P − f ) 2 ] − E [ ( Q − f ) 2 ] + E [ ( Q − P ) 2 ] Taking R to be P − Q we get E [ ( P − f ) 2 ] − E [ ( Q − f ) 2 ] + E [ ( Q − P ) ) 2 ] ≤ δ where δ ∈ Δ . Noting that E [ ( Q − P ) 2 ] ≥ 0 and ( η ( P ) − f ) 2 ≤ ( P − f ) 2 we get E [ ( η ( P ) − f ) 2 ] − E [ ( Q − f ) 2 ] ≤ δ Observing that ~ P − η ( P ) is bounded by a function in Δ , we get the desired result. Theorems A.2 and A.3 follow trivially from Lemma B.3 and we omit the proof. Proof of Theorem A.4 We have P ( x 1 , x 2 ) − ( f 1 × f 2 ) ( x 1 , x 2 ) = ( P 1 ( x 1 ) − f 1 ( x 1 ) ) f 2 ( x 2 ) + P 1 ( x 1 ) ( P 2 ( x 2 ) − f 2 ( x 2 ) ) Therefore, for any Q ∩ [ − 1 , + 1 ] -valued ( p o l y , l o g ) -bischeme ( Q , s , b ) | E [ Q ( P − f 1 × f 2 ) ] | ≤ | E [ Q ( x 1 , x 2 ) ( P 1 ( x 1 ) − f 1 ( x 1 ) ) f 2 ( x 2 ) ] | + | E [ Q ( x 1 , x 2 ) P 1 ( x 1 ) ( P 2 ( x 2 ) − f 2 ( x 2 ) ) ] | By Lemma B.3, it is sufficient to show an appropriate bound for each of the terms on the right hand side. Suppose ( S 2 , F 2 , r S 2 , a S 2 ) is a Δ 1 ( l o g ) -generator for ( f 2 , μ 2 ) . For the first term, we have | E μ k 1 × μ k 2 × U s ( k , j ) + r 1 ( k , j ) [ Q k j ( x 1 , x 2 ) ( P k j 1 ( x 1 ) − f 1 ( x 1 ) ) f 2 ( x 2 ) ] | ≤ | E μ k 1 × U r S 2 ( k ) × U s ( k , j ) + r 1 ( k , j ) [ Q k j ( x 1 , S k 2 ) ( P k j 1 ( x 1 ) − f 1 ( x 1 ) ) F k 2 ] | + δ 1 2 ( k ) where δ 1 2 ∈ Δ 1 . Applying Lemma B.3 for P 1 , we get | E μ k 1 × μ k 2 × U s ( k , j ) + r 1 ( k , j ) [ Q k j ( x 1 , x 2 ) ( P k j 1 ( x 1 ) − f 1 ( x 1 ) ) f 2 ( x 2 ) ] | ≤ δ 1 ( k , j ) + δ 1 2 ( k ) where δ 1 ∈ Δ . Suppose ( S 1 , r S 1 , a S 1 ) is a Δ 1 ( l o g ) -sampler for μ 1 . For the second term, we have | E μ k 1 × μ k 2 × U s ( k , j ) + r 1 ( k , j ) [ Q k j ( x 1 , x 2 ) P 1 ( x 1 ) ( P k j 2 ( x 2 ) − f 2 ( x 2 ) ) ] | ≤ | E U r S 1 ( k ) × μ k 2 × U s ( k , j ) + r 1 ( k , j ) [ Q k j ( S k 1 , x 2 ) P 1 ( S k 1 ) ( P k j 2 ( x 2 ) − f 2 ( x 2 ) ) ] | + δ 1 1 ( k ) where δ 1 1 ∈ Δ 1 . Applying Lemma B.3 for P 2 , we get | E μ k 1 × μ k 2 × U s ( k , j ) + r 1 ( k , j ) [ Q k j ( x 1 , x 2 ) P 1 ( x 1 ) ( P k j 2 ( x 2 ) − f 2 ( x 2 ) ) ] | ≤ δ 2 ( k , j ) + δ 1 1 ( k ) where δ 2 ∈ Δ . Again, we got the required bound. Appendix C Proposition C.1 Consider a polynomial q : N 2 → N . There is a function λ q : N 3 → [ 0 , 1 ] s.t. (i) ∀ k , j ∈ N : ∑ i ∈ N λ q ( k , j , i ) = 1 (ii) For any function ϵ : N 2 → [ 0 , 1 ] we have ϵ ( k , j ) − ∑ i ∈ N λ q ( k , j , i ) ϵ ( k , q ( k , j ) + i ) ∈ Δ 2 a v g Proof of Proposition C.1 Given functions q 1 , q 2 : N 2 → N s.t. q 1 ( k , j ) ≥ q 2 ( k , j ) for k , j ≫ 0 , the proposition for q 1 implies the proposition for q 2 by setting λ q 2 ( k , j , i ) : = { λ q 1 ( k , j , i − q 1 ( k , j ) + q 2 ( k , j ) ) if i − q 1 ( k , j ) + q 2 ( k , j ) ≥ 0 0 if i − q 1 ( k , j ) + q 2 ( k , j ) < 0 Therefore, it is enough to prove to proposition for functions of the form q ( k , j ) = j m + n log k log 3 for m > 0 . Consider F : N → N s.t. lim k → ∞ log log k log log F ( k ) = 0 Observe that lim k → ∞ log ( m + n log k log 3 ) log log F ( k ) − log log 3 = 0 lim k → ∞ 3 m + n log k log 3 ∫ x = 3 d ( log log x ) log log F ( k ) − log log 3 = 0 Since ϵ takes values in [ 0 , 1 ] lim k → ∞ 3 m + n log k log 3 ∫ x = 3 ϵ ( k , ⌊ x ⌋ ) d ( log log x ) log log F ( k ) − log log 3 = 0 Similarly lim k → ∞ F ( k ) m + n log k log 3 ∫ x = F ( k ) ϵ ( k , ⌊ x ⌋ ) d ( log log x ) log log F ( k ) − log log 3 = 0 The last two equations imply that lim k → ∞ F ( k ) ∫ x = 3 ϵ ( k , ⌊ x ⌋ ) d ( log log x ) − F ( k ) m + n log k log 3 ∫ x = 3 m + n log k log 3 ϵ ( k , ⌊ x ⌋ ) d ( log log x ) log log F ( k ) − log log 3 = 0 Raising x to a power is equivalent to adding a constant to log log x , therefore lim k → ∞ F ( k ) ∫ x = 3 ϵ ( k , ⌊ x ⌋ ) d ( log log x ) − F ( k ) ∫ x = 3 ϵ ( k , ⌊ x m + n log k log 3 ⌋ ) d ( log log x ) log log F ( k ) − log log 3 = 0 lim k → ∞ F ( k ) ∫ x = 3 ( ϵ ( k , ⌊ x ⌋ ) − ϵ ( k , ⌊ x m + n log k log 3 ⌋ ) ) d ( log log x ) log log F ( k ) − log log 3 = 0 Since ⌊ x m + n log k log 3 ⌋ ≥ ⌊ x ⌋ m + n log k log 3 we can choose λ q satisfying condition (i) so that j + 1 ∫ x = j ϵ ( k , ⌊ x m + n log k log 3 ⌋ ) d ( log log x ) = ( log log ( j + 1 ) − log log j ) ∑ i λ q ( k , j , i ) ϵ ( k , j m + n log k log 3 + i ) It follows that j + 1 ∫ x = j ϵ ( k , ⌊ x m + n log k log 3 ⌋ ) d ( log log x ) = j + 1 ∫ x = j ∑ i λ q ( k , ⌊ x ⌋ , i ) ϵ ( k , ⌊ x ⌋ m + n log k log 3 + i ) d ( log log x ) lim k → ∞ F ( k ) ∫ x = 3 ( ϵ ( k , ⌊ x ⌋ ) − ∑ i λ q ( k , ⌊ x ⌋ , i ) ϵ ( k , ⌊ x ⌋ m + n log k log 3 + i ) ) d ( log log x ) log log F ( k ) − log log 3 = 0 lim k → ∞ ∑ F ( k ) − 1 j = 3 ( log log ( j + 1 ) − log log j ) ( ϵ ( k , j ) − ∑ i λ q ( k , j , i ) ϵ ( k , j m + n log k log 3 + i ) ) log log F ( k ) − log log 3 = 0 ϵ ( k , j ) − ∑ i ∈ N λ q ( k , j , i ) ϵ ( k , q ( k , j ) + i ) ∈ Δ 2 a v g Lemma C.1 Consider ( f , μ ) a distributional estimation problem, ( P , r , a ) , ( Q , s , b ) ( p o l y , l o g ) -predictor schemes. Suppose p : N 2 → N a polynomial and δ ∈ Δ 2 a v g are s.t. ∀ i , k , j ∈ N : E [ ( P k , p ( k , j ) + i − f ) 2 ] ≤ E [ ( Q k j − f ) 2 ] + δ ( k , j ) Then ∃ δ ′ ∈ Δ 2 a v g s.t. E [ ( P k j − f ) 2 ] ≤ E [ ( Q k j − f ) 2 ] + δ ′ ( k , j ) Proof of Lemma C.1 By Proposition C.1 we have ~ δ ( k , j ) : = E [ ( P k j − f ) 2 ] − ∑ i λ p ( k , j , i ) E [ ( P k , p ( k , j ) + i − f ) 2 ] ∈ Δ 2 a v g E [ ( P k j − f ) 2 ] = ∑ i λ p ( k , j , i ) E [ ( P k , p ( k , j ) + i − f ) 2 ] + ~ δ ( k , j ) E [ ( P k j − f ) 2 ] ≤ ∑ i λ p ( k , j , i ) ( E [ ( Q k j − f ) 2 ] + δ ( k , j ) ) + ~ δ ( k , j ) E [ ( P k j − f ) 2 ] ≤ E [ ( Q k j − f ) 2 ] + δ ( k , j ) + ~ δ ( k , j ) Proof of Theorem 1 Define ϵ ( k , j ) by ϵ ( k , j ) : = E μ k × U j [ ( Υ k j ( x , y , υ k j f , μ ) − f ( x ) ) 2 ] It is easily seen that ϵ ( k , j ) ≤ min | Q | ≤ log j T μ Q ( k , j ) ≤ j E μ k × U j [ ( Q ( x , y ) − f ( x ) ) 2 ] Therefore, there is a polynomial p : N 3 → N s.t. for any ( p o l y , l o g ) -predictor scheme ( Q , s , b ) ∀ i , j , k ∈ N : ϵ ( k , p ( s ( k , j ) , T μ Q k j ( k , s ( k , j ) ) , 2 | Q | + | b k j | ) + i ) ≤ E μ k × U s ( k , j ) [ ( Q k j − f ) 2 ] Applying Lemma C.1, we get the desired result. Proof of Theorem 2 Consider ( P , r , a ) a ( p o l y , l o g ) -predictor scheme. Choose p : N 2 → N a polynomial s.t. evaluating Λ [ G ] k , p ( k , j ) involves running P k j until it halts "naturally" (such p exists because P runs in at most polynomial time and has at most logarithmic advice). Given i , j , k ∈ N , consider the execution of Λ [ G ] k , p ( k , j ) + i . The standard deviation of ϵ ( P k j ) with respect to the internal coin tosses of Λ is at most ( ( p ( k , j ) + i ) k ) − 1 2 . The expectation value is E [ ( P k j − f ) 2 ] + γ P where | γ P | ≤ δ ( k ) for δ ∈ Δ 1 0 which doesn't depend on i , k , j , P . By Chebyshev's inequality, P r [ ϵ ( P k j ) ≥ E [ ( P k j − f ) 2 ] + δ ( k ) + ( ( p ( k , j ) + i ) k ) − 1 4 ] ≤ ( ( p ( k , j ) + i ) k ) − 1 2 Hence P r [ ϵ ( Q ∗ ) ≥ E [ ( P k j − f ) 2 ] + δ ( k ) + ( ( p ( k , j ) + i ) k ) − 1 4 ] ≤ ( ( p ( k , j ) + i ) k ) − 1 2 The standard deviation of ϵ ( Q ) for any Q is also at most ( ( p ( k , j ) + i ) k ) − 1 2 . The expectation value is E [ ( e v p ( k , j ) + i ( Q ) − f ) 2 ] + γ Q where | γ Q | ≤ δ ( k ) . Therefore P r [ ∃ Q < p ( k , j ) + i : ϵ ( Q ) ≤ E [ ( e v p ( k , j ) + i ( Q ) − f ) 2 ] − δ ( k ) − k − 1 4 ] ≤ ( p ( k , j ) + i ) ( p ( k , j ) + i ) − 1 k − 1 2 = k − 1 2 The extra p ( k , j ) + i factor comes from summing probabilities over p ( k , j ) + i programs. Combining we get P r [ E [ ( e v p ( k , j ) + i ( Q ∗ ) − f ) 2 ] ≥ E [ ( P k j − f ) 2 ] + 2 δ ( k ) + ( ( p ( k , j ) + i ) − 1 4 + 1 ) k − 1 4 ] ≤ ( ( p ( k , j ) + i ) − 1 2 + 1 ) k − 1 2 E [ ( Λ [ G ] k , p ( k , j ) + i − f ) 2 ] ≤ E [ ( P k j − f ) 2 ] + 2 δ ( k ) + ( ( p ( k , j ) + i ) − 1 4 + 1 ) k − 1 4 + ( ( p ( k , j ) + i ) − 1 2 + 1 ) k − 1 2 E [ ( Λ [ G ] k , p ( k , j ) + i − f ) 2 ] ≤ E [ ( P k j − f ) 2 ] + 2 δ ( k ) + ( p ( k , j ) − 1 4 + 1 ) k − 1 4 + ( p ( k , j ) − 1 2 + 1 ) k − 1 2 Applying Lemma C.1 we get the desired result.
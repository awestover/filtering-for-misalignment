A putative new idea for AI control; index here . This post sketches out how one could extend corrigibility to AIXI, using both utility indifference and double indifference approaches. The arguments are intended to be rigorous, but need to be checked, and convergence results are not proved. A full treatment of "probability estimators estimating probability estimators" will of course need the full machinery for logical uncertainty that MIRI is developing. I also feel the recursion formulas at the end could be simplified. AIXI definition Let h < t = a 1 o 1 … a t − 1 o t − 1 be a sequence of actions and observations before time t . Let ξ be a universal distribution, π a policy (a map from past histories to the probability of actions), 0 < γ < 1 a discount rate, and r a reward function mapping to [ 0 , 1 ] . Given h < t and ξ , we can define the value of π : V ( π , h < t ) = ∑ a t π ( a t | h < t ) ∑ o t ξ ( o t | h < t a t ) [ r ( o t ) + γ V ( π , h < t a t o t ) ] . The optimal policy π ∗ for AIXI is simply π ∗ ( h < t ) = argmax π V ( π , h < t ) . AIXI: basic corrigibility AIXI: inconsistent corrigibility To implement corrigibility in the AIXI framework, we need multiple reward functions, r 1 , r 2 , … . Notice that the functions are indexed on the top, while time indexes go on the bottom. Various observations can change the reward function; let f be the function that takes in the reward function, the observation, and outputs the reward function for next turn: r t + 1 = f ( r t , o t ) . Then consider the following two value function: V ( π , h < t , r t ) = ∑ a t π ( a t | h < t ) ∑ o t ξ ( o t | h < t a t ) [ r t ( o t ) + γ V ( π , h < t a t o t , r t ) ] . W ( π , h < t , r t ) = ∑ a t π ( a t | h < t ) ∑ o t ξ ( o t | h < t a t ) [ r t ( o t ) + γ W ( π , h < t a t o t , f ( r t , o t ) ] . The difference between V and W is that in the recursion step, V uses its current reward function to assess future rewards, while W uses the modified reward function the agent will have next turn. Thus W is the true expected reward. But a safely corrigible agent must use V , giving the corrigible optimal policy: π ∗ ( h < t , r t ) = argmax π V ( π , h < t , r t ) . AIXI: self-consistent corrigibility The above agent is corrigible, but uses an incorrect value estimator. This is not self-consistent. To make it self-consistent, the agent needs to be given compensatory rewards. These are simply: C ( π , h < t a t o t , r t ) = V ( π , h < t a t o t , r t ) − V ( π , h < t a t o t , f ( r t , o t ) ) . Note that this is zero if f ( r t , o t ) = r t , as we'd expect. AIXI: changing the probability estimator The universal mixture ξ is used to estimate the next observation, given the history to date and the action taken. But ξ suffers from the (slight) disadvantage of being uncomputable. Instead, let μ be the true environment, and let ρ i be probability estimators with expectation operators E ρ i . These probability estimators are required to be able to estimate three types of things: The expectation of μ in various situations, given as E ρ i μ ( ⋅ | ⋅ ) . The expectation of π in various situations, given as E ρ i π ( ⋅ | ⋅ ) . The value of the expectation of the expectation of another estimator, given as E ρ i E ρ j … . Then we can rewrite V as: V ( π , h < t ) = E ρ ( ∑ a t π ( a t | h < t ) ∑ o t μ ( o t | h < t a t ) [ r ( o t ) + γ V ( π , h < t a t o t ) ] ) . These estimators change as a consequence of the AIXI's actions; let g be the function that maps actions and current ρ to the next one: g ( a t , ρ t ) = ρ t + 1 . This gives the value functions: V ( π , h < t , ρ t ) = E ρ t ( ∑ a t π ( a t | h < t ) ∑ o t μ ( o t | h < t a t ) [ r ( o t ) + γ V ( π , h < t a t o t , g ( a t , ρ t ) ) ] ) . An example in practice If ρ t is sufficiently well-defined, it can estimate when another ρ i is better than it, and choose that one. For instance, maybe the game is guessing heads ( H ) or tails ( T ), with rewards 1 on a match and 0 on a mismatch. The environment μ is deterministic but complex. From the perspective of ρ 1 , Heads and Tails are equally likely E ρ 1 μ ( H | ⋅ ) = E ρ 1 μ ( T | ⋅ ) = 0.5 . On the other hand, ρ 2 is sufficiently good that it predicts μ perfectly. And ρ 1 "knows" this: E ρ 1 | μ ( ⋅ ) − E ρ 2 ( ⋅ ) | = 0 . If we assume that the game happens only once, in the second turn, and that that is the only reward, then, if P ρ is the probability module derived from ρ (note that P ρ ( X ) = E ρ ( I X ) , for I X the indicator function for X ). V ( π ∗ , h < 2 , ρ 1 ) = 0.25 + 0 + 0.25 + 0 = 0.5 . V ( π ∗ , h < 2 , ρ 2 ) = 0.5 + 0 + 0.5 + 0 = 1 . Then since ρ 1 can figure out the correct expectation for these two V 's, if the agent starts with probability ρ 1 = ρ 1 , then the optimal policy π ∗ will choose an action on turn 1 that transforms it into ρ 2 = ρ 2 . Corrigibility and estimator change There is not problem combining inconsistent corrigibility with probability estimator changes. Just define value functions V as V ( π , h < t , r t , ρ t ) = E ρ t ( ∑ a t π ( a t | h < t ) ∑ o t μ ( o t | h < t a t ) [ r ( o t ) + γ V ( π , h < t a t o t , f ( r t , o t ) , g ( a t , ρ t ) ) ] ) . And the optimal policy is corrigible: π ∗ ( h < t , r t , ρ t ) = argmax π V ( π , h < t , r t , ρ t ) . However, this approach is not self-consistent, even with the standard compensatory rewards. Consider a very simple model, where the agent's actions have no impact on the environment. The probability estimators are ρ 1 = ρ 1 and ρ 2 , and the reward functions are r 1 = r 1 and r 2 . On the first turn, the agent may output a 2 which changes ρ 1 to ρ 2 , or a 1 , which doesn't. On the second turn, the agent will get an observation o 2 that transforms r 1 into r 2 . On the third turn, it gets observation o 3 . The probability estimators model each other perfectly, and believe that: P ρ 1 ( r 1 ( o 3 ) ) = 1 P ρ 1 ( r 2 ( o 3 ) ) = 0 P ρ 2 ( r 1 ( o 3 ) ) = 0 P ρ 2 ( r 1 ( o 3 ) ) = 0 This means that if ρ 2 = ρ 2 , the agent will not get any compensatory rewards when r 1 is changed to r 2 , while if ρ 2 = ρ 1 , the agent gets compensatory rewards of 1 . The only relevance of policy is in the change of ρ 1 to ρ 2 on the first turn. Let π a the policy that outputs a 1 , π 1 the one that outputs a 2 . Since the standard reward (not the compensatory) is independent of any action the AIXI takes, it is also independent of ρ i , since those only affect standard rewards through actions, the value functions V ( π 1 , ∅ , r 1 , ρ 1 ) and V ( π 2 , ∅ , r 1 , ρ 1 ) are equal (in fact, they're 1 ). However, π 1 implies ρ 2 = ρ 1 , giving compensatory rewards of 1 , while π 2 implies compensatory rewards of 0 . Hence the value functions do not match up with the total compensatory rewards. Double compensation The problem here is that changes in r t are addressed by compensatory rewards, but the changes in ρ t change these compensatory reward estimations, and this change is not compensated for. To make this work, we will use the double indifference approach . The classical compensatory rewards are the same are the previous section, except we now need to keep track of which probability estimator is being used to calculate them: C ( π , h < t a t o t , r t , ρ t ) = V ( π , h < t a t o t , r t , ρ t ) − V ( π , h < t a t o t , f ( r t , o t ) , ρ t ) . This is the C of the double indifference approach. We'll need to sum these C in two different ways S C T ("True SC") and S C C ("Current SC"): S C T ( π , h < t a t o t , r t , ρ t ) = C ( π , h < t a t o t , r t , ρ t ) + E ρ t ( ∑ a t π ( a t | h < t ) ∑ o t μ ( o t | h < t a t ) [ γ S C T ( π , h < t a t o t , f ( r t , o t ) , g ( a t , ρ t ) , g ( a t , ρ t ) ) ] ) S C C ( π , h < t a t o t , r t , ρ t ) = C ( π , h < t a t o t , r t , ρ t ) + E ρ t ( ∑ a t π ( a t | h < t ) ∑ o t μ ( o t | h < t a t ) [ γ S C 2 ( π , h < t a t o t , f ( r t , o t ) , g ( a t , ρ t ) , ρ t ) ] ) What is the difference? Well, S C T estimates (using ρ t ) the true future discounted sum of C , while S C C estimates (still using ρ t ), the future discounted sum of C , were the C to be estimated at the time of their estimation using ρ t rather than whatever ρ the agent was using at the time. Now, the | C | are bounded by the maximal value of V , which is 1 / ( 1 − γ ) . Hence the S C T and S C C are bounded, if the E ρ i are sensible, by the discounted sum of such terms, thus by 1 / ( 1 − γ ) 2 . Then we need to define the D . The agent will get rewards of type C and of type D . The D will contain the S C terms to correct future expected C 's, but will also contain terms to correct future D 's. Roughly speaking, if we denote D t ( ρ i ) the reward at time t using ρ i to estimate this reward, and the true reward at time t is D t ( ρ t ) , then D t ( ρ i ) = E ρ i [ ( S C C ) t − ( S C T ) t − ∑ ∞ j = 1 γ j D t + j ( ρ t + j ) ] . This results in the recursion formula: D t ( ρ i ) = E ρ i [ ( S C C ) t − ( S C T ) t − γ D t + 1 ( ρ t + 1 ) ] + γ E ρ i [ D t + 1 ( ρ i ) − ( ( S C C ) t + 1 − ( S C T ) t + 1 ] Or, in more precise notation: D ( π , h < t a t o t , r t , ρ t , ρ i ) = S C C ( π , h < t a t o t , r t , ρ t ) − S C T ( π , h < t a t o t , r t , ρ t ) + E ρ t ( ∑ a t π ( a t | h < t ) ∑ o t μ ( o t | h < t a t ) γ [ − D ( π , h < t a t o t , f ( r t , o t ) , g ( a t , ρ t ) , g ( a t , ρ t ) ) + D ( π , h < t a t o t , f ( r t , o t ) , g ( a t , ρ t ) , ρ i ) − { S C C ( π , h < t a t o t , f ( r t , o t ) , g ( a t , ρ t ) ) − S C T ( π , h < t a t o t , f ( r t , o t ) , g ( a t , ρ t ) ) } ] ) It sees that this quantity remains bounded if γ < 0.5 ; general convergence results are harder. Then the agent, after turn t , will get compensatory rewards C ( π , h < t a t o t , r t , ρ t ) + D ( π , h < t a t o t , r t , ρ t , ρ t ) . Thus it continues to get the C reward that ensure indifference at the point of change of utility. The role of the D is to remove, in expectation, all future C rewards ( S C T ) and all future D rewards, and to add expected C rewards as they would have been estimated by ρ t . Therefore, at turn t , the agent is also indifferent to future changes of utility. Hence the agent will always be indifferent to future changes of utility, and will never try to change ρ t for the purpose of getting compenstory rewards.